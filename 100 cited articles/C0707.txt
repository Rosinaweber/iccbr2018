 Extracting Semantic Orientations of Words using Spin Model Hiroya Takamura  Precision and Intelligence Laboratory  Tokyo Institute of Technology  have a positive attitude on the topic. The goal of this paper is to propose a method for automatically cre-We propose a method for extracting  seating such a word list from glosses (i.e., definition  mantic orientations of words: desirable  or explanation sentences ) in a dictionary, as well as or undesirable. Regarding semantic ori-from a thesaurus and a corpus. For this purpose, we  entations as spins of electrons, we use  use spin model, which is a model for a set of elec-the mean field approximation to compute  trons with spins. Just as each electron has a  directhe approximate probability function of  tion of spin (up or down), each word has a semantic  the system instead of the intractable  acorientation (positive or negative). We therefore  retual probability function.  We also  progard words as a set of electrons and apply the mean  pose a criterion for parameter selection on  field approximation to compute the average  orientathe basis of magnetization. Given only  tion of each word. We also propose a criterion for  a small number of seed words, the  proparameter selection on the basis of magnetization, a  posed method extracts semantic  orientanotion in statistical physics. Magnetization indicates tions with high accuracy in the exper-the global tendency of polarization.  iments on English lexicon.  The result  We empirically show that the proposed method  is comparable to the best value ever  reworks well even with a small number of seed words.  ported.  Related Work  Introduction  Turney and Littman (2003) proposed two algorithms  for extraction of semantic orientations of words. To  Identification of emotions (including opinions and  calculate the association strength of a word with  posattitudes) in text is an important task which has a va-itive (negative) seed words, they used the number  riety of possible applications. For example, we can  of hits returned by a search engine, with a query  efficiently collect opinions on a new product from  consisting of the word and one of seed words (e.g.,  the internet, if opinions in bulletin boards are  auto word NEAR good , word NEAR bad ). They re-matically identified. We will also be able to grasp  garded the difference of two association strengths as  people's attitudes in questionnaire, without actually  a measure of semantic orientation. They also  proreading all the responds.  posed to use Latent Semantic Analysis to compute  An important resource in realizing such  identifithe association strength with seed words. An  emcation tasks is a list of words with semantic  orientapirical evaluation was conducted on 3596 words  extracted from General Inquirer (Stone et al., 1966).  Frequent appearance of positive words in a  docuHatzivassiloglou and McKeown (1997) focused  ment implies that the writer of the document would  on conjunctive expressions such as simple and  Proceedings of the 43rd Annual Meeting of the ACL, pages 133 140, Ann Arbor, June 2005. c  2005 Association for Computational Linguistics  well-received and simplistic but well-received , 3  Spin Model and Mean Field  where the former pair of words tend to have the same  Approximation  semantic orientation, and the latter tend to have the  We give a brief introduction to the spin model  opposite orientation. They first classify each  conand the mean field approximation, which are  welljunctive expression into the same-orientation class  studied subjects both in the statistical mechanics  or the different-orientation class. They then use the  and the machine learning communities (Geman and  classified expressions to cluster words into the  posGeman, 1984; Inoue and Carlucci, 2001; Mackay,  itive class and the negative class. The experiments  were conducted with the dataset that they created on  A spin system is an array of N electrons, each of their own. Evaluation was limited to adjectives.  which has a spin with one of two values +1 (up) or  Kobayashi et al. (2001) proposed a method for  ex 1 (down) . Two electrons next to each other en-tracting semantic orientations of words with  bootergetically tend to have the same spin. This model  strapping. The semantic orientation of a word is  is called the Ising spin model, or simply the spin determined on the basis of its gloss, if any of their  model (Chandler, 1987). The energy function of a 52 hand-crafted rules is applicable to the sentence.  spin system can be represented as  Rules are applied iteratively in the bootstrapping  framework. Although Kobayashi et al.'s work  provided an accurate investigation on this task and  inspired our work, it has drawbacks: low recall and  language dependency. They reported that the  semanwhere xi and xj ( x) are spins of electrons i and j, tic orientations of only 113 words are extracted with  matrix W = {wij} represents weights between two precision 84.1% (the low recall is due partly to their electrons.  large set of seed words (1187 words)). The  handIn a spin system, the variable vector x follows the crafted rules are only for Japanese.  Kamps et al. (2004) constructed a network by  connecting each pair of synonymous words provided  by WordNet (Fellbaum, 1998), and then used the  where Z( W ) =  x exp( E(x , W )) is the nor-shortest paths to two seed words good and bad  malization factor, which is called the partition  to obtain the semantic orientation of a word.  Limifunction and is a constant called the inverse-tations of their method are that a synonymy  dictiotemperature. As this distribution function suggests, nary is required, that antonym relations cannot be  a configuration with a higher energy value has a  incorporated into the model. Their evaluation is  resmaller probability.  stricted to adjectives. The method proposed by Hu  Although we have a distribution function,  comand Liu (2004) is quite similar to the shortest-path  puting various probability values is computationally  method. Hu and Liu's method iteratively determines  difficult. The bottleneck is the evaluation of Z( W ), the semantic orientations of the words neighboring  since there are 2 N configurations of spins in this sys-any of the seed words and enlarges the seed word  We therefore approximate P (x |W ) with a simple Subjective words are often semantically oriented.  function Q(x; ). The set of parameters for Q, is Wiebe (2000) used a learning method to collect sub-determined such that Q(x; ) becomes as similar to jective adjectives from corpora. Riloff et al. (2003)  P (x |W ) as possible. As a measure for the distance focused on the collection of subjective nouns.  between P and Q, the variational free energy F is We later compare our method with Turney and  often used, which is defined as the difference  beLittman's method and Kamps et al.'s method.  tween the mean energy with respect to Q and the The other pieces of research work mentioned  entropy of Q :  above are related to ours, but their objectives are dif-X  Q(x; ) E(x; W ) ferent from ours.  Q(x; ) log Q(x; ) . (3) Using the mean-field method developed in statis-x  tical mechanics, we determine the semantic  orientations on the network in a global manner. The global  The parameters that minimizes the variational free optimization enables the incorporation of possibly  energy will be chosen. It has been shown that  mininoisy resources such as glosses and corpora, while  mizing F is equivalent to minimizing the Kullback-existing simple methods such as the shortest-path  Leibler divergence between P and Q (Mackay, method and the bootstrapping method cannot work  in the presence of such noisy evidences.  Those  We next assume that the function Q(x; ) has the methods depend on less-noisy data such as a the-factorial form :  Construction of Lexical Networks  Simple substitution and transformation leads us to  We construct a lexical network by linking two words  the following variational free energy :  if one word appears in the gloss of the other word.  Each link belongs to one of two groups: the  sameorientation links SL and the different-orientation ij  links DL. If at least one word precedes a  negation word (e.g., not) in the gloss of the other word,  the link is a different-orientation link. Otherwise the (5)  With the usual method of Lagrange multipliers,  We next set weights W = ( wij) to links : we obtain the mean field equation :  otherwise  This equation is solved by the iterative update rule : where lij denotes the link between word i and word  j, and d( i) denotes the degree of word i, which x x  means the number of words linked with word i. Two  words without connections are regarded as being  connected by a link of weight 0. We call this  network the gloss network (G).  Extraction of Semantic Orientation of  another  the  Words with Spin Model  thesaurus network (GT), by linking synonyms,  We use the spin model to extract semantic  orientaantonyms and hypernyms, in addition to the the  tions of words.  above linked words. Only antonym links are in DL.  Each spin has a direction taking one of two values:  We enhance the gloss-thesaurus network with  up or down. Two neighboring spins tend to have the  cooccurrence information extracted from corpus. As  same direction from a energetic reason. Regarding  mentioned in Section 2, Hatzivassiloglou and  McKeach word as an electron and its semantic orientation  eown (1997) used conjunctive expressions in corpus.  as the spin of the electron, we construct a lexical net-Following their method, we connect two adjectives  work by connecting two words if, for example, one  if the adjectives appear in a conjunctive form in the  word appears in the gloss of the other word.  Intucorpus. If the adjectives are connected by and , the  ition behind this is that if a word is semantically ori-link belongs to SL. If they are connected by but ,  ented in one direction, then the words in its gloss  the link belongs to DL. We call this network the  tend to be oriented in the same direction.  gloss-thesaurus-corpus network (GTC).  Extraction of Orientations  where [ t] is 1 if t is negative, otherwise 0, and  We suppose that a small number of seed words are  calculated with the right-hand-side of Equation (6),  given. In other words, we know beforehand the  sewhere the penalty term (  mantic orientations of those given words. We  incoris ignored. We choose that minimizes this value.  porate this small labeled dataset by modifying the  However, when a large amount of labeled data is  unavailable, the value of pseudo leave-one-out error  Instead of E(x , W ) in Equation (2), we use the rate is not reliable. In such cases, we use magnetiza-following function H( , x , W ) : tion m for hyper-parameter prediction :  At a high temperature, spins are randomly  oriwhere L is the set of seed words, ai is the orientation ented ( paramagnetic phase, m 0).  At a low  of seed word i, and is a positive constant. This temperature, most of the spins have the same di-expression means that if xi ( i L) is different from rection ( ferromagnetic phase, m 6= 0).  It is  ai, the state is penalized.  known that at some intermediate temperature,  ferroUsing function H, we obtain the new update rule magnetic phase suddenly changes to paramagnetic  phase. This phenomenon is called phase transition.  Slightly before the phase transition, spins are locally x x  isold  polarized; strongly connected spins have the same  polarity, but not in a global way.  isold  Intuitively, the state of the lexical network is  locally polarized. Therefore, we calculate values of  where sold  and  are the  m with several different values of and select the averages of xi respectively before and after update.  value just before the phase transition.  What is discussed here was constructed with the  reference to work by Inoue and Carlucci (2001), in  Discussion on the Model  which they applied the spin glass model to image  In our model, the semantic orientations of words  restoration.  are determined according to the averages values of  Initially, the averages of the seed words are set  the spins. Despite the heuristic flavor of this  deciaccording to their given orientations. The other  avsion rule, it has a theoretical background related to  maximizer of posterior marginal (MPM) estimation,  When the difference in the value of the variational  free energy is smaller than a threshold before and  quin, 1985). In MPM, the average is the marginal  after update, we regard computation converged.  distribution over xi obtained from the distribution The words with high final average values are clas-over x. We should note that the finite-temperature sified as positive words. The words with low final  decoding is quite different from annealing type  algoaverage values are classified as negative words.  rithms or zero-temperature decoding', which  corHyper-parameter Prediction  tion and also often used in natural language  processThe performance of the proposed method largely  depends on the value of hyper-parameter . In order to Since the model estimation has been reduced  make the method more practical, we propose criteria  to simple update calculations, the proposed model  is similar to conventional spreading activation  apWhen a large labeled dataset is available, we can  proaches, which have been applied, for example, to  obtain a reliable pseudo leave-one-out error rate : word sense disambiguation (Veronis and Ide, 1990).  Actually, the proposed model can be regarded as a  spreading activation model with a specific update  rule, as long as we are dealing with 2-class model Table 1: Classification accuracy (%) with various  networks and four different sets of seed words. In  However, there are some advantages in our  modthe parentheses, the predicted value of is written.  elling. The largest advantage is its theoretical  backFor cv, no value is written for , since 10 different ground. We have an objective function and its ap-values are obtained.  proximation method. We thus have a measure of  goodness in model estimation and can use another  better approximation method, such as Bethe  approxThe theory tells  us which update rule to use. We also have a  notion of magnetization, which can be used for  hyperparameter estimation. We can use a plenty of  knowledge, methods and algorithms developed in the field  accuracy, seed words are eliminated from these 3596  of statistical mechanics. We can also extend our  We conducted experiments with different values  Another interesting point is the relation to  maxiof from 0.1 to 2.0, with the interval 0.1, and pre-mum entropy model (Berger et al., 1996), which is  dicted the best value as explained in Section 4.3. The popular in the natural language processing commu-threshold of the magnetization for hyper-parameter  nity. Our model can be obtained by maximizing the  estimation is set to 1 . 0 10 5. That is, the pre-entropy of the probability distribution Q(x) under dicted optimal value of is the largest whose constraints regarding the energy function.  corresponding magnetization does not exceeds the  threshold value.  We performed 10-fold cross validation as well as  We used glosses, synonyms, antonyms and  hyperexperiments with fixed seed words. The fixed seed  nyms of WordNet (Fellbaum, 1998) to construct an  words are the ones used by Turney and Littman: 14  English lexical network.  For part-of-speech  tagseed words { good, nice, excellent, positive, fortu-ging and lemmatization of glosses, we used  Treenate, correct, superior, bad, nasty, poor, negative,  Tagger (Schmid, 1994). 35 stopwords (quite  freunfortunate, wrong, inferior }; 4 seed words { good, quent words such as be and have ) are removed  superior, bad, inferior }; 2 seed words { good, bad }.  from the lexical network. Negation words include  33 words. In addition to usual negation words such  Classification Accuracy  as not and never , we include words and phrases  Table 1 shows the accuracy values of semantic  oriwhich mean negation in a general sense, such as  entation classification for four different sets of seed  free from and lack of . The whole network  conwords and various networks. In the table, cv  corresists of approximately 88,000 words. We collected  sponds to the result of 10-fold cross validation, in  804 conjunctive expressions from Wall Street  Jourwhich case we use the pseudo leave-one-out error  nal and Brown corpus as described in Section 4.2.  for hyper-parameter estimation, while in other cases  The labeled dataset used as a gold standard is  General Inquirer lexicon (Stone et al., 1966) as in the In most cases, the synonyms and the cooccurrence  work by Turney and Littman (2003). We extracted  information from corpus improve accuracy.  The  the words tagged with Positiv or Negativ , and  only exception is the case of 2 seed words, in which  reduced multiple-entry words to single entries. As a  G performs better than GT. One possible reason of  result, we obtained 3596 words (1616 positive words  this inversion is that the computation is trapped in a and 1980 negative words) 1. In the computation of  local optimum, since a small number of seed words  leave a relatively large degree of freedom in the  so1Although we preprocessed in the same way as Turney and lution space, resulting in more local optimal points.  Littman, there is a slight difference between their dataset and our dataset. However, we believe this difference is insignificant.  We compare our results with Turney and  Table 2: Actual best classification accuracy (%) with various networks and four different sets of seed  words. In the parenthesis, the actual best value of  is written, except for cv.  Accuracy  Littman's results. With 14 seed words, they achieved  61.26% for a small corpus (approx. 1 107 words), accuracy  76.06% for a medium-sized corpus (approx. 2 109  words), 82.84% for a large corpus (approx. 1 1011  Without a corpus nor a thesaurus (but with glosses  in a dictionary), we obtained accuracy that is  comparable to Turney and Littman's with a medium-sized  corpus. When we enhance the lexical network with  Figure 1: Example of magnetization and  classificacorpus and thesaurus, our result is comparable to  tion accuracy(14 seed words).  Turney and Littman's with a large corpus.  Prediction of  We examine how accurately our prediction method  for works by comparing Table 1 above and Table 2 below. Our method predicts good quite well 100  especially for 14 seed words. For small numbers of  seed words, our method using magnetization tends  to predict a little larger value.  We also display the figure of magnetization and  accuracy in Figure 1. We can see that the sharp  change of magnetization occurs at around = 1 . 0  (phrase transition). At almost the same point, the  classification accuracy reaches the peak.  Precision  Precision for the Words with High  We next evaluate the proposed method in terms of  precision for the words that are classified with high  confidence. We regard the absolute value of each  average as a confidence measure and evaluate the top  Number of selected words  words with the highest absolute values of averages.  The result of this experiment is shown in Figure 2,  Figure 2: Precision (%) with 14 seed words.  for 14 seed words as an example. The top 1000  words achieved more than 92% accuracy. This  result shows that the absolute value of each average  We also tested the shortest path method and the Table 3:  Precision (%) for selected adjectives.  bootstrapping method on GTC and GT, and obtained  Comparison between the proposed method and the  low accuracies as expected in the discussion in  Secshortest-path method.  tion 4.  short. path  Error Analysis  We investigated a number of errors and concluded  that there were mainly three types of errors.  One is the ambiguity of word senses. For  example, one of the glosses of costly is entailing great Table 4: Precision (%) for adjectives. Comparison  The word great here means  between the proposed method and the bootstrapping  large , although it usually means outstanding and  is positively oriented.  Another is lack of structural information. For  example, arrogance means overbearing pride  evidenced by a superior manner toward the weak .  Although arrogance is mistakingly predicted as  positive due to the word superior , what is superior here is manner .  can work as a confidence measure of classification.  The last one is idiomatic expressions. For  example, although brag means show off , neither of  Comparison with other methods  show and off has the negative orientation.  IdIn order to further investigate the model, we conduct  iomatic expressions often does not inherit the  seexperiments in restricted settings.  mantic orientation from or to the words in the gloss.  We first construct a lexical network using only  The current model cannot deal with these types of  We compare the spin model with  errors. We leave their solutions as future work.  the shortest-path method proposed by Kamps et  al. (2004) on this network, because the  shortestConclusion and Future Work  path method cannot incorporate negative links of  antonyms. We also restrict the test data to 697  adWe proposed a method for extracting semantic  orijectives, which is the number of examples that the  entations of words. In the proposed method, we  reshortest-path method can assign a non-zero  oriengarded semantic orientations as spins of electrons,  tation value. Since the shortest-path method is  deand used the mean field approximation to compute  signed for 2 seed words, the method is extended  the approximate probability function of the system  to use the average shortest-path lengths for 4 seed  instead of the intractable actual probability function.  words and 14 seed words. Table 3 shows the  reWe succeeded in extracting semantic orientations  sult. Since the only difference is their algorithms,  with high accuracy, even when only a small number  we can conclude that the global optimization of the  of seed words are available.  spin model works well for the semantic orientation  There are a number of directions for future work.  One is the incorporation of syntactic information.  We next compare the proposed method with a  Since the importance of each word consisting a gloss  simple bootstrapping method proposed by Hu and  depends on its syntactic role. syntactic information  Liu (2004). We construct a lexical network using  in glosses should be useful for classification.  synonyms and antonyms. We restrict the test data  Another is active learning.  To decrease the  to 1470 adjectives for comparison of methods. The  amount of manual tagging for seed words, an active  result in Table 4 also shows that the global  optimizalearning scheme is desired, in which a small number  tion of the spin model works well for the semantic  of good seed words are automatically selected.  Although our model can easily extended to a  multi-state model, the effectiveness of using such a Jaap Kamps, Maarten Marx, Robert J. Mokken, and  multi-state model has not been shown yet.  Using wordnet to  measure semantic orientation of adjectives. In Proceed-Our model uses only the tendency of having the  ings of the 4th International Conference on Language same orientation. Therefore we can extract seman-Resources and Evaluation (LREC 2004), volume IV, tic orientations of new words that are not listed in  a dictionary. The validation of such extension will  widen the possibility of application of our method.  Dictionary-based acquisition of the lexical  Larger corpora such as web data will improve  perknowledge for p/n analysis (in Japanese).  formance. The combination of our method and the  ceedings of Japanese Society for Artificial Intelligence, method by Turney and Littman (2003) is promising.  Finally, we believe that the proposed model is  apDavid J. C. Mackay. 2003. Information Theory, Infer-plicable to other tasks in computational linguistics.  ence and Learning Algorithms. Cambridge University Press.  tors for image segmentation and surface  reconstruction. Technical Report A.I. Memo 839, Massachusetts  Institute of Technology.  to natural language processing. Computational  LinEllen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.  Learning subjective nouns using extraction pattern  David Chandler. 1987. Introduction to Modern Statisti-bootstrapping.  In Proceedings of the Seventh  Concal Mechanics. Oxford University Press.  ference on Natural Language Learning (CoNLL-03), pages 25 32.  Jim Cowie, Joe Guthrie, and Louise Guthrie. 1992. Lexical disambiguation using simulated annealing. In Pro-Helmut Schmid. 1994. Probabilistic part-of-speech  tagceedings of the 14th conference on Computational linging using decision trees. In Proceedings of Interna-guistics, volume 1, pages 359 365.  tional Conference on New Methods in Language  ProWordNet: An Electronic  Lexical Database, Language, Speech, and  CommuniPhilip J. Stone, Dexter C. Dunphy, Marshall S. Smith,  cation Series. MIT Press.  and Daniel M. Ogilvie. 1966. The General Inquirer: Stuart Geman and Donald Geman. 1984. Stochastic re-A Computer Approach to Content Analysis. The MIT  laxation, gibbs distributions, and the bayesian restora-Press.  tion of images. IEEE Transactions on Pattern Analysis Kazuyuki Tanaka, Junichi Inoue, and Mike Titterington.  and Machine Intelligence, 6:721 741.  2003. Probabilistic image processing by means of the  Vasileios Hatzivassiloglou and Kathleen R. McKeown.  bethe approximation for the q-ising model. Journal 1997. Predicting the semantic orientation of adjec-of Physics A: Mathematical and General, 36:11023  tives. In Proceedings of the Thirty-Fifth Annual Meet-11035.  ing of the Association for Computational Linguistics Peter D. Turney and Michael L. Littman. 2003. Measur-and the Eighth Conference of the European Chapter of ing praise and criticism: Inference of semantic orien-the Association for Computational Linguistics, pages tation from association. ACM Transactions on Infor-174 181.  mation Systems, 21(4):315 346.  rizing customer reviews. In Proceedings of the 2004  ACM SIGKDD international conference on  Knowlambiguation with very large neural networks extracted  edge discovery and data mining (KDD-2004), pages from machine readable dictionaries. In Proceedings 168 177.  of the 13th Conference on Computational Linguistics, volume 2, pages 389 394.  Yukito Iba. 1999. The nishimori line and bayesian statistics. Journal of Physics A: Mathematical and General, Janyce M. Wiebe.  Learning subjective  adjectives from corpora. In Proceedings of the 17th  National Conference on Artificial Intelligence (AAAI-Junichi Inoue and Domenico M. Carlucci. 2001. Image  restoration using the q-ising spin glass. Physical Review E, 64:036121 1 036121 18. 