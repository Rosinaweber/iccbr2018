 Word representations:  A simple and general method for semi-supervised learning Joseph Turian  Department of  Computer Science  University of Illinois at  already been induced plug these word features  into an existing system, and observe a significant If we take an existing supervised NLP sys-increase in accuracy. But which word features are tem, a simple and general way to improve  good for what tasks? Should we prefer certain  accuracy is to use unsupervised word  word features? Can we combine them?  representations as extra word features. We  A word representation is a mathematical object  evaluate Brown clusters, Collobert and  associated with each word, often a vector. Each  Weston (2008) embeddings, and HLBL  dimension's value corresponds to a feature and  might even have a semantic or grammatical  of words on both NER and chunking.  interpretation, so we call it a word feature.  We use near state-of-the-art supervised  Conventionally, supervised lexicalized NLP  apbaselines, and find that each of the three  proaches take a word and convert it to a symbolic word representations improves the accu-ID, which is then transformed into a feature vector racy of these baselines. We find further  using a one-hot representation: The feature vector improvements by combining different  has the same length as the size of the vocabulary, word representations. You can download  and only one dimension is on.  However, the  our word features, for off-the-shelf use  one-hot representation of a word suffers from data in existing NLP systems, as well as our  sparsity: Namely, for words that are rare in the code,  labeled training data, their corresponding model com/projects/wordreprs/  parameters will be poorly estimated. Moreover,  at test time, the model cannot handle words that 1  Introduction  do not appear in the labeled training data. These By using unlabelled data to reduce data sparsity limitations of one-hot word representations have in the labeled training data, semi-supervised  prompted researchers to investigate unsupervised approaches  methods for inducing word representations over  Semi-supervised models such as Ando and Zhang  Word features can be  hand-designed, but our goal is to learn them.  (2009) achieve state-of-the-art accuracy.  One common approach to inducing  unsuperHowever, these approaches dictate a particular  vised word representation is to use clustering,  choice of model and training regime. It can be  perhaps hierarchical. This technique was used by tricky and time-consuming to adapt an existing su-a variety of researchers (Miller et al., 2004; Liang, pervised NLP system to use these semi-supervised 2005; Koo et al., 2008; Ratinov & Roth, 2009; techniques. It is preferable to use a simple and Huang & Yates, 2009). This leads to a one-hot general method to adapt existing supervised NLP  representation over a smaller vocabulary size.  systems to be semi-supervised.  One approach that is becoming popular is  to use unsupervised methods to induce word  2007; Collobert & Weston, 2008), on the other features or to download word features that have  hand, induce dense real-valued low-dimensional  Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384 394, Uppsala, Sweden, 11-16 July 2010. c  2010 Association for Computational Linguistics  word embeddings using unsupervised approaches.  (columns): The first or second W are counted if the (See Bengio (2008) for a more complete list of  word c occurs within a window of 10 to the left or references on neural language models.)  right of the word w, respectively. f is chosen by Unsupervised  taking the 200 columns (out of 140K in F) with  been used in previous NLP work, and have  the highest variances. ICA is another technique to demonstrated improvements in generalization  transform F into f . (V yrynen & Honkela, 2004; accuracy on a variety of tasks. But different word V yrynen & Honkela, 2005; V yrynen et al.,  representations have never been systematically  2007). ICA is expensive, and the largest  vocabcompared in a controlled way. In this work, we  ulary size used in these works was only 10K. As  compare different techniques for inducing word  far as we know, ICA methods have not been used  representations, evaluating them on the tasks of when the size of the vocab W is 100K or more.  named entity recognition (NER) and chunking.  Explicitly storing cooccurrence matrix F can be  We retract former negative results published in  memory-intensive, and transforming F to f can  Turian et al. (2009) about Collobert and Weston  be time-consuming. It is preferable that F never (2008) embeddings, given training improvements  be computed explicitly, and that f be constructed that we describe in Section 7.1.  incrementally. eh u ek and Sojka (2010) describe an incremental approach to inducing LSA and  Distributional representations  LDA topic models over 270 millions word tokens  Distributional word representations are based  with a vocabulary of 315K word types. This is  upon a cooccurrence matrix F of size W C, where similar in magnitude to our experiments.  W is the vocabulary size, each row Fw is the ini-Another incremental approach to constructing f  tial representation of word w, and each column Fc is using a random projection: Linear mapping g is is some context. Sahlgren (2006) and Turney and  multiplying F by a random matrix chosen a  priPantel (2010) describe a handful of possible  deori. This random indexing method is motivated  sign decisions in contructing F, including choice by the Johnson-Lindenstrauss lemma, which states of context types (left window? right window? size that for certain choices of random matrix, if d is of window?) and type of frequency count (raw?  sufficiently large, then the original distances be-binary? tf-idf?). Fw has dimensionality W, which tween words in F will be preserved in f (Sahlgren, can be too large to use Fw as features for word w in 2005). Kaski (1998) uses this technique to proa supervised model. One can map F to matrix f of duce 100-dimensional representations of docu-size W d, where d C, using some function g,  ments. Sahlgren (2001) was the first author to use where f = g(F). fw represents word w as a vector random indexing using narrow context. Sahlgren  with d dimensions. The choice of g is another  de(2006) does a battery of experiments exploring  sign decision, although perhaps not as important different design decisions involved in construct-as the statistics used to initially construct F.  ing F, prior to using random indexing. However,  The self-organizing semantic map (Ritter &  like all the works cited above, Sahlgren (2006)  only uses distributional representation to improve that maps words to two dimensions, such that  existing systems for one-shot classification tasks, syntactically and semantically related words are such as IR, WSD, semantic knowledge tests, and  It is not well-understood  what settings are appropriate to induce distribu-1998), LSI, and LDA (Blei et al., 2003) induce  tional word representations for structured predic-distributional representations over F in which  tion tasks (like parsing and MT) and sequence la-each column is a document context. In most of the beling tasks (like chunking and NER). Previous  other approaches discussed, the columns represent research has achieved repeated successes on these word contexts. In LSA, g computes the SVD of F.  tasks using clustering representations (Section 3) Hyperspace Analogue to Language (HAL) is  and distributed representations (Section 4), so we another early distributional approach (Lund et al., focus on these representations in our work.  1995; Lund & Burgess, 1996) to inducing word 3  Clustering-based word representations  representations. They compute F over a corpus of 160 million word tokens with a vocabulary size W  Another type of word representation is to induce of 70K word types. There are 2 W types of context a clustering over words. Clustering methods and  distributional methods can overlap. For example, a baseline CRF chunker (Sha & Pereira, 2003).  Pereira et al. (1993) begin with a cooccurrence  Goldberg et al. (2009) use an HMM to assign  matrix and transform this matrix into a clustering.  POS tags to words, which in turns improves  the accuracy of the PCFG-based Hebrew parser.  Brown clustering  Deschacht and Moens (2009) use a latent-variable The Brown algorithm is a hierarchical clustering language model to improve semantic role labeling.  algorithm which clusters words to maximize the  Distributed representations  mutual information of bigrams (Brown et al.,  So it is a class-based bigram language  Another approach to word representation is to  model. It runs in time O(V K2), where V is the size learn a distributed representation.  (Not to be  of the vocabulary and K is the number of clusters.  confused with distributional representations.)  The hierarchical nature of the clustering means  A distributed representation is dense,  that we can choose the word class at several  dimensional, and real-valued. Distributed word  levels in the hierarchy, which can compensate for representations are called word embeddings. Each poor clusters of a small number of words. One  dimension of the embedding represents a latent  downside of Brown clustering is that it is based feature of the word, hopefully capturing useful  solely on bigram statistics, and does not consider syntactic and semantic properties. A distributed word usage in a wider context.  representation is compact, in the sense that it can Brown clusters have been used successfully in  represent an exponential number of clusters in the a variety of NLP applications: NER (Miller et al., number of dimensions.  2004; Liang, 2005; Ratinov & Roth, 2009), PCFG  Word embeddings are typically induced  usparsing (Candito & Crabb , 2009), dependency ing neural language models, which use neural  parsing (Koo et al., 2008; Suzuki et al., 2009), and networks as the underlying predictive model  semantic dependency parsing (Zhao et al., 2009).  (Bengio, 2008). Historically, training and testing Martin et al.  (1998) presents algorithms for  of neural language models has been slow, scaling inducing hierarchical clusterings based upon word as the size of the vocabulary for each model com-bigram and trigram statistics.  presents an extension to the Brown clustering  However, many approaches have been proposed  algorithm, and learn hierarchical clusterings of in recent years to eliminate that linear dependency words as well as phrases, which they apply to  on vocabulary size (Morin & Bengio, 2005;  Collobert & Weston, 2008; Mnih & Hinton, 2009) and allow scaling to very large training corpora.  Other  cluster-based  Collobert and Weston (2008) embeddings  Collobert and Weston (2008) presented a neural  Lin and Wu (2009) present a K-means-like  language model that could be trained over billions non-hierarchical clustering algorithm for phrases, of words, because the gradient of the loss was  which uses MapReduce.  computed stochastically over a small sample of  HMMs can be used to induce a soft clustering,  possible outputs, in a spirit similar to Bengio and specifically a multinomial distribution over posS n cal (2003). This neural model of Collobert  sible clusters (hidden states). Li and McCallum  and Weston (2008) was refined and presented in  POS tagging and Chinese Word Segmentation.  The  and  Huang and Yates (2009) induce a fully-connected  For each training update, we  HMM, which emits a multinomial distribution  over possible vocabulary words. They perform  n) from the corpus.  The model concatenates the learned embeddings  hard clustering using the Viterbi algorithm.  of the n words, giving e(w  (Alternately, they could keep the soft clustering, 1) . . . e(wn), where  e is the lookup table and is concatenation.  with the representation for a particular word token We also create a corrupted or noise n-gram  being the posterior probability distribution over  the states.) However, the CRF chunker in Huang  n), where  uniformly from the vocabulary.1 For convenience, and Yates (2009), which uses their HMM word  clusters as extra features, achieves F1 lower than 1In Collobert and Weston (2008), the middle word in the 386  we write e(x) to mean e(w1) . . . e(wn). We 5  predict a score s(x) for x by passing e(x) through We evaluate the hypothesis that one can take an  a single hidden layer neural network. The training existing, near state-of-the-art, supervised NLP  criterion is that n-grams that are present in the system, and improve its accuracy by including  training corpus like x must have a score at least word representations as word features.  This  some margin higher than corrupted n-grams like  technique for turning a supervised approach into a  x. Specifically: L(x) = max(0, 1 s(x) + s( x)). We semi-supervised one is general and task-agnostic.  minimize this loss stochastically over the n-grams However, we wish to find out if certain word  in the corpus, doing gradient descent  simultanerepresentations are preferable for certain tasks.  ously over the neural network parameters and the Lin and Wu (2009) finds that the representations embedding lookup table.  that are good for NER are poor for search query  We implemented the approach of Collobert and  We apply  clusWeston (2008), with the following differences:  tering and distributed representations to NER  We did not achieve as low log-ranks on the  and chunking, which allows us to compare our  English Wikipedia as the authors reported in  semi-supervised models to those of Ando and  Bengio et al. (2009), despite initially attempting Zhang (2005) and Suzuki and Isozaki (2008).  to have identical experimental conditions.  We corrupt the last word of each n-gram.  We had a separate learning rate for the  emChunking is a syntactic sequence labeling task.  beddings and for the neural network weights.  We follow the conditions in the CoNLL-2000  We found that the embeddings should have a  shared task (Sang & Buchholz, 2000).  learning rate generally 1000 32000 times higher  The linear CRF chunker of Sha and Pereira  than the neural network weights. Otherwise, the  (2003) is a standard near-state-of-the-art baseline unsupervised training criterion drops slowly.  chunker. In fact, many off-the-shelf CRF  imple Although their sampling technique makes  trainmentations now replicate Sha and Pereira (2003), ing fast, testing is still expensive when the size of including their choice of feature set:  the vocabulary is large. Instead of cross-validating  using the log-rank over the validation data as  they do, we instead used the moving average of  by  the training loss on training examples before the bottou.org/projects/sgd)  weight update.  www.chokkan.org/software/crfsuite/)  We use CRFsuite because it makes it  simThe log-bilinear model (Mnih & Hinton, 2007) is ple to modify the feature generation code,  a probabilistic and linear neural model. Given an so one can easily add new features.  n-gram, the model concatenates the embeddings  use SGD optimization,  and enable negative  of the n 1 first words, and learns a linear model state  features  and  to predict the embedding of the last word. The  similarity between the predicted embedding and  the current actual embedding is transformed  Table 1 shows the features in the baseline  chuninto a probability by exponentiating and then  ker. As you can see, the Brown and embedding  normalizing. Mnih and Hinton (2009) speed up  features are unigram features, and do not partici-model evaluation during training and testing by  pate in conjunctions like the word features and tag using a hierarchy to exponentially filter down  features do. Koo et al. (2008) sees further accu-the number of computations that are performed.  racy improvements on dependency parsing when  This hierarchical evaluation technique was first using word representations in compound features.  proposed by Morin and Bengio (2005).  The  The data comes from the Penn Treebank, and  model, combined with this optimization, is called is newswire from the Wall Street Journal in 1989.  the hierarchical log-bilinear (HLBL) model.  Of the 8936 training sentences, we used 1000  randomly sampled sentences (23615 words) for  n-gram is corrupted. In Bengio et al. (2009), the last word in the n-gram is corrupted.  We trained models on the 7936  Word features: wi for i in { 2, 1, 0, +1, +2}, from Zhang and Johnson (2003):  Previous two predictions yi 1 and yi 2  Tag features: wi for i in { 2, 1, 0, +1, +2},  xi word type information:  is-capitalized, all-digits, alphanumeric, etc.  Embedding features [if applicable]: ei[d] for i  Prefixes and suffixes of xi, if the word contains in { 2, 1, 0, +1, +2}, where d ranges over the  hyphens, then the tokens between the hyphens  dimensions of the embedding ei.  the  Brown features [if applicable]: substr(b ,  for i in { 2, 1, 0, +1, +2}, where substr takes  Capitalization pattern in the window c  the p-length prefix of the Brown cluster bi.  Conjunction of c and yi 1.  Word representation features, if present, are used Table 1: Features templates used in the CRF chunker.  the same way as in Table 1.  When using the lexical features, we normalize  dates and numbers. For example, 1980 becomes  training partition sentences, and evaluated their  and 212-325-4751 becomes  *DDD*F1 on the development set. After choosing  hy*DDD*-*DDDD*. This allows a degree of  abstracperparameters to maximize the dev F1, we would  tion to years, phone numbers, etc. This  delexiretrain the model using these hyperparameters on calization is performed separately from using the the full 8936 sentence training set, and evaluate word representation. That is, if we have induced on test. One hyperparameter was l2-regularization an embedding for 12/3/2008 , we will use the em-sigma, which for most models was optimal at 2 or bedding of 12/3/2008 , and *DD*/*D*/*DDDD*  3.2. The word embeddings also required a scaling in the baseline features listed above.  hyperparameter, as described in Section 7.2.  Unlike in our chunking experiments, after we  Named entity recognition  chose the best model on the development set, we  used that model on the test set too. (In chunking, NER is typically treated as a sequence prediction after finding the best hyperparameters on the  problem. Following Ratinov and Roth (2009), we  development set, we would combine the dev  use the regularized averaged perceptron model.  and training set and training a model over this  Ratinov and Roth (2009) describe different  combined set, and then evaluate on test.)  sequence encoding like BILOU and BIO, and  show that the BILOU encoding outperforms BIO,  The standard evaluation benchmark for NER  and the greedy inference performs competitively  is the CoNLL03 shared task dataset drawn from  to Viterbi while being significantly faster.  the Reuters newswire. The training set contains  cordingly, we use greedy inference and BILOU  204K words (14K sentences, 946 documents), the  text chunk representation. We use the publicly  test set contains 46K words (3.5K sentences, 231  available implementation from Ratinov and Roth  documents), and the development set contains  (2009) (see the end of this paper for the URL). In 51K words (3.3K sentences, 216 documents).  our baseline experiments, we remove gazetteers  We also evaluated on an out-of-domain (OOD)  and non-local features (Krishnan & Manning,  dataset, the MUC7 formal run (59K words).  2006). However, we also run experiments that  MUC7 has a different annotation standard than  include these features, to understand if the infor-the CoNLL03 data. It has several NE types that  mation they provide mostly overlaps with that of don't appear in CoNLL03: money, dates, and  the word representations.  numeric quantities. CoNLL03 has MISC, which  is not present in MUC7. To evaluate on MUC7,  After each epoch over the training set, we  we perform the following postprocessing steps  measured the accuracy of the model on the  development set. Training was stopped after the  accuracy on the development set did not improve  1. In the gold-standard MUC7 data, discard  for 10 epochs, generally about 50 80 epochs  (label as O') all NEs with type  NUMThe epoch that performed best on the  development set was chosen as the final model.  2. In the predicted model output on MUC7 data,  We use the following baseline set of features  discard (label as O') all NEs with type MISC.  These postprocessing steps will adversely affect 7  all NER models across-the-board, nonetheless  Details of inducing word representations  allowing us to compare different models in a  The Brown clusters took roughly 3 days to induce, when we induced 1000 clusters, the baseline in  Unlabled Data  prior work (Koo et al., 2008; Ratinov & Roth, 2009).  We also induced 100, 320, and 3200  Unlabeled data is used for inducing the word  Brown clusters, for comparison. (Because Brown  representations. We used the RCV1 corpus, which  clustering scales quadratically in the number of contains one year of Reuters English newswire,  clusters, inducing 10000 clusters would have  from August 1996 to August 1997, about 63  been prohibitive.)  Because Brown clusters are  millions words in 3.3 million sentences.  hierarchical, we can use cluster supersets as  left case intact in the corpus.  features. We used clusters at path depth 4, 6, 10, Collobert and Weston (2008) downcases words  and 20 (Ratinov & Roth, 2009). These are the and delexicalizes numbers.  prefixes used in Table 1.  We use a preprocessing technique proposed  The Collobert and Weston (2008) (C&W)  by Liang, (2005, p. 51), which was later used  embeddings were induced over the course of a  by Koo et al. (2008): Remove all sentences that  few weeks, and trained for about 50 epochs. One  are less than 90% lowercase a z.  We assume  of the difficulties in inducing these embeddings is that whitespace is not counted, although this  that there is no stopping criterion defined, and that is not specified in Liang's thesis.  We call this  the quality of the embeddings can keep improving preprocessing step cleaning.  as training continues.  Collobert (p.c.)  simply  (2009), we found that all  leaves one computer training his embeddings  word representations performed better on the  indefinitely. We induced embeddings with 25, 50, supervised task when they were induced on the  100, or 200 dimensions over 5-gram windows.  clean unlabeled data, both embeddings and Brown  In comparison to Turian et al. (2009), we use  clusters. This is the case even though the cleaning improved C&W embeddings in this work:  process was very aggressive, and discarded more  They were trained for 50 epochs, not just 20  than half of the sentences.  According to the  evidence and arguments presented in Bengio et al.  (2009), the non-convex optimization process for  formly in the range [-0.01, +0.01], not [-1,+1].  Collobert and Weston (2008) embeddings might  For rare words, which are typically updated only be adversely affected by noise and the statistical 143 times per epoch2, and given that our embed-sparsity issues regarding rare words, especially ding learning rate was typically 1e-6 or 1e-7, this at the beginning of training. For this reason, we means that rare word embeddings will be concen-hypothesize that learning representations over the trated around zero, instead of spread out randomly.  most frequent words first and gradually increasing The HLBL embeddings were trained for 100  the vocabulary a curriculum training strategy  epochs (7 days).3 Unlike our Collobert and  Weet al., 2010) would provide better results than  tune the learning rates for HLBL. We used a learn-cleaning.  ing rate of 1e-3 for both model parameters and  After cleaning, there are 37 million words (58%  embedding parameters. We induced embeddings  of the original) in 1.3 million sentences (41% of with 100 dimensions over 5-gram windows, and  the original). The cleaned RCV1 corpus has 269K  embeddings with 50 dimensions over 5-gram  winword types. This is the vocabulary size, i.e. how dows. Embeddings were induced over one pass  many word representations were induced. Note  that cleaning is applied only to the unlabeled data, 2A rare word will appear 5 (window size) times per not to the labeled data used in the supervised tasks.  epoch as a positive example, and 37M (training examples per epoch) / 269K (vocabulary size) = 138 times per epoch as a RCV1 is a superset of the CoNLL03 corpus.  For this reason, NER results that use RCV1  The HLBL model updates require fewer matrix  multiplies than Collobert and Weston (2008) model updates.  word representations are a form of transductive  Additionally, HLBL models were trained on a GPGPU, learning.  which is faster than conventional CPU arithmetic.  approach using a random tree, not two passes with work.  an updated tree and embeddings re-estimation.  able to prescribe a default value for scaling the 7.2  Scaling of Word Embeddings  embeddings. However, these curves demonstrate  that a reasonable choice of scale factor is such that Like many NLP systems, the baseline system con-the embeddings have a standard deviation of 0.1.  tains only binary features. The word embeddings, however, are real numbers that are not necessarily 7.3  Capacity of Word Representations  in a bounded range.  If the range of the word  # of embedding dimensions  embeddings is too large, they will exert more  influence than the binary features.  We generally found that embeddings had zero  We can scale the embeddings by a  hyperparameter, to control their standard deviation.  Assume that the embeddings are represented by a  Brown  is a scaling constant that sets the new standard  # of Brown clusters  deviation after scaling the embeddings.  Brown  # of Brown clusters  Scaling factor  Figure 2: Effect as we vary the capacity of the word 92.5  representations on the validation set F1.  There are capacity controls for the word  representations: number of Brown clusters, and  number of dimensions of the word embeddings.  Figure 2 shows the effect on the validation F1 as 90  we vary the capacity of the word representations.  In general, it appears that more Brown clusters  are better. We would like to induce 10000 Brown  Scaling factor  clusters, however this would take several months.  In Turian et al. (2009), we hypothesized on  Figure 1: Effect as we vary the scaling factor (Equa-the basis of solely the HLBL NER curve that  tion 1) on the validation set F1.  We experiment with  Collobert and Weston (2008) and HLBL embeddings of various dimensionality. (a) Chunking results. (b) NER results.  higher accuracy.  Figure 2 shows that this  hypothesis is not true. For NER, the C&W curve is Figure 1 shows the effect of scaling factor  almost flat, and we were suprised to find the even on both supervised tasks.  We were surprised  to find that on both tasks, across Collobert and well. For chunking, 50-dimensional embeddings  Weston (2008) and HLBL embeddings of various  had the highest validation F1 for both C&W and dimensionality, that all curves had similar shapes HLBL. These curves indicates that the optimal  and optima.  This is one contributions of our  capacity of the word embeddings is task-specific.  Brown, 3200 clusters  Brown, 3200 clusters  Brown+HLBL, 37M  Brown+C&W+HLBL, 37M  Brown+C&W, 37M  Frequency of word in unlabeled data  Table 2: Final chunking F1 results. In the last section, we show how many unlabeled words were used.  Brown, 1000 clusters  Baseline+Nonlocal  Brown, 1000 clusters  Brown+HLBL  Brown+C&W  Frequency of word in unlabeled data  Figure 3: For word tokens that have different frequency Lin and Wu (2009), 3.4B  in the unlabeled data, what is the total number of per-token Ando and Zhang (2005), 27M  errors incurred on the test set? (a) Chunking results. (b) NER  All (Brown+C&W+HLBL+Gaz), 37M 93.17 90.04 82.50  All+Nonlocal, 37M  bining representations leads to small increases in Lin and Wu (2009), 700B  the test F1. In comparison to chunking,  combining different word representations on NER seems  Table 3: Final NER F1 results, showing the cumulative effect of adding word representations, non-local features, and gives larger improvements on the test F1.  gazetteers to the baseline. To speed up training, in combined On NER, Brown clusters are superior to the  experiments (C&W plus another word representation), word embeddings. Since much of the NER F1  we used the 50-dimensional C&W embeddings, not the 200-dimensional ones.  In the last section, we show how  is derived from decisions made over rare words,  many unlabeled words were used.  we suspected that Brown clustering has a superior representation for rare words.  Brown makes  a single hard clustering decision, whereas the  embedding for a rare word is close to its initial Table 2 shows the final chunking results and Ta-value since it hasn't received many training  ble 3 shows the final NER F1 results. We compare updates (see Footnote 2). Figure 3 shows the total to the state-of-the-art methods of Ando and Zhang number of per-token errors incurred on the test  (2005), Suzuki and Isozaki (2008), and for  set, depending upon the frequency of the word  NER Lin and Wu (2009). Tables 2 and 3 show  token in the unlabeled data. For NER, Figure 3 (b) that accuracy can be increased further by combin-shows that most errors occur on rare words, and  ing the features from different types of word rep-that Brown clusters do indeed incur fewer errors resentations. But, if only one word representation for rare words.  This supports our hypothesis  is to be used, Brown clusters have the highest ac-that, for rare words, Brown clustering produces  curacy. Given the improvements to the C&W em-better representations than word embeddings that beddings since Turian et al. (2009), C&W em-haven't received sufficient training updates. For beddings outperform the HLBL embeddings. On  chunking, Brown clusters and C&W embeddings  chunking, there is only a minute difference  beincur almost identical numbers of errors, and  tween Brown clusters and the embeddings.  Comerrors are concentrated around the more common  words. We hypothesize that non-rare words have and that jointly learns the supervised and unsu-good representations, regardless of the choice  pervised tasks (Ando & Zhang, 2005; Suzuki & of word representation technique. For tasks like Isozaki, 2008; Suzuki et al., 2009).  chunking in which a syntactic decision relies upon Unsupervised word representations have been  looking at several token simultaneously,  comused in previous NLP work, and have  demonpound features that use the word representations strated improvements in generalization accuracy  might increase accuracy more (Koo et al., 2008).  on a variety of tasks. Ours is the first work to Using word representations in NER brought  systematically compare different word  reprelarger gains on the out-of-domain data than on the sentations in a controlled way.  We found that  in-domain data. We were surprised by this result, Brown clusters and word embeddings both can  because the OOD data was not even used during  improve the accuracy of a near-state-of-the-art  the unsupervised word representation induction,  supervised NLP system. We also found that  comas was the in-domain data.  We are curious to  bining different word representations can improve investigate this phenomenon further.  accuracy further.  Error analysis indicates that  Ando and Zhang (2005) present a  semiBrown clustering induces better representations  supervised learning algorithm called alternating for rare words than C&W embeddings that have structure optimization (ASO). They find a low-not received many training updates.  dimensional projection of the input features that Another contribution of our work is a default  gives good linear classifiers over auxiliary tasks.  method for setting the scaling parameter for  These auxiliary tasks are sometimes specific  word embeddings. With this contribution, word  to the supervised task, and sometimes general  embeddings can now be used off-the-shelf as  language modeling tasks like predict the missing word features, with no tuning.  word . Suzuki and Isozaki (2008) present a  semiFuture work should explore methods for  supervised extension of CRFs. (In Suzuki et al.  inducing phrase representations, as well as  tech(2009), they extend their semi-supervised  apniques for increasing in accuracy by using word  proach to more general conditional models.) One  representations in compound features.  of the advantages of the semi-supervised learning Replicating our experiments  approach that we use is that it is simpler and more general than that of Ando and Zhang (2005) and  projects/wordreprs/  Suzuki and Isozaki (2008). Their methods dictate to find:  The word  a particular choice of model and training regime representations we induced,  which you can  and could not, for instance, be used with an NLP  download and use in your experiments; The code  system based upon an SVM classifier.  for inducing the word representations, which you can use to induce word representations on your  Lin and Wu (2009) present a K-means-like  own data; The NER and chunking system, with  non-hierarchical clustering algorithm for phrases, code for replicating our experiments.  which uses MapReduce.  Since they can scale  to millions of phrases, and they train over 800B  Acknowledgments  unlabeled words, they achieve state-of-the-art  Thank you to Magnus Sahlgren, Bob Carpenter,  accuracy on NER using their phrase clusters.  Percy Liang, Alexander Yates, and the anonymous  This suggests that extending word  representareviewers for useful discussion.  Thank you to  tions to phrase representations is worth further Andriy Mnih for inducing his embeddings on  investigation.  RCV1 for us. Joseph Turian and Yoshua Bengio  acknowledge the following agencies for  research funding and computing support: NSERC,  Word features can be learned in advance in an  RQCHP, CIFAR. Lev Ratinov was supported by  unsupervised, task-inspecific, and model-agnostic the Air Force Research Laboratory (AFRL) under  manner. These word features, once learned, are  easily disseminated with other researchers, and  opinions, findings, and conclusion or  recommeneasily integrated into existing supervised NLP  dations expressed in this material are those of the systems. The disadvantage, however, is that ac-author and do not necessarily reflect the view of curacy might not be as high as a semi-supervised the Air Force Research Laboratory (AFRL).  method that includes task-specific information  A  highperformance semi-supervised learning method  Self-organizing maps of  for text chunking. ACL.  words for natural language processing  applications. Proceedings of the International ICSC  Symposium on Soft Computing.  Scholarpedia, 3, 3881.  Bengio, Y., Ducharme, R., & Vincent, P. (2001).  Contextual relations of words in grimm tales,  analyzed by self-organizing map. ICANN.  Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, Huang, F., & Yates, A. (2009). Distributional rep-C. (2003).  resentations for handling sparsity in supervised model. Journal of Machine Learning Research,  Dimensionality reduction by  Bengio, Y., Louradour, J., Collobert, R., &  random mapping: Fast similarity computation  Weston, J. (2009). Curriculum learning. ICML.  for clustering. IJCNN (pp. 413 418).  Bengio, Y., & S n cal, J.-S. (2003). Quick train-Koo, T., Carreras, X., & Collins, M. (2008).  ing of probabilistic neural nets by importance  Simple semi-supervised dependency parsing.  Latent dirichlet allocation. Journal of Machine  effective two-stage model for exploiting  nonLearning Research, 3, 993 1022.  local dependencies in named entity recognition.  Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra, COLING-ACL.  An introduction to latent semantic analysis.  Linguistics, 18, 467 479.  Candito, M., & Crabb , B. (2009). Improving gen-Li, W., & McCallum, A. (2005). Semi-supervised erative statistical parsing with semi-supervised sequence modeling with syntactic topic models.  word clustering. IWPT (pp. 138 141).  Collobert, R., & Weston, J. (2008).  A unified  Semi-supervised learning  architecture for natural language processing:  for natural language.  Master's thesis,  MasDeep neural networks with multitask learning.  sachusetts Institute of Technology.  Phrase clustering  for discriminative learning. ACL-IJCNLP (pp.  supervised semantic role labeling using the  Latent Words Language Model. EMNLP (pp.  Producing  highdimensional semantic spaces from lexical  Behavior Research Methods,  Deerwester, S., & Harshman, R. (1988). Using Instrumentation, and Computers, 28, 203 208.  latent semantic analysis to improve access to  textual information.  SIGCHI Conference on  Semantic and associative priming in  highHuman Factors in Computing Systems (pp.  281 285). ACM.  Proceedings, LEA (pp. 660 665).  Elman, J. L. (1993). Learning and development  Martin, S., Liermann, J., & Ney, H. (1998). Algo-in neural networks: The importance of starting  rithms for bigram and trigram word clustering.  Speech Communication, 24, 19 37.  Enhancing unlexicalized parsing  Name tagging with word clusters and  discrimperformance using a wide coverage lexicon,  inative training. HLT-NAACL (pp. 337 342).  Mnih, A., & Hinton, G. E. (2007).  Three  is more in unsupervised dependency parsing.  new graphical models for statistical language  Suzuki, J., & Isozaki, H. (2008). Semi-supervised Mnih, A., & Hinton, G. E. (2009). A scalable sequential labeling and segmentation using  hierarchical distributed language model. NIPS  Hierarchical  probabilistic neural network language model.  (2009). An empirical study of semi-supervised  structured conditional models for dependency  Pereira, F., Tishby, N., & Lee, L. (1993). Distributional clustering of english words. ACL (pp.  Turian, J., Ratinov, L., Bengio, Y., & Roth, D.  A preliminary evaluation of word  representations for named-entity recognition.  Ratinov, L., & Roth, D. (2009).  Design  chalNIPS Workshop on Grammar Induction,  Reprelenges and misconceptions in named entity  sentation of Language and Language Learning.  Turney, P. D., & Pantel, P. (2010). From frequency Ritter, H., & Kohonen, T. (1989). Self-organizing to meaning: Vector space models of semantics.  Biological Cybernetics,  Journal of Artificial Intelligence Research.  Hierarchical clustering of  Vector-based semantic  analysis: Representing word meanings based  on random labels. Proceedings of the Semantic  ison of independent component analysis and  Knowledge Acquisition and Categorisation  singular value decomposition in word context  Workshop, ESSLLI.  analysis. AKRR'05, International and  InterdisSahlgren, M. (2005). An introduction to random  ciplinary Conference on Adaptive Knowledge  indexing. Methods and Applications of  SemanRepresentation and Reasoning.  tic Indexing Workshop at the 7th International  V yrynen, J. J., & Honkela, T. (2004). Word cat-Conference on Terminology and Knowledge  egory maps based on emergent features created  Engineering (TKE).  by ICA. Proceedings of the STeP'2004  CogniThe word-space model:  tion + Cybernetics Symposium (pp. 173 185).  Using distributional analysis to represent  synFinnish Artificial Intelligence Society.  tagmatic and paradigmatic relations between  high-dimensional vector  Towards explicit semantic features  Doctoral dissertation, Stockholm University.  using independent component analysis.  Sang, E. T., & Buchholz, S. (2000). Introduction ceedings of the Workshop Semantic Content  to the CoNLL-2000 shared task: Chunking.  Acquisition and Representation (SCAR).  StockCoNLL.  holm, Sweden: Swedish Institute of Computer  eh u ek, R., & Sojka, P. (2010). Software frame-tionist language modeling for large vocabulary  work for topic modelling with large corpora.  continuous speech recognition.  International  Conference on Acoustics, Speech and Signal  Processing (ICASSP) (pp. 765 768). Orlando,  minimization based named entity recognition  low parsing with conditional random fields.  Multilingual dependency learning: a  huge feature engineering method to semantic  dependency parsing. CoNLL (pp. 55 60).  (2010). From baby steps to leapfrog: How less 