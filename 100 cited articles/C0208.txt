 Learning Extraction Patterns for Subjective Expressions  School of Computing  Department of Computer Science  University of Utah  University of Pittsburgh  Salt Lake City, UT 84112  must recognize rants and emotional tirades, among other things. In general, nearly any system that seeks to iden-This paper presents a bootstrapping process  tify information could benefit from being able to separate that learns linguistically rich extraction pat-factual and subjective information.  terns for subjective (opinionated) expressions.  Some existing resources contain lists of subjective  High-precision classifiers label unannotated  words (e.g., Levin's desire verbs (1993)), and some em-data to automatically create a large training set,  pirical methods in NLP have automatically identified ad-which is then given to an extraction pattern  jectives, verbs, and N-grams that are statistically associ-learning algorithm. The learned patterns are  ated with subjective language (e.g., (Turney, 2002; Hatzi-then used to identify more subjective sentences.  vassiloglou and McKeown, 1997; Wiebe, 2000; Wiebe  The bootstrapping process learns many  subjecet al., 2001)). However, subjective language can be extive patterns and increases recall while  mainhibited by a staggering variety of words and phrases. In taining high precision.  addition, many subjective terms occur infrequently, such as strongly subjective adjectives (e.g., preposterous, un-1  Introduction  seemly) and metaphorical or idiomatic phrases (e.g., dealt a blow, swept off one's feet). Consequently, we believe Many natural language processing applications could  that subjectivity learning systems must be trained on ex-benefit from being able to distinguish between factual tremely large text collections before they will acquire a and subjective information.  Subjective remarks come  subjective vocabulary that is truly broad and comprehen-in a variety of forms, including opinions, rants, allega-sive in scope.  tions, accusations, suspicions, and speculations. Ideally, To address this issue, we have been exploring the use  information extraction systems should be able to distin-of bootstrapping methods to allow subjectivity classifiers guish between factual information (which should be ex-to learn from a collection of unannotated texts. Our re-tracted) and non-factual information (which should be  search uses high-precision subjectivity classifiers to au-discarded or labeled as uncertain). Question answering tomatically identify subjective and objective sentences in systems should distinguish between factual and specula-unannotated texts. This process allows us to generate a tive answers. Multi-perspective question answering aims large set of labeled sentences automatically. The sec-to present multiple answers to the user based upon specu-ond emphasis of our research is using extraction patterns lation or opinions derived from different sources. Multi-to represent subjective expressions. These patterns are document summarization systems need to summarize dif-linguistically richer and more flexible than single words ferent opinions and perspectives. Spam filtering systems or N-grams. Using the (automatically) labeled sentences  This work was supported by the National Science Founda-as training data, we apply an extraction pattern learning tion under grants IIS-0208798, IIS-0208985, and IRI-9704240.  algorithm to automatically generate patterns represent-The data preparation was performed in support of the North-ing subjective expressions. The learned patterns can be east Regional Research Center (NRRC) which is sponsored by used to automatically identify more subjective sentences, the Advanced Research and Development Activity (ARDA), a which grows the training set, and the entire process can U.S. Government entity which sponsors and promotes research of import to the Intelligence Community which includes but is then be bootstrapped. Our experimental results show that not limited to the CIA, DIA, NSA, NIMA, and NRO.  this bootstrapping process increases the recall of the  highprecision subjective sentence classifier with little loss in jective or objective. Previous work on sentence-level sub-precision. We also find that the learned extraction pat-jectivity classification (Wiebe et al., 1999) used training terns capture subtle connotations that are more expressive corpora that had been manually annotated for subjectiv-than the individual words by themselves.  ity. Manually producing annotations is time consuming, This paper is organized as follows. Section 2 discusses so the amount of available annotated sentence data is rel-previous work on subjectivity analysis and extraction pat-atively small.  tern learning. Section 3 overviews our general approach, The goal of our research is to use high-precision sub-describes the high-precision subjectivity classifiers, and jectivity classifiers to automatically identify subjective explains the algorithm for learning extraction patterns as-and objective sentences in unannotated text corpora. The sociated with subjectivity. Section 4 describes the data high-precision classifiers label a sentence as subjective or that we use, presents our experimental results, and shows objective when they are confident about the classification, examples of patterns that are learned. Finally, Section 5  and they leave a sentence unlabeled otherwise.  Unannosummarizes our findings and conclusions.  tated texts are easy to come by, so even if the classifiers can label only 30% of the sentences as subjective or ob-2  Background  jective, they will still produce a large collection of labeled sentences. Most importantly, the high-precision classi-2.1  Subjectivity Analysis  fiers can generate a much larger set of labeled sentences Much previous work on subjectivity recognition has fo-than are currently available in manually created data sets.  cused on document-level classification.  For example,  Extraction Patterns  (Spertus, 1997) developed a system to identify inflamma-tory texts and (Turney, 2002; Pang et al., 2002) developed Information extraction (IE) systems typically use lexico-methods for classifying reviews as positive or negative.  syntactic patterns to identify relevant information. The Some research in genre classification has included the specific representation of these patterns varies across sys-recognition of subjective genres such as editorials (e.g., tems, but most patterns represent role relationships  sur(Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe rounding noun and verb phrases. For example, an IE  system designed to extract information about hijackings In contrast, the goal of our work is to classify individ-might use the pattern hijacking of < x>, which looks for ual sentences as subjective or objective. Document-level the noun hijacking and extracts the object of the prepo-classification can distinguish between subjective texts , sition of as the hijacked vehicle. The pattern < x> was such as editorials and reviews, and objective texts, such hijacked would extract the hijacked vehicle when it finds as newspaper articles. But in reality, most documents  the verb hijacked in the passive voice, and the pattern contain a mix of both subjective and objective sentences.  < x> hijacked would extract the hijacker when it finds Subjective texts often include some factual information.  the verb hijacked in the active voice.  For example, editorial articles frequently contain factual One of our hypotheses was that extraction patterns  information to back up the arguments being made, and  would be able to represent subjective expressions that movie reviews often mention the actors and plot of a  have noncompositional meanings. For example, consider  movie as well as the theatres where it's currently playing.  the common expression drives (someone) up the wall, Even if one is willing to discard subjective texts in their which expresses the feeling of being annoyed with some-entirety, the objective texts usually contain a great deal of thing. The meaning of this expression is quite different subjective information in addition to facts. For example, from the meanings of its individual words ( drives, up, newspaper articles are generally considered to be rela-wall). Furthermore, this expression is not a fixed word tively objective documents, but in a recent study (Wiebe sequence that could easily be captured by N-grams. It is et al., 2001) 44% of sentences in a news collection were a relatively flexible construction that may be more gener-found to be subjective (after editorial and review articles ally represented as < x> drives < y> up the wall, where x were removed).  and y may be arbitrary noun phrases. This pattern would One of the main obstacles to producing a sentence-match many different sentences, such as George drives level subjectivity classifier is a lack of training data. To me up the wall, She drives the mayor up the wall,  train a document-level classifier, one can easily find col-or The nosy old man drives his quiet neighbors up the lections of subjective texts, such as editorials and reviews.  For example, (Pang et al., 2002) collected reviews from We also wondered whether the extraction pattern repa movie database and rated them as positive, negative, or resentation might reveal slight variations of the same verb neutral based on the rating (e.g., number of stars) given or noun phrase that have different connotations. For ex-by the reviewer. It is much harder to obtain collections of ample, you can say that a comedian bombed last night, individual sentences that can be easily identified as sub-which is a subjective statement, but you can't express  this sentiment with the passive voice of bombed. In Sec-terns). These patterns are then used to identify more sen-tion 3.2, we will show examples of extraction patterns tences within the unannotated texts that can be classified representing subjective expressions which do in fact ex-as subjective. The extraction pattern learner can then re-hibit both of these phenomena.  train using the larger training set and the process repeats.  A variety of algorithms have been developed to  auThe subjective patterns can also be added to the  hightomatically learn extraction patterns.  Most of these  precision subjective sentence classifier as new features to algorithms require special training resources, such as improve its performance. The dashed lines in Figure 1  texts annotated with domain-specific tags (e.g.,  Aurepresent the parts of the process that are bootstrapped.  In this section, we will describe the high-precision sen-1995), RAPIER (Califf, 1998), SRV (Freitag, 1998),  tence classifiers, the extraction pattern learning process, WHISK (Soderland, 1999)) or manually defined key-and the details of the bootstrapping process.  words, frames, or object recognizers (e.g., PALKA (Kim and Moldovan, 1993) and LIEP (Huffman, 1996)).  High-Precision Subjectivity Classifiers  AutoSlog-TS (Riloff, 1996) takes a different approach, The high-precision classifiers (HP-Subj and HP-Obj) use requiring only a corpus of unannotated texts that have lists of lexical items that have been shown in previous been separated into those that are related to the target do-work to be good subjectivity clues. Most of the items are main (the relevant texts) and those that are not (the ir-single words, some are N-grams, but none involve syntac-relevant texts). Most recently, two bootstrapping algo-tic generalizations as in the extraction patterns. Any data rithms have been used to learn extraction patterns. Meta-used to develop this vocabulary does not overlap with the bootstrapping (Riloff and Jones, 1999) learns both extrac-test sets or the unannotated data used in this paper.  tion patterns and a semantic lexicon using unannotated Many of the subjective clues are from manually de-texts and seed words as input. ExDisco (Yangarber et al., veloped resources, including entries from (Levin, 1993; 2000) uses a bootstrapping mechanism to find new ex-Ballmer and Brennenstuhl, 1981), Framenet lemmas with  traction patterns using unannotated texts and some seed frame element experiencer (Baker et al., 1998), adjec-patterns as the initial input.  tives manually annotated for polarity (Hatzivassiloglou For our research, we adopted a learning process very  and McKeown, 1997), and subjectivity clues listed in  similar to that used by AutoSlog-TS, which requires only (Wiebe, 1990). Others were derived from corpora, in-relevant texts and irrelevant texts as its input. We describe cluding subjective nouns learned from unannotated data this learning process in more detail in the next section.  using bootstrapping (Riloff et al., 2003).  The subjectivity clues are divided into those that are 3  Learning and Bootstrapping Extraction  strongly subjective and those that are weakly subjective, Patterns for Subjectivity  using a combination of manual review and empirical  results on a small training set of manually annotated data.  We have developed a bootstrapping process for  subjecAs the terms are used here, a strongly subjective clue is tivity classification that explores three ideas: (1) high-one that is seldom used without a subjective meaning,  precision classifiers can be used to automatically iden-whereas a weakly subjective clue is one that commonly  tify subjective and objective sentences from unannotated has both subjective and objective uses.  texts, (2) this data can be used as a training set to auto-The high-precision subjective classifier classifies a sen-matically learn extraction patterns associated with subtence as subjective if it contains two or more of the  jectivity, and (3) the learned patterns can be used to grow strongly subjective clues. On a manually annotated test the training set, allowing this entire process to be boot-set, this classifier achieves 91.5% precision and 31.9%  strapped.  recall (that is, 91.5% of the sentences that it selected are Figure 1 shows the components and layout of the boot-subjective, and it found 31.9% of the subjective sentences strapping process. The process begins with a large collec-in the test set). This test set consists of 2197 sentences, tion of unannotated text and two high precision subjec-59% of which are subjective.  tivity classifiers. One classifier searches the unannotated The high-precision objective classifier takes a different corpus for sentences that can be labeled as subjective approach. Rather than looking for the presence of lexical with high confidence, and the other classifier searches items, it looks for their absence. It classifies a sentence as for sentences that can be labeled as objective with high objective if there are no strongly subjective clues and at confidence. All other sentences in the corpus are left most one weakly subjective clue in the current, previous, unlabeled. The labeled sentences are then fed to an ex-and next sentence combined. Why doesn't the objective  traction pattern learner, which produces a set of extrac-classifier mirror the subjective classifier, and consult its tion patterns that are statistically correlated with the sub-own list of strongly objective clues? There are certainly jective sentences (we will call these the subjective pat-lexical items that are statistically correlated with the  obUnannotated Text Collection  subjective patterns  High Precision Subjective  Sentence Classifier (HP Subj)  Known Subjective  Vocabulary  High Precision Objective  Extraction Pattern  Sentence Classifier (HP Obj)  Learner  subjective patterns  Pattern based Subjective  Sentence Classifier  Figure 1: Bootstrapping Process  jective class (examples are cardinal numbers (Wiebe et right column shows a specific extraction pattern that was al., 1999), and words such as per, case, market, and to-learned during our subjectivity experiments as an instantal), but the presence of such clues does not readily lead tiation of the syntactic form on the left. For example, the to high precision objective classification. Add sarcasm pattern < subj> was satisfied 1 will match any sentence or a negative evaluation to a sentence about a dry topic where the verb satisfied appears in the passive voice. The such as stock prices, and the sentence becomes subjec-pattern < subj> dealt blow represents a more complex extive. Conversely, add objective topics to a sentence con-pression that will match any sentence that contains a verb taining two strongly subjective words such as odious and phrase with head= dealt followed by a direct object with scumbag, and the sentence remains subjective.  head= blow. This would match sentences such as The The performance of the high-precision objective classi-experience dealt a stiff blow to his pride. It is important fier is a bit lower than the subjective classifier: 82.6% pre-to recognize that these patterns look for specific syntactic cision and 16.4% recall on the test set mentioned above constructions produced by a (shallow) parser, rather than (that is, 82.6% of the sentences selected by the objective exact word sequences.  classifier are objective, and the objective classifier found 16.4% of the objective sentences in the test set). Al-SYNTACTIC FORM  though there is room for improvement, the performance  <subj> was satisfied  proved to be good enough for our purposes.  <subj> complained  <subj> dealt blow  <subj> appear to be  Learning Subjective Extraction Patterns  <subj> has position  To automatically learn extraction patterns that are associated with subjectivity, we use a learning algorithm similar active-verb < dobj>  to AutoSlog-TS (Riloff, 1996). For training,  AutoSlogTS uses a text corpus consisting of two distinct sets of noun aux < dobj>  fact is <dobj>  texts: relevant texts (in our case, subjective sentences) and irrelevant texts (in our case, objective sentences).  A set of syntactic templates represents the space of pos-active-verb prep < np>  agrees with <np>  was worried about <np>  to resort to <np>  The learning process has two steps. First, the  syntactic templates are applied to the training corpus in an ex-Figure 2: Syntactic Templates and Examples of Patterns haustive fashion, so that extraction patterns are generated that were Learned  for (literally) every possible instantiation of the templates that appears in the corpus. The left column of Figure 2  shows the syntactic templates used by AutoSlog-TS. The 1This is a shorthand notation for the internal representation.  someone about a specific opinion. For example, here is  one such sentence from our training set: Ernest Bai  Koroma of RITCORP was asked to address his supporters on  talk of <np>  his views relating to full blooded Temne to head APC'.  In contrast, many of the sentences containing asked in  <subj> put an end  the active voice are more general in nature, such as The  mayor asked a newly formed JR about his petition.  <subj> is going to be  Figure 3 also shows that expressions using talk as a  was expected from <np>  noun (e.g., Fred is the talk of the town ) are highly  cor<subj> was expected  related with subjective sentences, while talk as a verb  <subj> is fact  (e.g., The mayor will talk about... ) are found in a mix fact is <dobj>  of subjective and objective sentences. Not surprisingly, longer expressions tend to be more idiomatic (and sub-Figure 3: Patterns with Interesting Behavior  jective) than shorter expressions (e.g., put an end (to) vs.  put; is going to be vs. is going; was expected from vs. was The second step of AutoSlog-TS's learning process ap-expected). Finally, the last two rows of Figure 3 show that plies all of the learned extraction patterns to the train-expressions involving the noun fact are highly correlated ing corpus and gathers statistics for how often each  with subjective expressions! These patterns match  senpattern occurs in subjective versus objective sentences.  tences such as The fact is... and ... is a fact, which appar-AutoSlog-TS then ranks the extraction patterns using a ently are often used in subjective contexts. This example metric called RlogF (Riloff, 1996) and asks a human to illustrates that the corpus-based learning method can find review the ranked list and make the final decision about phrases that might not seem subjective to a person intu-which patterns to keep.  itively, but that are reliable indicators of subjectivity.  In contrast, for this work we wanted a fully automatic process that does not depend on a human reviewer, and  we were most interested in finding patterns that can identify subjective expressions with high precision. So we 4.1  Subjectivity Data  ranked the extraction patterns using a conditional proba-The text collection that we used consists of  Englishbility measure: the probability that a sentence is subjec-language versions of foreign news documents from FBIS, tive given that a specific extraction pattern appears in it.  the U.S. Foreign Broadcast Information Service. The  The exact formula is:  data is from a variety of countries. Our system takes  unannotated data as input, but we needed annotated data f req(patterni)  to evaluate its performance. We briefly describe the man-where subjf req(patterni) is the frequency of patterni ual annotation scheme used to create the gold-standard, in subjective training sentences, and f req(patterni) is and give interannotator agreement results.  the frequency of patterni in all training sentences. (This In 2002, a detailed annotation scheme (Wilson and  may also be viewed as the precision of the pattern on the Wiebe, 2003) was developed for a government-sponsored  training data.) Finally, we use two thresholds to select ex-project.  We only mention aspects of the annotation  traction patterns that are strongly associated with subjec-scheme relevant to this paper. The scheme was inspired tivity in the training data. We choose extraction patterns by work in linguistics and literary theory on subjectiv-for which f req(patterni) 1 and P r(subjective |  ity, which focuses on how opinions, emotions, etc. are patterni) 2.  expressed linguistically in context (Banfield, 1982). The Figure 3 shows some patterns learned by our system,  goal is to identify and characterize expressions of private the frequency with which they occur in the training data states in a sentence. Private state is a general covering (FREQ) and the percentage of times they occur in sub-term for opinions, evaluations, emotions, and  speculajective sentences (%SUBJ). For example, the first two tions (Quirk et al., 1985). For example, in sentence (1) rows show the behavior of two similar expressions us-the writer is expressing a negative evaluation.  ing the verb asked. 100% of the sentences that contain asked in the passive voice are subjective, but only 63%  (1) The time has come, gentlemen, for Sharon, the  asof the sentences that contain asked in the active voice are sassin, to realize that injustice cannot last long.  subjective. A human would probably not expect the  active and passive voices to behave so differently. To un-Sentence (2) reflects the private state of Western coun-derstand why this is so, we looked in the training data tries. Mugabe's use of overwhelmingly also reflects a pri-and found that the passive voice is often used to query vate state, his positive reaction to and characterization of  his victory.  (2) Western countries were left frustrated and impotent after Robert Mugabe formally declared that he had overwhelmingly won Zimbabwe's presidential election.  Annotators are also asked to judge the strength of each private state. A private state may have low, medium, high or extreme strength.  To allow us to measure interannotator agreement, three annotators (who are not authors of this paper) indepen-dently annotated the same 13 documents with a total of 210 sentences. We begin with a strict measure of agreement at the sentence level by first considering whether the annotator marked any private-state expression, of any strength, anywhere in the sentence. If so, the sentence is subjective. Otherwise, it is objective. The average pairwise percentage agreement is 90% and the average  pairFigure 4: Evaluating the Learned Patterns on Test Data wise value is 0.77.  One would expect that there are clear cases of  objecsentences, 54% of which are subjective.  tive sentences, clear cases of subjective sentences, and Figure 4 shows sentence recall and pattern (instance-borderline sentences in between. The agreement study  level) precision for the learned extraction patterns on the supports this. In terms of our annotations, we define a test set. In this figure, precision is the proportion of pat-sentence as borderline if it has at least one private-state tern instances found in the test set that are in subjective expression identified by at least one annotator, and all sentences, and recall is the proportion of subjective sen-strength ratings of private-state expressions are low. On tences that contain at least one pattern instance.  average, 11% of the corpus is borderline under this def-inition. When those sentences are removed, the average We evaluated 18 different subsets of the patterns, by  pairwise percentage agreement increases to 95% and the selecting the patterns that pass certain thresholds in the average pairwise value increases to 0.89.  training data. We tried all combinations of 1 = {2,10}  As expected, the majority of disagreement cases  inand 2 = {.60,.65,.70,.75,.80,.85,.90,.95,1.0}. The data volve low-strength subjectivity. The annotators consis-points corresponding to 1=2 are shown on the upper line tently agree about which are the clear cases of subjective in Figure 4, and those corresponding to 1=10 are shown sentences. This leads us to define the gold-standard that on the lower line. For example, the data point correspond-we use when evaluating our results. A sentence is subjec-ing to 1=10 and 2=.90 evaluates only the extraction pattive if it contains at least one private-state expression of terns that occur at least 10 times in the training data and medium or higher strength. The second class, which we  with a probability .90 (i.e., at least 90% of its occur-call objective, consists of everything else.  rences are in subjective training sentences).  Overall, the extraction patterns perform quite well.  Evaluation of the Learned Patterns  The precision ranges from 71% to 85%, with the expected Our pool of unannotated texts consists of 302,163 indi-tradeoff between precision and recall. This experiment vidual sentences. The HP-Subj classifier initially labeled confirms that the extraction patterns are effective at rec-roughly 44,300 of these sentences as subjective, and the ognizing subjective expressions.  HP-Obj classifier initially labeled roughly 17,000 sen-4.3  Evaluation of the Bootstrapping Process  tences as objective. In order to keep the training set relatively balanced, we used all 17,000 objective sentences In our second experiment, we used the learned extrac-and 17,000 of the subjective sentences as training data for tion patterns to classify previously unlabeled sentences the extraction pattern learner.  from the unannotated text collection. The new  subjec17,073 extraction patterns were learned that have  tive sentences were then fed back into the Extraction Pat-f requency 2 and P r(subjective | patterni) .60 on tern Learner to complete the bootstrapping cycle depicted the training data. We then wanted to determine whether by the rightmost dashed line in Figure 1. The Pattern-the extraction patterns are, in fact, good indicators of sub-based Subjective Sentence Classifier classifies a sentence jectivity. To evaluate the patterns, we applied different as subjective if it contains at least one extraction pattern subsets of them to a test set to see if they consistently oc-with 1 5 and 2 1.0 on the training data. This process cur in subjective sentences. This test set consists of 3947  produced approximately 9,500 new subjective sentences  that were previously unlabeled.  age point drop in precision. This result shows that the Since our bootstrapping process does not learn new ob-learned extraction patterns do improve the performance jective sentences, we did not want to simply add the new of the high-precision subjective sentence classifier, allow-subjective sentences to the training set, or it would being it to classify more sentences as subjective with nearly come increasingly skewed toward subjective sentences.  the same high reliability.  Since HP-Obj had produced roughly 17,000 objective  sentences used for training, we used the 9,500 new sub-HP-Subj  jective sentences along with 7,500 of the previously iden-Recall  Precision  Precision  tified subjective sentences as our new training set. In 32.9  other words, the training set that we used during the second bootstrapping cycle contained exactly the same  obTable 1: Bootstrapping the Learned Patterns into the  jective sentences as the first cycle, half of the same sub-High-Precision Sentence Classifier  jective sentences as the first cycle, and 9,500 brand new subjective sentences.  Table 2 gives examples of patterns used to augment the On this second cycle of bootstrapping, the extraction  HP-Subj classifier which do not overlap in non-function pattern learner generated many new patterns that were not words with any of the clues already known by the original discovered during the first cycle. 4,248 new patterns were system. For each pattern, we show an example sentence  found that have  from our corpus that matches the pattern.  1 2 and 2 .60. If we consider only the  strongest (most subjective) extraction patterns, 308 new 5  patterns were found that had 1 10 and 2 1.0. This is a substantial set of new extraction patterns that seem to This research explored several avenues for improving the be very highly correlated with subjectivity.  state-of-the-art in subjectivity analysis. First, we demon-An open question was whether the new patterns  prostrated that high-precision subjectivity classification can vide additional coverage. To assess this, we did a sim-be used to generate a large amount of labeled training data ple test: we added the 4,248 new patterns to the origi-for subsequent learning algorithms to exploit. Second, we nal set of patterns learned during the first bootstrapping showed that an extraction pattern learning technique can cycle. Then we repeated the same analysis that we de-learn subjective expressions that are linguistically richer pict in Figure 4. In general, the recall numbers increased than individual words or fixed phrases. We found that  by about 2-4% while the precision numbers decreased by similar expressions may behave very differently, so that less, from 0.5-2%.  one expression may be strongly indicative of subjectivity In our third experiment, we evaluated whether the  but the other may not. Third, we augmented our  origilearned patterns can improve the coverage of the  highnal high-precision subjective classifier with these newly precision subjectivity classifier (HP-Subj), to complete learned extraction patterns. This bootstrapping process the bootstrapping loop depicted in the top-most dashed resulted in substantially higher recall with a minimal loss line of Figure 1. Our hope was that the patterns would al-in precision. In future work, we plan to experiment with low more sentences from the unannotated text collection different configurations of these classifiers, add new sub-to be labeled as subjective, without a substantial drop in jective language learners in the bootstrapping process, precision. For this experiment, we selected the learned and address the problem of how to identify new objec-extraction patterns that had 1 10 and 2 1.0 on the tive sentences during bootstrapping.  training set, since these seemed likely to be the most reliable (high precision) indicators of subjectivity.  Acknowledgments  We modified the HP-Subj classifier to use extraction  We are very grateful to Theresa Wilson for her invaluable patterns as follows. All sentences labeled as subjective programming support and help with data preparation.  by the original HP-Subj classifier are also labeled as subjective by the new version. For previously unlabeled sentences, the new version classifies a sentence as subjective References  if (1) it contains two or more of the learned patterns, or C. Baker, C. Fillmore, and J. Lowe.  The Berkeley  (2) it contains one of the clues used by the original HP-FrameNet Project. In Proceedings of the COLING-ACL-98.  Subj classifier and at least one learned pattern. Table 1  shows the performance results on the test set mentioned T. Ballmer and W. Brennenstuhl. 1981. Speech Act Classifi-in Section 3.1 (2197 sentences) for both the original HP-cation: A Study in the Lexical Analysis of English Speech Subj classifier and the new version that uses the learned Activity Verbs. Springer-Verlag.  extraction patterns. The extraction patterns produce a 7.2  A. Banfield. 1982. Unspeakable Sentences. Routledge and percentage point gain in coverage, and only a 1.1 percent-Kegan Paul, Boston.  I am pleased that there now seems to be broad political consensus . . .  Jiang's subdued tone . . . underlined his desire to avoid disputes . . .  pretext of <np>  On the pretext of the US opposition . . .  atmosphere of <np>  Terrorism thrives in an atmosphere of hate . . .  These are fine words, but they do not reflect the reality . . .  The pictures resemble an attempt to satisfy a primitive thirst for revenge . . .  way with <np>  . . . to ever let China use force to have its way with . . .  bring about <np>  Everything must be done by everyone to bring about de-escalation, Mr Chirac added.  expense of <np>  at the expense of the world's security and stability voiced <dobj>  Khatami . . . voiced Iran's displeasure.  turn into <np>  . . . the surging epidemic could turn into a national security threat, he said.  Table 2: Examples of Learned Patterns Used by HP-Subj and Sample Matching Sentences M. E. Califf. 1998. Relational Learning Techniques for Natural E. Riloff. 1993. Automatically Constructing a Dictionary for Language Information Extraction. Ph.D. thesis, Tech. Rept.  Information Extraction Tasks. In Proceedings of the AAAI-AI98-276, Artificial Intelligence Laboratory, The University 93.  of Texas at Austin.  E. Riloff. 1996. Automatically Generating Extraction Patterns Dayne Freitag. 1998. Toward General-Purpose Learning for from Untagged Text. In Proceedings of the AAAI-96.  Information Extraction. In Proceedings of the ACL-98.  V. Hatzivassiloglou and K. McKeown. 1997. Predicting the CRYSTAL: Inducing a Conceptual Dictionary. In Proceed-Semantic Orientation of Adjectives. In Proceedings of the ings of the IJCAI-95.  S. Soderland. 1999. Learning Information Extraction Rules for S. Huffman.  Learning information extraction  patSemi-Structured and Free Text. Machine Learning, 34(1-terns from examples.  and Gabriele Scheler, editors, Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language E. Spertus. 1997. Smokey: Automatic Recognition of Hostile Processing, pages 246 260. Springer-Verlag, Berlin.  Messages. In Proceedings of the IAAI-97.  J. Karlgren and D. Cutting. 1994. Recognizing Text Genres P. Turney. 2002. Thumbs Up or Thumbs Down? Semantic Ori-with Simple Metrics Using Discriminant Analysis. In Pro-entation Applied to Unsupervised Classification of Reviews.  ceedings of the COLING-94.  In Proceedings of the ACL-02.  J. Wiebe, R. Bruce, and T. O'Hara. 1999. Development and B. Kessler, G. Nunberg, and H. Sch tze. 1997. Automatic De-Use of a Gold Standard Data Set for Subjectivity Classifica-tection of Text Genre. In Proceedings of the ACL-EACL-97.  tions. In Proceedings of the ACL-99.  J. Kim and D. Moldovan. 1993. Acquisition of Semantic Pat-J. Wiebe, T. Wilson, and M. Bell. 2001. Identifying Collo-terns for Information Extraction from Corpora. In Proceed-cations for Recognizing Opinions.  In Proceedings of the  ings of the Ninth IEEE Conference on Artificial Intelligence ACL-01 Workshop on Collocation: Computational Extrac-for Applications.  tion, Analysis, and Exploitation.  Beth Levin. 1993. English Verb Classes and Alternations: A J. Wiebe. 1990. Recognizing Subjective Sentences: A Compu-Preliminary Investigation. University of Chicago Press.  tational Investigation of Narrative Text. Ph.D. thesis, State B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sen-University of New York at Buffalo.  timent Classification Using Machine Learning Techniques.  J. Wiebe. 2000. Learning Subjective Adjectives from Corpora.  In Proceedings of the EMNLP-02.  In Proceedings of the AAAI-00.  R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A T. Wilson and J. Wiebe. 2003. Annotating Opinions in the Comprehensive Grammar of the English Language. Long-World Press. In Proceedings of the ACL SIGDIAL-03.  man, New York.  E. Riloff and R. Jones. 1999. Learning Dictionaries for In-2000. Automatic Acquisiton of Domain Knowledge for  Information Extraction by Multi-Level Bootstrapping. In Pro-formation Extraction. In Proceedings of COLING 2000.  ceedings of the AAAI-99.  E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning Subjective Nouns using Extraction Pattern Bootstrapping. In Proceedings of the Seventh Conference on Computational Natural Language Learning (CoNLL-03). 