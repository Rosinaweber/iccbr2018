 Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales  Bo Pang and Lillian Lee  (1) Department of Computer Science, Cornell University (2) Language Technologies Institute, Carnegie Mellon University (3) Computer Science Department, Carnegie Mellon University Abstract  become aware of the scientific challenges posed and  the scope of new applications enabled by the  proWe address the rating-inference problem,  cessing of subjective language. (The papers  colwherein rather than simply decide whether  lected by Qu, Shanahan, and Wiebe (2004) form a  a review is thumbs up or thumbs  representative sample of research in the area.) Most  down , as in previous sentiment  analyprior work on the specific problem of categorizing  sis work, one must determine an author's  expressly opinionated text has focused on the  bievaluation with respect to a multi-point  nary distinction of positive vs. negative (Turney,  scale (e.g., one to five stars ). This task  2002; Pang, Lee, and Vaithyanathan, 2002; Dave,  represents an interesting twist on  stanLawrence, and Pennock, 2003; Yu and  Hatzivassiloglou, 2003). But it is often helpful to have more cause there are several different degrees  information than this binary distinction provides, es-of similarity between class labels; for  expecially if one is ranking items by recommendation  ample, three stars is intuitively closer to  or comparing several reviewers' opinions: example  four stars than to one star .  applications include collaborative filtering and  deWe first evaluate human performance at  ciding which conference submissions to accept.  the task.  Then, we apply a  metaTherefore, in this paper we consider generalizing  algorithm, based on a metric labeling  forto finer-grained scales: rather than just determine mulation of the problem, that alters a  whether a review is thumbs up or not, we attempt  given -ary classifier's output in an  exto infer the author's implied numerical rating, such  plicit attempt to ensure that similar items  as three stars or four stars . Note that this differs receive similar labels.  We show that  from identifying opinion strength (Wilson, Wiebe, the meta-algorithm can provide signifi-and Hwa, 2004): rants and raves have the same  cant improvements over both multi-class  strength but represent opposite evaluations, and  refand regression versions of SVMs when we  eree forms often allow one to indicate that one is  employ a novel similarity measure  approvery confident (high strength) that a conference  subpriate to the problem.  mission is mediocre (middling rating). Also, our  Publication info: Proceedings of the  task differs from ranking not only because one can ACL, 2005.  be given a single item to classify (as opposed to a  set of items to be ordered relative to one another),  but because there are settings in which classification 1 Introduction  is harder than ranking, and vice versa.  There has recently been a dramatic surge of  interOne can apply standard -ary classifiers or  regresest in sentiment analysis, as more and more people sion to this rating-inference problem; independent  work by Koppel and Schler (2005) considers such  Rating diff.  Pooled  Subject 1  Subject 2  methods. But an alternative approach that  explicor more  itly incorporates information about item similarities 2 (e.g., 1 star)  together with label similarity information (for  instance, one star is closer to two stars than to  four stars ) is to think of the task as one of metric labeling (Kleinberg and Tardos, 2002), where Table 1: Human accuracy at determining relative  positivity. Rating differences are given in notches .  This observation yields a meta-algorithm, applicable  Parentheses enclose the number of pairs attempted.  to both semi-supervised (via graph-theoretic  techniques) and supervised settings, that alters a given  ratings not entirely supported by the text1.  -ary classifier's output so that similar items tend to For data, we first collected Internet movie reviews  be assigned similar labels.  in English from four authors, removing explicit  ratIn what follows, we first demonstrate that  huing indicators from each document's text  automatimans can discern relatively small differences in (hid-cally. Now, while the obvious experiment would be  den) evaluation scores, indicating that rating  inferto ask subjects to guess the rating that a review rep-ence is indeed a meaningful task. We then present  resents, doing so would force us to specify a fixed  three types of algorithms one-vs-all, regression,  rating-scale granularity in advance. Instead, we  exand metric labeling that can be distinguished by  amined people's ability to discern relative differ-how explicitly they attempt to leverage similarity  ences, because by varying the rating differences rep-between items and between labels. Next, we  conresented by the test instances, we can evaluate  mulsider what item similarity measure to apply,  proposing one based on the positive-sentence percentage.  cally, at intervals over a number of weeks, we  auIncorporating this new measure within the  metricthors (a non-native and a native speaker of English)  labeling framework is shown to often provide  sigexamined pairs of reviews, attemping to determine  nificant improvements over the other algorithms.  whether the first review in each pair was (1) more  We hope that some of the insights derived here  positive than, (2) less positive than, or (3) as posi-might apply to other scales for text classifcation that tive as the second. The texts in any particular review have been considered, such as clause-level opin-pair were taken from the same author to factor out  ion strength (Wilson, Wiebe, and Hwa, 2004);  afthe effects of cross-author divergence.  fect types like disgust (Subasic and Huettner, 2001;  As Table 1 shows, both subjects performed  perfectly when the rating separation was at least 3  (Collins-Thompson and Callan, 2004); and urgency  notches in the original scale (we define a notch  or criticality (Horvitz, Jacobs, and Hovel, 1999).  as a half star in a or five-star scheme and 10  points in a 100-point scheme). Interestingly,  al2 Problem validation and formulation  though human performance drops as rating  difference decreases, even at a one-notch separation, both  subjects handily outperformed the random-choice  We first ran a small pilot study on human subjects  baseline of 33%. However, there was large variation  in order to establish a rough idea of what a  reasonin accuracy between subjects.2  able classification granularity is: if even people cannot accurately infer labels with respect to a five-star 1For example, the critic Dennis Schwartz writes that some-scheme with half stars, say, then we cannot expect a  times the review itself [indicates] the letter grade should have learning algorithm to do so. Indeed, some potential  been higher or lower, as the review might fail to take into consideration my overall impression of the film which I hope to obstacles to accurate rating inference include lack  capture in the grade (http://www.sover.net/ ozus/cinema.htm).  of calibration (e.g., what an understated author  in2One contributing factor may be that the subjects viewed tends as high praise may seem lukewarm), author  disjoint document sets, since we wanted to maximize experimental coverage of the types of document pairs within each dif-inconsistency at assigning fine-grained ratings, and  ference class. We thus cannot report inter-annotator agreement,  Because of this variation, we defined two  differit is possible to gather author-specific information  ent classification regimes. From the evidence above,  in some practical applications: for instance, systems a three-class task (categories 0, 1, and 2 es-that use selected authors (e.g., the Rotten Tomatoes  sentially negative , middling , and positive ,  removie-review website where, we note, not all  spectively) seems like one that most people would  authors provide explicit ratings) could require that  do quite well at (but we should not assume 100%  someone submit rating-labeled samples of  newlyhuman accuracy: according to our one-notch  readmitted authors' work. Moreover, our results at  sults, people may misclassify borderline cases like  least partially generalize to mixed-author situations 2.5 stars). Our study also suggests that people could (see Section 5.2).  do at least fairly well at distinguishing full stars in a to four-star scheme. However, when we  3 Algorithms  began to construct five-category datasets for each  Recall that the problem we are considering is  multiof our four authors (see below), we found that in  category classification in which the labels can be  each case, either the most negative or the most  posnaturally mapped to a metric space (e.g., points on a itive class (but not both) contained only about 5%  line); for simplicity, we assume the distance metric  of the documents. To make the classes more  balthroughout. In this section, we  anced, we folded these minority classes into the  adpresent three approaches to this problem in order of  jacent class, thus arriving at a four-class problem increasingly explicit use of pairwise similarity  infor(categories 0-3, increasing in positivity). Note that mation between items and between labels. In order  the four-class problem seems to offer more  possito make comparisons between these methods  meanbilities for leveraging class relationship information ingful, we base all three of them on Support Vec-than the three-class setting, since it involves more  tor Machines (SVMs) as implemented in Joachims'  class pairs. Also, even the two-category version of  the rating-inference problem for movie reviews has  proven quite challenging for many automated  classification techniques (Pang, Lee, and Vaithyanathan,  2002; Turney, 2002).  The standard SVM formulation applies only to  binary classification. One-vs-all (OVA) (Rifkin and We applied the above two labeling schemes to  Klautau, 2004) is a common extension to the -ary  a scale dataset3 containing four corpora of movie case. Training consists of building, for each label , reviews.  All reviews were automatically  prean SVM binary classifier distinguishing label from  processed to remove both explicit rating indicators  . We consider the final output to be a label  and objective sentences; the motivation for the latter preference function  , defined as the signed  step is that it has previously aided positive vs.  negdistance of (test) item 0 to the side of the vs.  ative classification (Pang and Lee, 2004). All of the decision plane.  1770, 902, 1307, or 1027 documents in a given  corpus were written by the same author. This decision  Clearly, OVA makes no explicit use of pairwise  facilitates interpretation of the results, since it fac-label or item relationships. However, it can perform  tors out the effects of different choices of methods  well if each class exhibits sufficiently distinct language; see Section 4 for more discussion.  for calibrating authors' scales.4 We point out that  but since our goal is to recover a reviewer's true recommen-3.2 Regression  Alternatively, we can take a regression perspective While another factor might be degree of English fluency, in an informal experiment (six subjects viewing the same three by assuming that the labels come from a discretiza-pairs), native English speakers made the only two errors.  tion of a continuous function  mapping from the  or negative. Even though Eric Lurio uses a 5 star system, his 4From the Rotten Tomatoes website's FAQ: star systems grading is very relaxed. So, 2 stars can be positive. Thus, are not consistent between critics. For critics like Roger Ebert calibration may sometimes require strong familiarity with the and James Berardinelli, 2.5 stars or lower out of 4 stars is al-authors involved, as anyone who has ever needed to reconcile ways negative. For other critics, 2.5 stars can either be positive conflicting referee reports probably knows.  feature space to a metric space.5 If we choose  learning 6 (Atkeson, Moore, and Schaal, 1997).) In a 4  from a family of sufficiently gradual functions,  sense, we are using explicit item and label similarity then similar items necessarily receive similar labels.  information to increasingly penalize the initial clas-In particular, we consider linear, -insensitive SVM  sifier as it assigns more divergent labels to similar 5  items.  1998); the idea is to find the hyperplane that best fits In this paper, we only report supervised-learning  the training data, but where training points whose la-experiments in which the nearest neighbors for any  bels are within distance of the hyperplane incur no  given test item were drawn from the training set  loss. Then, for (test) instance 0 , the label preference alone. In such a setting, the labeling decisions for  function  is the negative of the distance  bedifferent test items are independent, so that solving  tween and the value predicted for 0 by the fitted  the requisite optimization problem is simple.  hyperplane function.  Wilson, Wiebe, and Hwa (2004) used SVM  reAside: transduction The above formulation also gression to classify clause-level strength of opinion, allows for transductive semi-supervised learning as reporting that it provided lower accuracy than other  well, in that we could allow nearest neighbors to  methods. However, independently of our work,  come from both the training and test sets. We  Koppel and Schler (2005) found that applying  linintend to address this case in future work, since  ear regression to classify documents (in a different  there are important settings in which one has a  corpus than ours) with respect to a three-point  ratsmall number of labeled reviews and a large  numing scale provided greater accuracy than OVA SVMs  ber of unlabeled reviews, in which case  considerand other algorithms.  ing similarities between unlabeled texts could prove  quite helpful. In full generality, the  correspond3.3 Metric labeling  ing multi-label optimization problem is intractable,  Regression implicitly encodes the similar items, but for many families of  similar labels heuristic, in that one can restrict  vex) there exist practical exact or approximation  consideration to gradual functions. But we can  algorithms based on techniques for finding  minialso think of our task as a metric labeling prob-mum s-t cuts in graphs (Ishikawa and Geiger, 1998; lem (Kleinberg and Tardos, 2002), a special case  of the maximum a posteriori estimation problem Interestingly, previous sentiment analysis research  for Markov random fields, to explicitly encode our found that a minimum-cut formulation for the binary  desideratum. Suppose we have an initial label  prefsubjective/objective distinction yielded good results erence function 1023 , perhaps computed via one  (Pang and Lee, 2004). Of course, there are many  of the two methods described above. Also, let  other related semi-supervised learning algorithms  be a distance metric on labels, and let  that we would like to try as well; see Zhu (2005)  note the  nearest neighbors of item 0 according  for a survey.  to some item-similarity function  . Then, it is  4 Class struggle: finding a label-correlated  ping of instances  item-similarity function  to labels <D (respecting the  original labels of the training instances) that minimizes We need to specify an item similarity function A_B1C  to use the metric-labeling formulation described in  Section 3.3. We could, as is commonly done,  employ a term-overlap-based measure such as the  cowhere  is monotonically increasing (we chose  sine between term-frequency-based document  vectors (henceforth TO(cos) ). However, Table 2  unless otherwise specified) and J is a  trade-off and/or scaling parameter. (The inner  sum6If we ignore the  term, different choices of  mation is familiar from work in locally-weighted  respond to different versions of nearest-neighbor learning, e.g., majority-vote, weighted average of labels, or weighted median 5We discuss the ordinal regression variant in Section 6.  of labels.  to be the two-dimensional  vector  , and then set the  itemThree-class data 37% 33%  similarity function required by the metric-labeling  optimization  function  (Section 3.3) to  Table 2: Average over authors and class pairs of  between-class vocabulary overlap as the class labels  of the pair grow farther apart.  Positive-sentence percentage (PSP) statistics  Author a  Author b  Author c  shows that in aggregate, the vocabularies of distant  Author d  classes overlap to a degree surprisingly similar to  that of the vocabularies of nearby classes. Thus,  item similarity as measured by TO(cos) may not  correlate well with similarity of the item's true labels.  We can potentially develop a more useful  similarmean and standard deviation of PSP  ity metric by asking ourselves what, intuitively,  accounts for the label relationships that we seek to exploit. A simple hypothesis is that ratings can be  derating (in notches)  termined by the positive-sentence percentage (PSP) of a text, i.e., the number of positive sentences di-Figure 1: Average and standard deviation of PSP for  vided by the number of subjective sentences.  (Termreviews expressing different ratings.  based versions of this premise have motivated much  sentiment-analysis work for over a decade (Das and  But before proceeding, we note that it is  possiChen, 2001; Tong, 2001; Turney, 2002).) But  counble that similarity information might yield no extra  terexamples are easy to construct: reviews can  conbenefit at all. For instance, we don't need it if we  tain off-topic opinions, or recount many positive  ascan reliably identify each class just from some set  pects before describing a fatal flaw.  of distinguishing terms. If we define such terms  We therefore tested the hypothesis as follows.  ) that appear in a  sinTo avoid the need to hand-label sentences as  posigle class 50% or more of the time, then we do find  tive or negative, we first created a sentence polarity many instances; some examples for one author are:  dataset 7 consisting of 10,662 movie-review  snip meaningless , disgusting (class 0); pleasant ,  pets (a striking extract usually one sentence long)  uneven (class 1); and oscar , gem (class 2)  downloaded from www.rottentomatoes.com; each  for the three-class case, and, in the four-class case, snippet was labeled with its source review's label  flat , tedious (class 1) versus straightforward , (positive or negative) as provided by Rotten  Toma likeable (class 2). Some unexpected  distinguishtoes. Then, we trained a Naive Bayes classifier on  ing terms for this author are lion for class 2 (three-this data set and applied it to our scale dataset to  class case), and for class 2 in the four-class case,  identify the positive sentences (recall that objective  jennifer , for a wide variety of Jennifers.  sentences were already removed).  Figure 1 shows that all four authors tend to  exhibit a higher PSP when they write a more  posThis section compares the accuracies of the  apitive review, and we expect that most typical  reproaches outlined in Section 3 on the four corpora  viewers would follow suit. Hence, PSP appears to  comprising our scale dataset. (Results using  be a promising basis for computing document  simror were qualitatively similar.) Throughout, when  ilarity for our rating-inference task. In particular, 8While admittedly we initially chose this function because it was convenient to work with cosines, post hoc analysis re-7Available at http://www.cs.cornell.edu/People/pabo/movie-vealed that the corresponding metric space stretched certain review-data as sentence polarity dataset v1.0.  distances in a useful way.  we refer to something as significant , we mean sta-results, often to a significant degree, and yields the tistically so with respect to the paired -test,  best overall accuracies. Thus, we can in fact  effecThe results that follow are based on  's  tively exploit similarities in the three-class case.  Additionally, in both the and class scenar-OVA. Preliminary analysis of the effect of varying  ios, metric labeling often brings the performance of  the regression parameter in the four-class case  rethe weaker base method up to that of the stronger  vealed that the default value was often optimal.  one (as indicated by the disappearance of upward  The notation AI B denotes metric labeling  triangles in corresponding table rows), and never  where method A provides the initial label preference  hurts performance significantly.  function  and B serves as similarity measure. To  In the four-class case, metric labeling and  regrestrain, we first select the meta-parameters  and J  sion seem roughly equivalent. One possible  interby running 9-fold cross-validation within the  trainpretation is that the relevant structure of the problem ing set. Fixing and J to those values yielding the  is already captured by linear regression (and  perbest performance, we then re-train A (but with SVM  haps a different kernel for regression would have  parameters fixed, as described above) on the whole  improved its three-class performance). However,  training set. At test time, the nearest neighbors of  according to additional experiments we ran in the  each item are also taken from the full training set.  four-class situation, the test-set-optimal parameter  settings for metric labeling would have produced  significant improvements, indicating there may be  Figure 2 summarizes our average 10-fold  crossgreater potential for our framework. At any rate, we  validation accuracy results. We first observe from  view the fact that metric labeling performed quite  the plots that all the algorithms described in Section well for both rating scales as a definitely positive re-3 always definitively outperform the simple baseline  of predicting the majority class, although the  improvements are smaller in the four-class case.  Incidentally, the data was distributed in such a way  Q: Metric labeling looks like it's just combining that the absolute performance of the baseline it-SVMs with nearest neighbors, and classifier  combiself does not change much between the and  nation often improves performance. Couldn't we get  four-class case (which implies that the three-class  the same kind of results by combining SVMs with  datasets were relatively more balanced); and Author  any other reasonable method?  c's datasets seem noticeably easier than the others.  No. For example, if we take the strongest  We now examine the effect of implicitly using  labase SVM method for initial label preferences, but  bel and item similarity. In the four-class case,  rereplace PSP with the term-overlap-based cosine  gression performed better than OVA (significantly  (TO(cos)), performance often drops significantly.  so for two authors, as shown in the righthand  taThis result, which is in accordance with Section  ble); but for the three-category task, OVA  signifi4's data, suggests that choosing an item similarity  cantly outperforms regression for all four authors.  function that correlates well with label similarity  One might initially interprete this flip as showing is important. (ovaI PSP  that in the four-class scenario, item and label  similarities provide a richer source of information  relaQ: Could you explain that notation, please?  tive to class-specific characteristics, especially since A: Triangles point toward the significantly bet-for the non-majority classes there is less data availter algorithm for some dataset.  For instance,  able; whereas in the three-class setting the categories  N [3c] means, In the 3-class task, method  are better modeled as quite distinct entities.  M is significantly better than N for two author  However, the three-class results for metric  labeldatasets and significantly worse for one dataset (so  ing on top of OVA and regression (shown in Figure 2  the algorithms were statistically indistinguishable on by black versions of the corresponding icons) show  the remaining dataset) . When the algorithms  bethat employing explicit similarities always improves  ing compared are statistically indistinguishable on  Average accuracies, three-class data  Author a  Author b  Author c  Author d  Author a  Author b  Author c  Author d  Average ten-fold cross-validation accuracies. Open icons: SVMs in either one-versus-all (square) or regression (circle) mode; dark versions: metric labeling using the corresponding SVM together with the positive-sentence percentage (PSP). The W -axes of the two plots are aligned.  Significant differences, three-class data  Triangles point towards significantly better algorithms for the results plotted above. Specifically, if the difference between a row and a column algorithm for a given author dataset (a, b, c, or d) is significant, a triangle points to the better one; otherwise, a dot (.) is shown. Dark icons highlight the effect of adding PSP  information via metric labeling.  all four datasets (the no triangles case), we  indibel preference function to the constant function  cate this with an equals sign ( = ).  0, but even with test-set-optimal parameter settings, doing so underperforms the trained met-Q: Thanks.  Doesn't Figure 1 show that the  ric labeling algorithm with access to an  inipositive-sentence percentage would be a good  tial SVM classifier (ovaI PSP  classifier even in isolation, so metric labeling isn't  necessary?  No. Predicting class labels directly from  Q: What about using PSP as one of the features for the PSP value via trained thresholds isn't as  input to a standard classifier?  effective  threshold PSP [3c];  A: Our focus is on investigating the utility of  simithreshold PSP [4c]).  larity information. In our particular rating-inference  Alternatively, we could use only the PSP  comsetting, it so happens that the basis for our  pairponent of metric labeling by setting the  lawise similarity measure can be incorporated as an  item-specific feature, but we view this as a tan-work is needed to derive a clear analysis of this set-gential issue. That being said, preliminary  experiting. (Abusing notation, since we're already  playments show that metric labeling can be better, barely ing fast and loose: [3c]: baseline 52.4%, reg 61.4%,  (for test-set-optimal parameter settings for both  algorithms: significantly better results for one author,  [4c]: baseline 38.8%, reg (51.9%)  four-class case; statistically indistinguishable  otherwise), although one needs to determine an  appropriIn future work, it would be interesting to  deterate weight for the PSP feature to get good  performine author-independent characteristics that can be  used on (or suitably adapted to) data for specific au-Q:  thors.  You defined the metric transformation  function  as the identity function U , imposing  Q: How about trying  greater loss as the distance between labels assigned  A: Yes, there are many alternatives. A few  to two similar items increases. Can you do just as  that we tested are described in the Appendix, and  we propose some others in the next section. We  by the same amount, or does the distance between  should mention that we have not yet experimented  labels really matter?  with all-vs.-all (AVA), another standard binary-to-A: You're asking for a comparison to the Potts multi-category classifier conversion method, be-model, which sets  to the function  cause we wished to focus on the effect of  omitotherwise.  In the one  setting pairwise information. In independent work on  ting in which there is a significant difference  3-category rating inference for a different corpus,  between the two, the Potts model does worse  Koppel and Schler (2005) found that regression  out(ovaI PSP ovaI PSP [3c]). Also, employing the  performed AVA, and Rifkin and Klautau (2004)  arPotts model generally leads to fewer significant  gue that in principle OVA should do just as well as  improvements over a chosen base method  (comAVA. But we plan to try it out.  pare Figure 2's tables with: regI PSP reg [3c];  6 Related work and future directions  note that regI PSP reg [4c]). We note that  optiIn this paper, we addressed the rating-inference  mizing the Potts model in the multi-label case is  NPproblem, showing the utility of employing label  simhard, whereas the optimal metric labeling with the  ilarity and (appropriate choice of) item similarity  identity metric-transformation function can be  effi either implicitly, through regression, or explicitly ciently obtained (see Section 3.3).  and often more effectively, through metric labeling.  Q: Your datasets had many labeled reviews and only In the future, we would like to apply our methods  one author each. Is your work relevant to settings  to other scale-based classification problems, and  exwith many authors but very little data for each?  plore alternative methods. Clearly, varying the  kerA: As discussed in Section 2, it can be quite dif-nel in SVM regression might yield better results.  ficult to properly calibrate different authors' scales, Another choice is ordinal regression (McCullagh, since the same number of stars even within what  is ostensibly the same rating system can mean  differwhich only considers the ordering on labels, rather  ent things for different authors. But since you ask:  than any explicit distances between them; this  apwe temporarily turned a blind eye to this serious  isproach could work well if a good metric on labels is  sue, creating a collection of 5394 reviews by 496  aulacking. Also, one could use mixture models (e.g.,  thors with at most 80 reviews per author, where we  pretended that our rating conversions mapped  corels) to capture class relationships (McCallum, 1999;  rectly into a universal rating scheme. Preliminary  Schapire and Singer, 2000; Takamura, Matsumoto,  results on this dataset were actually comparable to  the results reported above, although since we are  We are also interested in framing multi-class but  not confident in the class labels themselves, more  non-scale-based categorization problems as metric  labeling tasks. For example, positive vs. negative vs.  tions on Pattern Analysis and Machine Intelligence (PAMI) 23(11):1222 1239, 2001.  ered in which neutral means either objective  (Engstr m, 2004) or a conflation of objective with a rat-Collins-Thompson, Kevyn and Jamie Callan. 2004. A language ing of mediocre (Das and Chen, 2001). (Koppel and  modeling approach to predicting reading difficulty. In HLT-NAACL: Proceedings of the Main Conference, pages 193  ious types of neutrality.) In either case, we could  Das, Sanjiv and Mike Chen. 2001. Yahoo! for Amazon: Ex-apply a metric in which positive and negative are  tracting market sentiment from stock message boards. In closer to objective (or objective+mediocre) than to  Proceedings of the Asia Pacific Finance Association Annual each other. As another example, hierarchical label  Conference (APFA).  relationships can be easily encoded in a label  metDave, Kushal, Steve Lawrence, and David M. Pennock. 2003.  Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of WWW, Finally, as mentioned in Section 3.3, we would  like to address the transductive setting, in which one has a small amount of labeled data and uses rela-Engstr m, Charlotta. 2004. Topic dependence in sentiment classification. Master's thesis, University of Cambridge.  tionships between unlabeled items, since it is  particularly well-suited to the metric-labeling approach Herbrich, Ralf, Thore Graepel, and Klaus Obermayer. 2000.  Large margin rank boundaries for ordinal regression. In and may be quite important in practice.  Alexander J. Smola, Peter L. Bartlett, Bernhard Sch lkopf, and Dale Schuurmans, editors, Advances in Large Margin Acknowledgments We thank Paul Bennett, Dave Blei, Classifiers, Neural Information Processing Systems. MIT  Claire Cardie, Shimon Edelman, Thorsten Joachims, Jon Klein-Press, pages 115 132.  berg, Oren Kurland, John Lafferty, Guy Lebanon, Pradeep Horvitz, Eric, Andy Jacobs, and David Hovel. 1999. Attention-Ravikumar, Jerry Zhu, and the anonymous reviewers for many sensitive alerting. In Proceedings of the Conference on Un-very useful comments and discussion. We learned of Moshe certainty and Artificial Intelligence, pages 305 313.  Koppel and Jonathan Schler's work while preparing the camera-Ishikawa, Hiroshi. 2003. Exact optimization for Markov ran-ready version of this paper; we thank them for so quickly andom fields with convex priors. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(10).  swering our request for a pre-print. Our descriptions of their work are based on that pre-print; we apologize in advance for Ishikawa, Hiroshi and Davi Geiger. 1998. Occlusions, discon-any inaccuracies in our descriptions that result from changes tinuities, and epipolar lines in stereo. In Proceedings of the 5th European Conference on Computer Vision (ECCV), vol-between their pre-print and their final version. We also thank ume I, pages 232 248, London, UK. Springer-Verlag.  CMU for its hospitality during the year. This paper is based Joachims, Thorsten. 1999. Making large-scale SVM learning upon work supported in part by the National Science Founda-practical. In Bernhard Sch lkopf and Alexander Smola, edi-tion (NSF) under grant no. IIS-0329064 and CCR-0122581; tors, Advances in Kernel Methods Support Vector Learning.  SRI International under subcontract no. 03-000211 on their MIT Press, pages 44 56.  project funded by the Department of the Interior's National Kleinberg, Jon and va Tardos. 2002. Approximation al-Business Center; and by an Alfred P. Sloan Research Fellow-gorithms for classification problems with pairwise relationships: Metric labeling and Markov random fields. Journal ship. Any opinions, findings, and conclusions or recommen-of the ACM, 49(5):616 639.  dations expressed are those of the authors and do not necessarily reflect the views or official policies, either expressed or Koppel, Moshe and Jonathan Schler. 2005. The importance of neutral examples for learning sentiment. In Workshop on implied, of any sponsoring institutions, the U.S. government, or the Analysis of Informal and Formal Information Exchange any other entity.  during Negotiations (FINEXIN).  Liu, Hugo, Henry Lieberman, and Ted Selker. 2003. A model References  of textual affect sensing using real-world knowledge. In Proceedings of Intelligent User Interfaces (IUI), pages 125 132.  1997. Locally weighted learning. Artificial Intelligence Re-McCallum, Andrew. 1999. Multi-label text classification with view, 11(1):11 73.  a mixture model trained by EM. In AAAI Workshop on Text Learning.  Boykov, Yuri, Olga Veksler, and Ramin Zabih. 1999. Fast ap-proximate energy minimization via graph cuts. In Proceed-McCullagh, Peter. 1980. Regression models for ordinal data.  ings of the International Conference on Computer Vision Journal of the Royal Statistical Society, 42(2):109 42.  Pang, Bo and Lillian Lee. 2004. A sentimental education: Sen-as negative labels. If we then consider the resulting timent analysis using subjectivity summarization based on classifier to output a positivity-preference function minimum cuts. In Proceedings of the ACL, pages 271 278.  , we can then learn a series of thresholds to  Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. 2002.  convert this value into the desired label set, under  Thumbs up? Sentiment classification using machine learning 10?  techniques. In Proceedings of EMNLP, pages 79 86.  the assumption that the bigger  is, the more  positive the review.9 This algorithm always  outperQu, Yan, James Shanahan, and Janyce Wiebe, editors. 2004.  forms the majority-class baseline, but not to the  deProceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.  gree that the best of SVM OVA and SVM  regresAAAI Press. AAAI technical report SS-04-07.  Rifkin, Ryan M. and Aldebaro Klautau. 2004. In defense of found in a three-class study that thresholding a pos-one-vs-all classification. Journal of Machine Learning Re-itive/negative classifier trained only on clearly posi-search, 5:101 141.  tive or clearly negative examples did not yield large Schapire, Robert E. and Yoram Singer. 2000. BoosTexter: improvements.  A boosting-based system for text categorization. Machine Learning, 39(2/3):135 168.  In our experiments with SVM regression, we  disrial on support vector regression. Technical Report Neuro-cretized regression output via a set of fixed decision COLT NC-TR-98-030, Royal Holloway College, University x  of London.  thresholds  to map it into our set of  class labels. Alternatively, we can learn the  threshSubasic, Pero and Alison Huettner. 2001. Affect analysis of olds instead. Neither option clearly outperforms the  text using fuzzy semantic typing. IEEE Transactions on Fuzzy Systems, 9(4):483 496.  other in the four-class case. In the three-class  setting, the learned version provides noticeably better  2004. Modeling category structures with a kernel function.  performance in two of the four datasets. But these  In Proceedings of CoNLL, pages 57 64.  results taken together still mean that in many cases, the difference is negligible, and if we had started  Tong, Richard M. 2001. An operational system for detecting and tracking opinions in on-line discussion. SIGIR Work-down this path, we would have needed to consider  shop on Operational Text Classification.  similar tweaks for one-vs-all SVM as well. We  Turney, Peter. 2002. Thumbs up or thumbs down? Semantic therefore stuck with the simpler version in order to  orientation applied to unsupervised classification of reviews.  maintain focus on the central issues at hand.  In Proceedings of the ACL, pages 417 424.  Vapnik, Vladimir. 1995. The Nature of Statistical Learning Theory. Springer.  Wilson, Theresa, Janyce Wiebe, and Rebecca Hwa. 2004. Just how mad are you? Finding strong and weak opinion clauses.  In Proceedings of AAAI, pages 761 769.  Yu, Hong and Vasileios Hatzivassiloglou. 2003. Towards an-swering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of EMNLP.  Zhu, Xiaojin (Jerry). 2005. Semi-Supervised Learning with Graphs. Ph.D. thesis, Carnegie Mellon University.  A Appendix: other variations attempted  A.1 Discretizing binary classification  In our setting, we can also incorporate class relations 9This is not necessarily true: if the classifier's goal is to opti-by directly altering the output of a binary classifier, mize binary classification error, its major concern is to increase as follows. We first train a standard SVM, treating  confidence in the positive/negative distinction, which may not correspond to higher confidence in separating five stars from ratings greater than 0.5 as positive labels and others  four stars . 