 Learning Subjective Nouns using Extraction Pattern Bootstrapping  Theresa Wilson  School of Computing  Department of Computer Science Intelligent Systems Program University of Utah  University of Pittsburgh  University of Pittsburgh  Salt Lake City, UT 84112  document summarization systems need to summarize different opinions and perspectives. Spam filtering systems We explore the idea of creating a subjectiv-must recognize rants and emotional tirades, among other ity classifier that uses lists of subjective nouns  things. In general, nearly any system that seeks to iden-learned by bootstrapping algorithms. The goal  tify information could benefit from being able to separate of our research is to develop a system that  factual and subjective information.  can distinguish subjective sentences from  obSubjective language has been previously studied in  jective sentences. First, we use two  bootstrapfields such as linguistics, literary theory, psychology, and ping algorithms that exploit extraction patterns  content analysis. Some manually-developed knowledge  to learn sets of subjective nouns.  Then we  resources exist, but there is no comprehensive dictionary train a Naive Bayes classifier using the subjec-of subjective language.  tive nouns, discourse features, and subjectivity  Meta-Bootstrapping (Riloff and Jones, 1999) and  clues identified in prior research. The  bootBasilisk (Thelen and Riloff, 2002) are bootstrapping al-strapping algorithms learned over 1000  subjecgorithms that use automatically generated extraction pattive nouns, and the subjectivity classifier  perterns to identify words belonging to a semantic cate-formed well, achieving 77% recall with 81%  We hypothesized that extraction patterns could  also identify subjective words. For example, the pattern â€œexpressed < direct object> ' often extracts subjective nouns, such as 'concern', 'hope', and 'support'.  Introduction  Furthermore, these bootstrapping algorithms require only a handful of seed words and unannotated texts for train-Many natural language processing applications could  ing; no annotated data is needed at all.  benefit from being able to distinguish between factual In this paper, we use the Meta-Bootstrapping and  and subjective information.  Subjective remarks come  Basilisk algorithms to learn lists of subjective nouns from in a variety of forms, including opinions, rants, allega-a large collection of unannotated texts. Then we train tions, accusations, suspicions, and speculation. Ideally, a subjectivity classifier on a small set of annotated data, information extraction systems should be able to distin-using the subjective nouns as features along with some guish between factual information (which should be ex-other previously identified subjectivity features. Our extracted) and non-factual information (which should be perimental results show that the subjectivity classifier discarded or labeled as uncertain). Question answering performs well (77% recall with 81% precision) and that systems should distinguish between factual and specula-the learned nouns improve upon previous state-of-the-art tive answers. Multi-perspective question answering aims subjectivity results (Wiebe et al., 1999).  to present multiple answers to the user based upon speculation or opinions derived from different sources. Multi-2  Subjectivity Data  This work was supported in part by the National Science Foundation under grants IIS-0208798 and IRI-9704240.  The Annotation Scheme  The data preparation was performed in support of the North-In  annotation  was  developed  east Regional Reseach Center (NRRC) which is sponsored by for a U.S. government-sponsored project with a  the Advanced Research and Development Activity (ARDA), a team  of  researchers  (the  U.S. Government entity which sponsors and promotes research of import to the Intelligence Community which includes but is tions and project reports are available on the Web  not limited to the CIA, DIA, NSA, NIMA, and NRO.  The scheme was inspired by work in linguistics and  literary theory on subjectivity, which focuses on how Subj  opinions, emotions, etc. are expressed linguistically in T agger A  context (Banfield, 1982). The scheme is more detailed Obj  and comprehensive than previous ones. We mention only those aspects of the annotation scheme relevant to this Table 1: Agreement for sentence-level annotations  The goal of the annotation scheme is to identify and T agger T  characterize expressions of private states in a sentence.  Private state is a general covering term for opinions, eval-T agger A  uations, emotions, and speculations (Quirk et al., 1985).  For example, in sentence (1) the writer is expressing a Table 2: Agreement for sentence-level annotations, low-negative evaluation.  strength cases removed  (1) 'The time has come, gentlemen, for Sharon, the as-sassin, to realize that injustice cannot last long.'  One would expect that there are clear cases of objec-Sentence (2) reflects the private state of Western countive sentences, clear cases of subjective sentences, and tries. Mugabe's use of 'overwhelmingly' also reflects a borderline sentences in between. The agreement study private state, his positive reaction to and characterization supports this. In terms of our annotations, we define of his victory.  a sentence as borderline if it has at least one private-state expression identified by at least one annotator, and (2) 'Western countries were left frustrated and impotent all strength ratings of private-state expressions are low.  after Robert Mugabe formally declared that he had over-Table 2 shows the agreement results when such borderwhelmingly won Zimbabwe's presidential election.'  line sentences are removed (19 sentences, or 11% of the Annotators are also asked to judge the strength of each agreement test corpus). The percentage agreement in-private state. A private state can have low, medium, high creases to 94% and the value increases to 0.87.  or extreme strength.  As expected, the majority of disagreement cases  involve low-strength subjectivity. The annotators consis-2.2  Corpus and Agreement Results  tently agree about which are the clear cases of subjective sentences. This leads us to define the gold-standard that Our data consists of English-language versions of foreign we use in our experiments. A sentence is subjective if it news documents from FBIS, the U.S. Foreign Broadcast contains at least one private-state expression of medium Information Service. The data is from a variety of publi-or higher strength. The second class, which we call ob-cations and countries. The annotated corpus used to train jective, consists of everything else. Thus, sentences with and test our subjectivity classifiers (the experiment cor-only mild traces of subjectivity are tossed into the objec-pus) consists of 109 documents with a total of 2197 sentive category, making the system's goal to find the clearly tences. We used a separate, annotated tuning corpus of subjective sentences.  33 documents with a total of 698 sentences to establish some experimental parameters.1  Using Extraction Patterns to Learn  Each document was annotated by one or both of two  Subjective Nouns  annotators, A and T. To allow us to measure interanno-In the last few years, two bootstrapping algorithms have tator agreement, the annotators independently annotated been developed to create semantic dictionaries by ex-the same 12 documents with a total of 178 sentences. We ploiting extraction patterns: Meta-Bootstrapping (Riloff began with a strict measure of agreement at the sentence and Jones, 1999) and Basilisk (Thelen and Riloff, 2002).  level by first considering whether the annotator marked Extraction patterns were originally developed for infor-any private-state expression, of any strength, anywhere mation extraction tasks (Cardie, 1997). They represent in the sentence. If so, the sentence should be subjective.  lexico-syntactic expressions that typically rely on shal-Otherwise, it is objective. Table 1 shows the contingency low parsing and syntactic role assignment. For example, table. The percentage agreement is 88%, and the value the pattern '<subject> was hired' would apply to sen-is 0.71.  tences that contain the verb 'hired' in the passive voice.  The subject would be extracted as the hiree.  The annotated data will be available to U.S. government contractors this summer. We are working to resolve copyright Meta-Bootstrapping and Basilisk were designed to  issues to make it available to the wider research community.  learn words that belong to a semantic category (e.g.,  'truck' is a VEHICLE and 'seashore' is a LOCATION).  seed words for a semantic category. The bootstrapping Both algorithms begin with unannotated texts and seed process involves three steps. (1) Basilisk automatically words that represent a semantic category. A bootstrap-generates a set of extraction patterns for the corpus and ping process looks for words that appear in the same ex-scores each pattern based upon the number of seed words traction patterns as the seeds and hypothesizes that those among its extractions. This step is identical to the first words belong to the same semantic class. The principle step of Meta-Bootstrapping. Basilisk then puts the best behind this approach is that words of the same semantic patterns into a Pattern Pool. (2) All nouns3 extracted by a class appear in similar pattern contexts. For example, the pattern in the Pattern Pool are put into a Candidate Word phrases 'lived in' and 'traveled to' will co-occur with Pool. Basilisk scores each noun based upon the set of many noun phrases that represent LOCATIONS.  patterns that extracted it and their collective association In our research, we want to automatically identify  with the seed words. (3) The top 10 nouns are labeled as words that are subjective. Subjective terms have many the targeted semantic class and are added to the dictio-different semantic meanings, but we believe that the same nary. The bootstrapping process then repeats, using the contextual principle applies to subjectivity. In this sec-original seeds and the newly labeled words.  tion, we briefly overview these bootstrapping algorithms The main difference between Basilisk and Meta-and explain how we used them to generate lists of subjec-Bootstrapping is that Basilisk scores each noun based tive nouns.  on collective information gathered from all patterns that extracted it. In contrast, Meta-Bootstrapping identifies 3.1  a single best pattern and assumes that everything it ex-The Meta-Bootstrapping ('MetaBoot') process (Riloff  tracted belongs to the same semantic class. The second and Jones, 1999) begins with a small set of seed words level of bootstrapping smoothes over some of the prob-that represent a targeted semantic category (e.g., 10  lems caused by this assumption. In comparative experi-words that represent LOCATIONS) and an unannotated  ments (Thelen and Riloff, 2002), Basilisk outperformed corpus. First, MetaBoot automatically creates a set of ex-Meta-Bootstrapping. But since our goal of learning sub-traction patterns for the corpus by applying and instanti-jective nouns is different from the original intent of the ating syntactic templates. This process literally produces algorithms, we tried them both. We also suspected they thousands of extraction patterns that, collectively, will ex-might learn different words, in which case using both al-tract every noun phrase in the corpus. Next, MetaBoot gorithms could be worthwhile.  computes a score for each pattern based upon the number of seed words among its extractions. The best pat-3.3  tern is saved and all of its extracted noun phrases are The Meta-Bootstrapping and Basilisk algorithms need  automatically labeled as the targeted semantic category.2  seed words and an unannotated text corpus as input.  MetaBoot then re-scores the extraction patterns, using the Since we did not need annotated texts, we created a much original seed words as well as the newly labeled words, larger training corpus, the bootstrapping corpus, by gath-and the process repeats. This procedure is called mutual ering 950 new texts from the FBIS source mentioned  in Section 2.2. To find candidate seed words, we auto-A second level of bootstrapping (the 'meta-'  bootmatically identified 850 nouns that were positively corre-strapping part) makes the algorithm more robust. When lated with subjective sentences in another data set. How-the mutual bootstrapping process is finished, all nouns ever, it is crucial that the seed words occur frequently that were put into the semantic dictionary are re-in our FBIS texts or the bootstrapping process will not evaluated. Each noun is assigned a score based on how get off the ground. So we searched for each of the 850  many different patterns extracted it. Only the five best nouns in the bootstrapping corpus, sorted them by fre-nouns are allowed to remain in the dictionary. The other quency, and manually selected 20 high-frequency words entries are discarded, and the mutual bootstrapping pro-that we judged to be strongly subjective. Table 3 shows cess starts over again using the revised semantic dictio-the 20 seed words used for both Meta-Bootstrapping and nary.  We ran each bootstrapping algorithm for 400  iteraBasilisk (Thelen and Riloff, 2002) is a more recent boot-tions, generating 5 words per iteration. Basilisk gener-strapping algorithm that also utilizes extraction patterns ated 2000 nouns and Meta-Bootstrapping generated 1996  to create a semantic dictionary. Similarly, Basilisk be-nouns.4 Table 4 shows some examples of extraction pat-gins with an unannotated text corpus and a small set of 3Technically, each head noun of an extracted noun phrase.  2Our implementation of Meta-Bootstrapping learns individ-4Meta-Bootstrapping will sometimes produce fewer than 5  ual nouns (vs. noun phrases) and discards capitalized words.  words per iteration if it has low confidence in its judgements.  cowardice  hatred  fool  sigh  twit  Table 6: Subjective Word Lexicons after Manual Review (B=Basilisk, M=MetaBootstrapping)  Table 3: Subjective Seed Words  Extraction Patterns  Examples of Extracted Nouns  views, worries, recognition  indicative of <np>  compromise, desire, thinking  vitality, hatred  reaffirmed  % of Words Subjective  show of <np>  support, strength, goodwill,  solidarity, feeling  <subject> was shared  anxiety, view, niceties, feeling  Table 4: Extraction Pattern Examples  Number of Words Generated  terns that were discovered to be associated with subjec-Figure 1: Accuracy during Bootstrapping  tive nouns.  Meta-Bootstrapping and Basilisk are semi-automatic  lexicon generation tools because, although the bootstrap-We classified the words as StrongSubjective, WeakSub-ping process is 100% automatic, the resulting lexicons jective, or Objective. Objective terms are not subjective at need to be reviewed by a human.5 So we manually re-all (e.g., 'chair' or 'city'). StrongSubjective terms have viewed the 3996 words proposed by the algorithms. This strong, unambiguously subjective connotations, such as process is very fast; it takes only a few seconds to classify  WeakSubjective was used for  each word. The entire review process took approximately three situations: (1) words that have weak subjective con-3-4 hours. One author did this labeling; this person did notations, such as 'aberration' which implies something not look at or run tests on the experiment corpus.  out of the ordinary but does not evoke a strong sense of judgement, (2) words that have multiple senses or uses, Strong Subjective  Weak Subjective  where one is subjective but the other is not. For example, tyranny  aberration  the word 'plague' can refer to a disease (objective) or an smokescreen  onslaught of something negative (subjective), (3) words apologist  apprehensions  that are objective by themselves but appear in idiomatic barbarian  beneficiary  expressions that are subjective. For example, the word condemnation  'eyebrows' was labeled WeakSubjective because the ex-sanctimonious  unity  pression 'raised eyebrows' probably occurs more often exaggeration  mockery  eyebrows  in our corpus than literal references to 'eyebrows'. Ta-repudiation  anguish  ble 5 shows examples of learned words that were classi-insinuation  fied as StrongSubjective or WeakSubjective.  Once the words had been manually classified, we could denunciation  go back and measure the effectiveness of the algorithms.  controversy  The graph in Figure 1 tracks their accuracy as the boot-humiliation  sincerity  eternity  strapping progressed. The X-axis shows the number of sympathy  rejection  words generated so far. The Y-axis shows the percentage of those words that were manually classified as sub-Table 5: Examples of Learned Subjective Nouns  jective. As is typical of bootstrapping algorithms, accuracy was high during the initial iterations but tapered 5This is because NLP systems expect dictionaries to have off as the bootstrapping continued.  high integrity. Even if the algorithms could achieve 90% accuracy, a dictionary in which 1 of every 10 words is defined both algorithms were 95% accurate. After 100 words  incorrectly would probably not be desirable.  Basilisk was 75% accurate and MetaBoot was 81%  accurate. After 1000 words, accuracy dropped to about 28%  Previously Established Features  for MetaBoot, but Basilisk was still performing reason-Wiebe, Bruce, & O'Hara (1999) developed a machine ably well at 53%. Although 53% accuracy is not high for learning system to classify subjective sentences. We ex-a fully automatic process, Basilisk depends on a human perimented with the features that they used, both to com-to review the words so 53% accuracy means that the hu-pare their results to ours and to see if we could benefit man is accepting every other word, on average. Thus, the from their features. We will refer to these as the WBO  reviewer's time was still being spent productively even features.  after 1000 words had been hypothesized.  WBO includes a set of stems positively correlated with Table 6 shows the size of the final lexicons created the subjective training examples (subjStems) and a set by the bootstrapping algorithms. The first two columns of stems positively correlated with the objective training show the number of subjective terms learned by Basilisk examples (objStems). We defined a three-valued feature and Meta-Bootstrapping. Basilisk was more prolific, gen-for the presence of 0, 1, or 2 members of subjStems erating 825 subjective terms compared to 522 for Meta-in a sentence, and likewise for objStems. For our exper-Bootstrapping. The third column shows the intersection iments, subjStems includes stems that appear 7 times between their word lists. There was substantial overlap, in the training set, and for which the precision is 1.25  but both algorithms produced many words that the other times the baseline word precision for that training set.  did not. The last column shows the results of merging objStems contains the stems that appear 7 times and their lists. In total, the bootstrapping algorithms produced for which at least 50% of their occurrences in the training 1052 subjective nouns.  set are in objective sentences. WBO also includes a binary feature for each of the following: the presence in the 4  Creating Subjectivity Classifiers  sentence of a pronoun, an adjective, a cardinal number, a modal other than will, and an adverb other than not.  To evaluate the subjective nouns, we trained a Naive Bayes classifier using the nouns as features. We also in-We also added manually-developed features found by  corporated previously established subjectivity clues, and other researchers. We created 14 feature sets represent-added some new discourse features. In this section, we ing some classes from (Levin, 1993; Ballmer and Bren-describe all the feature sets and present performance re-nenstuhl, 1981), some Framenet lemmas with frame elesults for subjectivity classifiers trained on different com-ment experiencer (Baker et al., 1998), adjectives manu-binations of these features. The threshold values and fea-ally annotated for polarity (Hatzivassiloglou and McKe-ture representations used in this section are the ones that own, 1997), and some subjectivity clues listed in (Wiebe, produced the best results on our separate tuning corpus.  1990). We represented each set as a three-valued feature based on the presence of 0, 1, or 2 members of the set.  Subjective Noun Features  We will refer to these as the manual features.  We defined four features to represent the sets of subjec-4.3  Discourse Features  tive nouns produced by the bootstrapping algorithms.  We created discourse features to capture the density of clues in the text surrounding a sentence. First, we com-BA-Strong: the set of StrongSubjective nouns generated puted the average number of subjective clues and objec-by Basilisk  tive clues per sentence, normalized by sentence length.  The subjective clues, subjClues, are all sets for which BA-Weak: the set of WeakSubjective nouns generated 3-valued features were defined above (except objStems).  The objective clues consist only of objStems. For sentence S, let ClueRate  and  MB-Strong: the set of StrongSubjective nouns generated subj (S) =  |subjClues in S|  by Meta-Bootstrapping  AvgClueRatesubj to be the average of ClueRate(S)  MB-Weak: the set of WeakSubjective nouns generated over all sentences S and similarly for AvgClueRateobj.  by Meta-Bootstrapping  Next, we characterize the number of subjective and  objective clues in the previous and next sentences as: For each set, we created a three-valued feature based on higher-than-expected ( high), lower-than-expected ( low), the presence of 0, 1, or 2 words from that set. We used or expected ( medium). The value for ClueRatesubj(S) the nouns as feature sets, rather than define a separate is high if ClueRatesubj(S) AvgClueRatesubj 1.3; feature for each word, so the classifier could generalize low if ClueRatesubj(S) AvgClueRatesubj/1.3; oth-over the set to minimize sparse data problems. We will erwise it is medium. The values for ClueRateobj(S) are refer to these as the SubjNoun features.  defined similarly.  Using these definitions we created four features:  propriate way to benefit from the subjective nouns is to ClueRatesubj for the previous and following sen-use them in tandem with other subjectivity clues.  tences, and ClueRateobj for the previous and  following sentences. We also defined a feature for sentence Acc  length. Let AvgSentLen be the average sentence length.  WBO+SubjNoun+  SentLen(S) is high if length(S) AvgSentLen 1.3; manual+discourse  low if length(S) AvgSentLen/1.3; and medium  othWBO+SubjNoun  Classification Results  Table 8: Results with New Features  We conducted experiments to evaluate the performance Table 8 shows the results of Naive Bayes classifiers of the feature sets, both individually and in various com-trained with different combinations of features. The ac-binations. Unless otherwise noted, all experiments in-curacy differences between all pairs of experiments in volved training a Naive Bayes classifier using a particu-Table 8 are statistically significant. Row (3) uses only lar set of features. We evaluated each classifier using 25-the WBO features (also shown in Table 7 as a baseline).  fold cross validation on the experiment corpus and used Row (2) uses the WBO features as well as the SubjNoun paired t-tests to measure significance at the 95% confi-features. There is a synergy between these feature sets: dence level. As our evaluation metrics, we computed ac-using both types of features achieves better performance curacy (Acc) as the percentage of the system's classifica-than either one alone. The difference is mainly precision, tions that match the gold-standard, and precision (Prec) presumably because the classifier found more and better and recall (Rec) with respect to subjective sentences.  combinations of features. In Row (1), we also added the manual and discourse features. The discourse features Acc  explicitly identify contexts in which multiple clues are (1) Bag-Of-Words  found. This classifier produced even better performance, (2) WBO  achieving 81.3% precision with 77.4% recall. The 76.1%  accuracy result is significantly higher than the accuracy results for all of the other classifiers (in both Table 8 and Table 7: Baselines for Comparison  Table 7).  Finally, higher precision classification can be obtained Table 7 shows three baseline experiments. Row (3)  by simply classifying a sentence as subjective if it con-represents the common baseline of assigning every sen-tains any of the StrongSubjective nouns. On our data, this tence to the most frequent class.  The Most-Frequent  method produces 87% precision with 26% recall. This  baseline achieves 59% accuracy because 59% of the sen-approach could support applications for which precision tences in the gold-standard are subjective. Row (2) is is paramount.  a Naive Bayes classifier that uses the WBO features, which performed well in prior research on sentence-level 5  Related Work  subjectivity classification (Wiebe et al., 1999). Row (1) shows a Naive Bayes classifier that uses unigram bag-of-Several types of research have involved document-level words features, with one binary feature for the absence subjectivity classification. Some work identifies inflam-or presence in the sentence of each word that appeared matory texts (e.g., (Spertus, 1997)) or classifies reviews during training. Pang et al. (2002) reported that a similar as positive or negative ((Turney, 2002; Pang et al., 2002)).  experiment produced their best results on a related clas-Tong's system (Tong, 2001) generates sentiment time-sification task. The difference in accuracy between Rows lines, tracking online discussions and creating graphs of (1) and (2) is not statistically significant (Bag-of-Word's positive and negative opinion messages over time. Re-higher precision is balanced by WBO's higher recall).  search in genre classification may include recognition of Next, we trained a Naive Bayes classifier using only subjective genres such as editorials (e.g., (Karlgren and the SubjNoun features. This classifier achieved good Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001)).  precision (77%) but only moderate recall (64%). Upon In contrast, our work classifies individual sentences, as further inspection, we discovered that the subjective does the research in (Wiebe et al., 1999). Sentence-level nouns are good subjectivity indicators when they appear, subjectivity classification is useful because most docu-but not every subjective sentence contains one of them.  ments contain a mix of subjective and objective  senAnd, relatively few sentences contain more than one, tences. For example, newspaper articles are typically making it difficult to recognize contextual effects (i.e., thought to be relatively objective, but (Wiebe et al., 2001) multiple clues in a region). We concluded that the ap-reported that, in their corpus, 44% of sentences (in  articles that are not editorials or reviews) were subjective.  sification: A Study in the Lexical Analysis of English Some previous work has focused explicitly on learn-Speech Activity Verbs. Springer-Verlag.  ing subjective words and phrases. (Hatzivassiloglou and A. Banfield. 1982. Unspeakable Sentences. Routledge McKeown, 1997) describes a method for identifying the and Kegan Paul, Boston.  semantic orientation of words, for example that beauti-ful expresses positive sentiments. Researchers have fo-S. Caraballo.  Automatic Acquisition of a  cused on learning adjectives or adjectival phrases (Tur-Hypernym-Labeled Noun Hierarchy from Text.  Proceedings of the 37th Annual Meeting of the Asso-ney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe, ciation for Computational Linguistics, pages 120 126.  2000) and verbs (Wiebe et al., 2001), but no previous work has focused on learning nouns. A unique aspect  C. Cardie. 1997. Empirical Methods in Information Ex-of our work is the use of bootstrapping methods that extraction. AI Magazine, 18(4):65 79.  ploit extraction patterns. (Turney, 2002) used patterns V. Hatzivassiloglou and K. McKeown. 1997. Predicting representing part-of-speech sequences, (Hatzivassiloglou the semantic orientation of adjectives. In ACL-EACL  and McKeown, 1997) recognized adjectival phrases, and 1997.  (Wiebe et al., 2001) learned N-grams. The extraction patterns used in our research are linguistically richer pat-M. Hearst. 1992. Automatic acquisition of hyponyms  from large text corpora. In Proc. of the 14th Interna-terns, requiring shallow parsing and syntactic role assign-tional Conference on Computational Linguistics.  In recent years several techniques have been developed J. Karlgren and D. Cutting. 1994. Recognizing text gen-for semantic lexicon creation (e.g., (Hearst, 1992; Riloff res with simple metrics using discriminant analysis. In and Shepherd, 1997; Roark and Charniak, 1998; Cara-COLING-94.  ballo, 1999)). Semantic word learning is different from B. Kessler, G. Nunberg, and H. Schutze. 1997. Auto-subjective word learning, but we have shown that Meta-matic detection of text genre. In Proc. ACL-EACL-97.  Bootstrapping and Basilisk could be successfully applied to subjectivity learning. Perhaps some of these other Beth Levin.  nations: A Preliminary Investigation. University of methods could also be used to learn subjective words.  Chicago Press, Chicago.  B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs  up? sentiment classification using machine learning  This research produced interesting insights as well as per-techniques. In Proceedings of the 2002 Conference on formance results. First, we demonstrated that weakly Empirical Methods in Natural Language Processing.  supervised bootstrapping techniques can learn subjec-R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.  tive terms from unannotated texts. Subjective features A Comprehensive Grammar of the English Language.  learned from unannotated documents can augment or en-Longman, New York.  hance features learned from annotated training data using more traditional supervised learning techniques. Sec-E. Riloff and R. Jones. 1999. Learning Dictionaries for ond, Basilisk and Meta-Bootstrapping proved to be use-Information Extraction by Multi-Level Bootstrapping.  In Proceedings of the 16th National Conference on Ar-ful for a different task than they were originally intended.  By seeding the algorithms with subjective words, the extraction patterns identified expressions that are associated E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-with subjective nouns. This suggests that the bootstrap-proach for Building Semantic Lexicons. In Proceedings of the Second Conference on Empirical Methods ping algorithms should be able to learn not only general in Natural Language Processing, pages 117 124.  semantic categories, but any category for which words appear in similar linguistic phrases. Third, our best sub-B. Roark and E. Charniak.  Noun-phrase  jectivity classifier used a wide variety of features. Sub-Co-occurrence Statistics for Semi-automatic  Semanjectivity is a complex linguistic phenomenon and our evi-tic Lexicon Construction. In Proceedings of the 36th Annual Meeting of the Association for Computational dence suggests that reliable subjectivity classification re-Linguistics, pages 1110 1116.  quires a broad array of features.  E. Spertus. 1997. Smokey: Automatic recognition of  hostile messages. In Proc. IAAI.  M. Thelen and E. Riloff. 2002. A Bootstrapping Method C. Baker, C. Fillmore, and J. Lowe. 1998. The berkeley for Learning Semantic Lexicons Using Extraction Pa  framenet project. In Proceedings of the COLING-ACL.  ttern Contexts. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Process-T. Ballmer and W. Brennenstuhl. 1981. Speech Act Clas-ing.  R. Tong. 2001. An operational system for detecting and tracking opinions in on-line discussion. In SIGIR 2001  Workshop on Operational Text Classification.  P. Turney. 2002. Thumbs Up or Thumbs Down?  Semantic Orientation Applied to Unsupervised Classification of Reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.  J. Wiebe, R. Bruce, and T. O'Hara. 1999. Development and use of a gold standard data set for subjectivity classifications. In Proc. 37th Annual Meeting of the Assoc.  for Computational Linguistics (ACL-99).  J. Wiebe, T. Wilson, and M. Bell. 2001. Identifying col-locations for recognizing opinions. In Proc. ACL-01  Workshop on Collocation: Computational Extraction, Analysis, and Exploitation, July.  J. Wiebe. 1990. Recognizing Subjective Sentences: A Computational Investigation of Narrative Text. Ph.D.  thesis, State University of New York at Buffalo.  J. Wiebe. 2000. Learning subjective adjectives from corpora. In 17th National Conference on Artificial Intelligence. 