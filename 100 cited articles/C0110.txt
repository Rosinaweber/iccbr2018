 Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon's Mechanical Turk  Chris Callison-Burch  Center for Language and Speech Processing  Johns Hopkins University  Baltimore, Maryland  of money to complete human intelligence tests  tasks that are difficult for computers but easy for  Manual evaluation of translation quality is  people. We show that:  generally thought to be excessively time  consuming and expensive. We explore a  fast and inexpensive way of doing it using  that are very similar to experts and that have  Amazon's Mechanical Turk to pay small  a stronger correlation than Bleu.  sums to a large number of non-expert  annotators. For $10 we redundantly  recre Mechanical Turk can be used for complex  ate judgments from a WMT08  translation task. We find that when combined  rate (HTER) and creating multiple reference  non-expert judgments have a high-level of  agreement with the existing gold-standard  Evaluating translation quality through  readjudgments of machine translation quality,  ing comprehension, which is rarely done, can  and correlate more strongly with expert  be easily accomplished through creative use  judgments than Bleu does. We go on to  of Mechanical Turk.  show that Mechanical Turk can be used to  calculate human-mediated translation edit  2 Related work  rate (HTER), to conduct reading  compreSnow et al. (2008) examined the accuracy of  lahension experiments with machine  transbels created using Mechanical Turk for a variety  lation, and to create high quality reference  of natural language processing tasks. These tasks  included word sense disambiguation, word  simi1 Introduction  larity, textual entailment, and temporal ordering  of events, but not machine translation. Snow et  Conventional wisdom holds that manual  evaluaal. measured the quality of non-expert annotations  tion of machine translation is too time-consuming  by comparing them against labels that had been  and expensive to conduct. Instead, researchers  previously created by expert annotators. They  reroutinely use automatic metrics like Bleu  (Papport inter-annotator agreement between expert and  ineni et al., 2002) as the sole evidence of  imnon-expert annotators, and show that the average  provement to translation quality. Automatic  metof many non-experts converges on performance of  rics have been criticized for a variety of reasons  a single expert for many of their tasks.  (Babych and Hartley, 2004; Callison-Burch et al.,  Although it is not common for manual  evalu2006; Chiang et al., 2008), and it is clear that  ation results to be reported in conference papers,  they only loosely approximate human judgments.  several large-scale manual evaluations of machine  Therefore, having people evaluate translation  outtranslation quality take place annually. These  input would be preferable, if it were more practical.  clude public forums like the NIST MT  EvaluIn this paper we demonstrate that the manual  ation Workshop, IWSLT and WMT, as well as  evaluation of translation quality is not as expensive  the project-specific Go/No Go evaluations for the  or as time consuming as generally thought. We  DARPA GALE program. Various types of human  use Amazon's Mechanical Turk, an online labor  judgments are used. NIST collects 5-point fluency  market that is designed to pay people small sums  and adequacy scores (LDC, 2005), IWSLT and  Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286 295, Singapore, 6-7 August 2009. c  2009 ACL and AFNLP  WMT collect relative rankings (Callison-Burch et have a specialized background in the subject, so  there is an obvious tradeoff between hiring  indiing HTER (Snover et al., 2006). The details of  viduals from this non-expert labor pool and  seekthese are provided later in the paper. Public  evaling out annotators who have a particular expertise.  uation campaigns provide a ready source of  goldstandard data that non-expert annotations can be  compared to.  We use Mechanical Turk as an inexpensive way  3 Mechanical Turk  of evaluating machine translation. In this section,  we measure the level of agreement between  exAmazon describes its Mechanical Turk web  serpert and non-expert judgments of translation  quality. To do so, we recreate an existing set of  goldand tag line refer to a historical hoax from the 18th  standard judgments of machine translation quality  century where an automaton appeared to be able to  taken from the Workshop on Statistical Machine  beat human opponents at chess using a clockwork  Translation (WMT), which conducts an annual  mechanism, but was, in fact, controlled by a  perlarge-scale human evaluation of machine  translason hiding inside the machine. The Mechanical  tion quality. The experts who produced the  goldTurk web site provides a way to pay people small  standard judgments are computational linguists  amounts of money to perform tasks that are  simwho develop machine translation systems.  ple for humans but difficult for computers.  ExamWe recreated all judgments from the WMT08  ples of these Human Intelligence Tasks (or HITs)  German-English News translation task. The  output of the 11 different machine translation systems  comments to providing feedback on relevance of  that participated in this task was scored by ranking  results for a search query.  translated sentences relative to each other. To  colAnyone with an Amazon account can either  lect judgements, we reproduced the WMT08 web  submit HITs or work on HITs that were  submitinterface in Mechanical Turk and provided these  ted by others. Workers are sometimes referred to  instructions:  as Turkers and people designing the HITs are  Evaluate machine translation quality Rank each  transla Requesters. Requesters can specify the amount  tion from Best to Worst relative to the other choices (ties are that they will pay for each item that is completed.  allowed). If you do not know the source language then you Payments are frequently as low as $0.01. Turkers  can read the reference translation, which was created by a are free to select whichever HITs interest them.  professional human translator.  Amazon provides three mechanisms to help  enThe web interface displaced 5 different machine  sure quality: First, Requesters can have each HIT  translations of the same source sentence, and had  be completed by multiple Turkers, which allows  radio buttons to rate them.  higher quality labels to be selected, for instance,  Turkers were paid a grand total of $9.75 to  by taking the majority label. Second, the  Recomplete nearly 1,000 HITs. These HITs  exquester can require that all workers meet a  particuactly replicated the 200 screens worth of expert  lar set of qualications, such as sufficient accuracy  judgments that were collected for the WMT08  on a small test set or a minimum percentage of  German-English News translation task, with each  previously accepted submissions. Finally, the  Rescreen being completed by five different Turkers.  quester has the option of rejecting the work of  inThe Turkers were shown a source sentence, a  refdividual workers, in which case they are not paid.  erence translation, and translations from five MT  The level of good-faith participation by Turkers  systems. They were asked to rank the translations  is surprisingly high, given the generally small  narelative to each other, assigning scores from best  ture of the payment.2 For complex undertakings  to worst and allowing ties.  like creating data for NLP tasks, Turkers do not  We evaluate non-expert Turker judges by  measuring their inter-annotator agreement with the  2For an analysis of the demographics of  TurkWMT08 expert judges, and by comparing the  corers and why they participate,  relation coefficient across the rankings of the  mabehind-the-enemy-lines.blogspot.com/  2008/03/mechanical-turk-demographics.  chine translation systems produced by the two sets  of judges.  (2360 items  compared)  perl analyze.perl sentence-ranking-Batch_25317_result.csv | cat ../wmt08-human-judgments.csv.expert_annotator_id | grep German-English | grep -v Europarl | grep ",RANK,"| perl ../process_judgements.pl run 2  57.8% <--weighted with non-experts  57.8% <--weighted with non-experts (only against same size choose)  53.0% without weighting  <--reported in WMT08 paper  (56 items  compared)  (4960 items  compared)  perl analyze.perl constituent-ranking-Batch_25553_result.csv.itemIDs_fixed | cat ../wmt08-human-judgments.csv.expert_annotator_id | grep German-English | grep -v Europarl | grep ",CONSTITUENT," | perl ../process_judgements.pl run 2  (339 items  compared)  items  compared)  perl analyze-yes-no.perl constituent-yes-no-Batch_40756_result.csv.itemIDs_fixed | cat ../wmt08-human-judgments.csv.expert_annotator_id | grep German-English | grep -v Europarl | grep ",CONSTITUENT_ACCEPT," | perl ../process_judgements.pl run 2  (294 items  compared)  Non-expert agreement with experts  Agreement of individual Turkers v # HITS  M0TLDD  AB1UOP54 66  64RQEK  CSJRS  Unweighted  Weighted by non-experts  Weighted by experts  Number of HITs completed  UHGFM  Figure 2: The agreement of individual Turkers  Number of non-experts voting  Agreement of Individuals on Yes No  with the experts. The most prolific Turker  performed barely above chance, indicating random  translated  sentences increases as more  clicking. This suggests that users who contribute  Weighting non-experts' votes  based on agreement  more tend to have lower quality.  with either experts or other  TMOZ15non-expert increases  it up further. Five weighted non-e  reach the  votes are counted equally.  Weighting votes Not all Turkers are created  equal. The quality of their works varies.  FigA2HNP1YL 22  ure 2 shows the agreement of individual Turkers  A1XH9D9A 10Each item is  redundantly judged by five non-e  xperts. We would  with expert annotators, plotted against the  numNumber of HITs completed  like to combine of their judgments  ber of HITs they completed. The figure shows  it is more  that their agreement varies considerably, and that  complicated than taking simple  Turker who completed the most judgments was  A1LZZD7E majority  voting, in which  among the worst performing.  voters rank a group of candidates  To avoid letting careless annotators drag down  in order of  preference. To create an ordering from  results, we experimented with weighted voting.  the the ranks  assigned to the systems by  We weighted votes in two ways:  2003). It is  guar Votes were weighted by measuring  agreeanteed to correctly pick the winner  that is  prement with experts on the 10 initial judgments  ferred pairwise over the other candidates.  It  furmade. This would be equivalent to giving  ther allows a complete ranking  of candidates to be  Turkers a pretest on gold standard data and  constructed, making it a suitable  method for  comthen calibrating their contribution based on  how well they performed.  Figure 1 shows the effect of  Votes were weighted based on how often one  experts judgments on their  UTEVT  with  exTurker agreed with the rest of the Turkers  perts. Agreement is measured  F5P7TI  by examining each  over the whole data set. This does not  repair of translated sentence and counting  when two  annotators both indicated that  goes beyond simple voting, because it looks  or A = B. Chance agreement  A1O9BHBJ 6 3 . The top line  at a Turker's performance over the entire set,  indicates the inter-annotator  rather than on an item-by-item basis.  WMT08 expert annotators, who  eed with  0.5 each  other 58% of the time. When we  A18B5QL32 5 have only a  sinFigure 1 shows that these weighting mechanisms  for each item,  perform similarly well. For this task, deriving  the agreement with experts is only  AG1TKYUK 4  weights from agreement with other non-experts  increase the number of non-experts  to five, their  is as effective as deriving weights from experts.  agreement with experts impro  Moreover, by weighting the votes of five Turkers,  4 to 53%, if their  AS65HZZ5T 4  A1IER46T8 3  AYUEYT9T 3  K0C6E  A2HNP1YL 2  A1T4TJIPC 2  Correlation with Expert ranking of systems  Non-expert weights  Spearman's correlation  erences were used that Bleu would likely have  stronger correlation. However, it is clear that the  cost of hiring professional translators to create  multiple references for the 2000 sentence test set  would be much greater than the $10 cost of  collecting manual judgments on Mechanical Turk.  5 Feasibility of more complex evaluations  In this section we report on a number of  creative uses of Mechanical Turk to do more  sophisticated tasks. We give evidence that Turkers  can create high quality translations for some  lanexpert weights  guages, which would make creating multiple  reference translations for Bleu less costly than using  Figure 3: Correlation with experts' ranking of  sysprofessional translators. We report on experiments  tems. All of the different ways of combining the  evaluating translation quality with HTER and with  reading comprehension tests.  non-expert judgments perform at the upper bound  of expert-expert correlation. All correlate more  5.1 Creating multiple reference translations  strongly than Bleu.  In addition to evaluating machine translation  quality, we also investigated the possibility of using  we are able to achieve the same rate of agreement  Mechanical Turk to create additional reference  with experts as they achieve with each other.  translations for use with automatic metrics like  Correlation when ranking systems In  addiBleu. Before trying this, we were skeptical that  Turkers would have sufficient language skills to  tion to measuring agreement with experts at  produce translations. Our translation HIT had the  the sentence-level, we also compare non-expert  system-level rankings with experts. Following  following instructions:  Translate these sentences Your task is to translate 10 sen-N.nw.2.7  Callison-Burch et al. (2008), we assigned a score  tences into English. Please make sure that your English N.nw.2.8  to each of the 11 MT systems based on how  oftranslation:  ten its translations were judged to be better than or  Is faithful to the original in both meaning and style N.nw.3.0  equal to any other system. These scores were used  Is grammatical, fluent, and natural-sounding English N.nw.3.2  to rank systems and we measured Spearman's  Does not add or delete information from the original against the system-level ranking produced by ex-N.nw.3.4  Does not contain any spelling errors  Figure 3 shows how well the non-expert  rankWhen creating your translation, please:  ings correlate with expert rankings.  Do not use any machine translation systems  per bound is indicated by the expert-expert bar.  This was created using a five-fold cross  valida You may look up a word on wordreference.com if you  do not know its translation  tion where we used 20% of the expert judgments  Afterwards, we'll ask you a few quick questions about your N.nw.4.3  to rank the systems and measured the correlation  against the rankings produced by the other 80%  We solicited translations for 50 sentences in  of the judgments. This gave a of 0.78. All ways  French, German, Spanish, Chinese and Urdu, and  of combining the non-expert judgments resulted in  designed the HIT so that five Turkers would  transnearly identical correlation, and all produced  corlate each sentence.  relation within the range of with what we would  experts to.  Filtering machine translation Upon inspecting  The rankings produced using Mechanical Turk  the Turker's translations it became clear that many  had a much stronger correlation with the WMT08  had ignored the instructions, and had simply  cutexpert rankings than the Blue score did. It should  and-paste machine translation rather then  translatbe noted that the WMT08 data set does not have  ing the text themselves. We therefore set up a  secmultiple reference translations. If multiple  refond HIT to filter these out. After receiving the  non-expert weights  expert weights 0.7679090909  Topline  Spanish  Spanish STDDEV  Chinese  Chinese STDDEV  Bleu scores of professional translators, Mechanical Turk, and MT  Spanish  Chinese  1 LDC translator v. other LDC translators  Mechanical Turk v. other LDC  MT v. other LDC  Figure 4: Bleu scores quantifying the quality of Turkers' translations. The chart shows the average Bleu score when one LDC translator is compared against the other 10 translators (or the other 2 translators in the case of Urdu). This gives an upper bound on the expected quality. The Turkers' translation quality falls within a standard deviation of LDC translators for Spanish, German and Chinese. For all languages, Turkers produce significantly better translations than an online machine translation system.  translations, we had a second group of Turkers  the Bleu score would be for a professional  transclean the results.  lator when measured against other professionals.  Detect machine translation Please use two online machine We calculated a Bleu score for each of the 11  translation systems to translate the text into English, and then LDC translators using the other 10 translators as  copy-and-paste the translations into the boxes below. Finally, the reference set. The average Bleu score for  look at a list of translations below and click on the ones that LDC2002T01 was 0.54, with a standard deviation  look like they came from the online translation services.  of 0.07. The average Bleu for the Urdu test set is  We automatically excluded Turkers whose  translalower because it has fewer reference translations.  tions were flagged 30% of the time or more.  To measure the Turkers' translation quality, we  randomly selected translations of each sentence  Quality of Turkers' translations Our 50  senfrom Turkers who passed the Detect MT HIT, and  tence test sets were selected so that we could  comcompared them against the same sets of 10  refpare the translations created by Turkers to  translaerence translations that the LDC translators were  tions commissioned by the Linguistics Data  Concompared against. We randomly sampled the  sortium. For the Chinese, French, Spanish, and  Turkers 10 times, and calculated averages and  German translations we used the the  Multiplestandard deviations for each source language.  FigTranslation Chinese Corpus.3 This corpus has  ure 4 the Bleu scores for the Turkers' translations  11 reference human translations for each Chinese  of Spanish, German and Chinese are within the  source sentence. We had bilingual graduate  sturange of the LDC translators. For all languages,  dents translate the first 50 English sentences of  the quality is significantly higher than an online  that corpus into French, German and Spanish, so  machine translation system. We used Yahoo's  Bathat we could re-use the multiple English reference  belfish for Spanish, German, French and Chinese,5  translations. The Urdu sentences were taken from  was likely and Babylon for Urdu.  the NIST MT Eval 2008 Urdu-English Test Set4  which includes three distinct English translations  Demographics We collected demographic  infor every Urdu source sentence.  formation about the Turkers who completed the  Figure 4 shows the Turker's translation quality  translation task. We asked how long they had  spoin terms of the Bleu metric. To establish an upper  ken the source language, how long they had  spobound on expected quality, we determined what  5We also compared against Google Translate, but  excluded the results since its average Bleu score was better than 3LDC catalog number LDC2002T01  the LDC translators, likely because the test data was used to 4LDC catalog number LDC2009E11  train Google's statistical system.  Spanish  English (7 people), Spanish (2), English-Spanish bilingual, Portuguese English, Hindi  Country  Spanish level  30+ years (2 people), 15 years (2), 6 years, 2 years (2), whole life (4) 18 years, 4 years  English level  15 years (3), whole life (9)  whole life , 15 years  German (3), Turkish (2), Italian, Danish, English, Norwegian, Hindi Marathi, Tamil, Hindi, English  Country  Germany (3), USA, Italy, China, Denmark, Turkey, Norway, India USA (2), India (2)  20 years (2), 10 years (3), 5 years (2), 2 years, whole life (3) 10 years, 1 year (2)  English level  20+ years (4), 10-20 years (5) whole life  whole life (2), 15-20 years (2)  English (9 people), Portuguese, Hindi  English (2)  Country  French level  20+ years (4 people), 8-12 years (4), 5 years (2), 2 years 10 years, 1 years, 6 years  English level  whole life (9), 20 years, 15 years  whole life (2),  Chinese  English (3) Hindi, Marathi, Tamil  Country  Chinese level  2 years, 1 year  3 years, 2 years, none  English level  18 years, 20+ years  16 years, whole life (2)  Country  whole life (6 people)  2 years, 1 year, never (2)  English level  20+ years (5), 15 years (2), 10 years  10+ years (5), 5 years  Table 1: Self-reported demographic information from Turkers who completed the translation HIT. The statistics on the left are for people who appeared to do the task honestly. The statistics on the right are for people who appeared to be using MT (marked as using it 20% or more in the Detect MT HIT).  ken English, what their native language was, and  multiple sites and each has a collection of  mawhere they lived. Table 1 gives their replies.  chine translation systems. A general strategy  employed by all teams is to perform system  combiCost and speed We paid Turkers $0.10 to  transnation over these systems to produce a synthetic  late each sentence, and $0.006 to detect whether a  translation that is better than the sum of its parts  sentence was machine translated. The cost is low  (Matusov et al., 2006; Rosti et al., 2007). The  conenough that we could create a multiple reference  tribution of each component system is weighted  set quite cheaply; it would cost less than $1,000 to  by the expectation that it will produce good  outcreate 4 reference translations for 2000 sentences.  put. To our knowledge, none of the teams perform  The time it took for the 250 translations to be  their own HTER evaluations in order to set these  completed for each language varied. It took less  than 4 hours for Spanish, 20 hours for French, 22.5  hours for German, 2 days for Chinese, and nearly  We evaluated the feasibility of using  Mechanical Turk to perform HTER. We simplified the  official GALE post-editing guidelines (NIST and  LDC, 2007). We provided these instructions:  Human-mediated translation edit rate (HTER)  Edit Machine Translation Your task is to edit the machine translation making as few changes as possible so that it is the official evaluation metric of the DARPA  matches the meaning of the human translation and is good GALE program. The evaluation is conducted an-English. Please follow these guidelines:  nually by the Linguistics Data Consortium, and  it is used to determine whether the teams  partic Change the machine translation so that it has the same meaning as the human translation.  ipating the program have met that year's  benchmarks. These evaluations are used as a Go / No  Make the machine translation into intelligible English.  Go determinant of whether teams will continue  Use as few edits as possible.  to receive funding. Thus, each team have a strong  Do not insert or delete punctuation simply to follow incentive to get as good a result as possible under  traditional rules about what is proper.  the metric.  Please do not copy-and-paste the human translation  Each of the three GALE teams encompasses  into the machine translation.  Number of editors  table reports averaged scores when the five  annotators are subsampled. This gives a sense of how  much each additional editor is able to minimize  the score for each system. The difference between  the TER score with zero editors, and the HTER  five editors is greatest for the rmbt5 system, which  has a delta of .29 and is smallest for jhu-tromble  with .07.  Table 2: HTER scores for five MT systems. The  edit rate decreases as the number of editors  in5.3 Reading comprehension  creases from zero (where HTER is simply the TER  score between the MT output and the reference  One interesting technique for evaluating machine  translation) and five.  translation quality is through reading  comprehension questions about automatically translated text.  The quality of machine translation systems can be  We displayed 10 sentences from a news article. In  quantified based on how many questions are  anone column was the reference English translation,  swered correctly.  in the other column were text boxes containing  the MT output to be edited. To minimize the edit  Jones et al. (2005) evaluated translation quality  rate, we collected edits from five different Turkers  using a reading comprehension test the Defense  for every machine translated segment. We verified  Language Proficiency Test (DLPT), which is  adthese with a second HIT were we prompted  Turkministered to military translators. The DLPT  coners to:  tains a collection of foreign articles of varying  levels of difficulties, and a set of short answer  quesJudge edited translations First, read the reference human translation. After that judge the edited machine translation tions. Jones et al used the Arabic DLPT to do a  using two criteria:  study of machine translation quality, by  automatically translating the Arabic documents into  En Does the edited translation have the same meaning as the reference human translation?  glish and seeing how many human subjects could  successfully pass the exam.  Is it acceptable English? Some small errors are OK, so long as its still understandable.  The advantage of this type of evaluation is that  the results have a natural interpretation. They  indiFor the final score, we choose the edited segment  cate how understandable the output of a machine  which passed the criteria and which minimized the  translation system is better than Bleu does, and  edit distance to the unedited machine translation  better than other manual evaluation like the  relaoutput. If none of the five edits was deemed to be  tive ranking. Despite this advantage, evaluating  acceptable, then we used the edit distance between  MT through reading comprehension hasn't caught  the MT and the reference.  on, due to the difficulty of administering it and due  to the fact that the DLPT or similar tests are not  Setup We evaluated five machine translation  systems using HTER. These systems were  selected from WMT09 (Callison-Burch et al., 2009).  We conducted a reading comprehension  evaluaWe wanted a spread in quality, so we took the top  tion using Mechanical Turk. Instead of simply  adtwo and bottom two systems from the  Germanministering the test on Mechanical Turk, we used  English task, and the top system from the  Frenchit for all aspects from test creation to answer  gradEnglish task (which significantly outperformed  ing. Our procedure was as follows:  everything else). Based on the results of the  Test creation We posted human translations of  WMT09 evaluation we would expect the see the  foreign news articles, and ask Tukers to write three  following ranking from the least edits to the most  simple instructions on what qualifies as a good  reading comprehension question.  Results Table 2 gives the HTER scores for the  Reading comprehension test Please read the short  newsfive systems. Their ranking is as predicted,  indipaper article, and then write three reading comprehension questions about it, giving sample answers for each of your cating that the editing is working as expected. The  Ask about why something happened or why someone System  % Correct Answers  did something.  reference  Ask about relationships between people or things.  Should be answerable in a few words.  Poor reading comprehension questions:  Ask about numbers or dates.  Only require a yes/no answer.  Question selection We posted the questions for  Table 3: The results of evaluating the MT output  each article back to Mechanical Turk, and asked  using a reading comprehension test  other Turkers to vote on whether each question  was a good and to indicate if it was redundant with  times per article. As a control, we had three  Turkany other questions in the set. We sorted questions  ers answer the reading comprehension questions  to maximize the votes and minimized  redundanusing the reference translation.  cies using a simple perl script, which discarded  Table 3 gives the percent of questions that were  questions below a threshold, and eliminated all  recorrectly answered using each of the different  systems' outputs and using the reference translation.  Taking the test We posted machine translated  The ranking is exactly what we would expect,  versions of the foreign articles along with the  based on the HTER scores and on the human  evalquestions, and had Turkers answer them. We  enuation of the systems in WMT09. This again  sured that no one would see multiple translations  helps to validate that the reading comprehension  of the same article.  methodology. The scores are more interpretable  Answer questions about a machine translated text You will than Blue scores and than the WMT09 relative  answer questions about an article that has been automat-rankings, since it gives an indication of how  unically translated from another language into English. The derstandable the MT output is.  translation contains many errors, but the goal is to see how understandable it is. Please do your best to guess at the right Appendix A shows some sample questions and  answers to the questions. Please:  answers for an article.  Read through the automatically translated article.  Answer the questions listed below, using just a few  Mechanical Turk is an inexpensive way of  gather Give your best guess at the answers, even if the trans-ing human judgments and annotations for a wide  lation is hard to understand.  variety of tasks. In this paper we demonstrate  Don't use any other information to answer the  questhat it is feasible to perform manual evaluations  tions.  of machine translation quality using the web  serGrading the answers We aggregated the  vice. The low cost of the non-expert labor found  answers and used Mechanical Turk to grade  on Mechanical Turk is cheap enough to collect  rethem. We showed the human translation of the  dundant annotations, which can be utilized to  enarticle, one question, the sample answer, and  sure translation quality. By combining the  judgdisplayed all answers to it. After the Turkers  ments of many non-experts we are able to achieve  graded the answers, we calculated the percentage  the equivalent quality of experts.  of questions that were answered correctly for each  The suggests that manual evaluation of  translation quality could be straightforwardly done to  validate performance improvements reported in  Turkers created 90 questions for 10 articles, which  conference papers, or even for mundane tasks  were subsequently filtered down to 47 good  queslike tracking incremental system updates. This  challenges the conventional wisdom which has  Turkers answered questions about each translated  long held that automatic metrics must be used  article. To avoid them answering the questions  since manual evaluation is too costly and  timemultiple times, we randomly selected which  sysconsuming.  tem's translation was shown to them. Each  sysWe have shown that Mechanical Turk can be  tem's translation was displayed an average of 5  used creatively to produce quite interesting things.  We showed how a reading comprehension test  Why did the bystander call emergency services?  could be created, administered, and graded, with  He was concerned for Ms. Locklear's life.  only very minimal intervention.  Why was Heather Locklear arrested in Santa Barbara?  Because she was driving under the influence of drugs  We believe that it is feasible to use Mechanical  Where did the witness see her acting abnormally?  Turk for a wide variety of other machine translated  Pulling out of parking in Montecito  tasks like creating word alignments for sentence  Where was Ms. Locklear two months ago?  pairs, verifying the accuracy of and  She was at a specialist clinic in Arizona.  sentence-alignments, performing non-simulated  5 questions were excluded as being redundant  active learning experiments for statistical machine  translation, even collecting training data for low  What was Heather Locklear arrested for?  Driving under the influence of drugs  Where was she taken for testing?  The cost of using Mechanical Turk is low  A specialized centre for drugs and alcohol  enough that we might consider attempting  Why was Heather Locklear arrested?  quixotic things like human-in-the-loop minimum  She was arested on suspicion of driving under the  influence of drugs.  error rate training (Zaidan and Callison-Burch,  Why did the policemen lead her to a specialized centre 2009), or doubling the amount of training data  for drugs and alcohol  Because she seemed confused.  For what was she cured for two months ago?  Acknowledgments  She was cured for anxiety and depression.  Answers to Where was Ms. Locklear two months ago?  This research was supported by the  EuroMatrixthat were judged to be correct:  Plus project funded by the European Commission,  Arizona hospital for treatment of depression; at a treat-and by the US National Science Foundation under  mend clinic in Arizona; in the Arizona clinic being treated for nervous breakdown; a clinic in Arizona; Arizona, un-grant IIS-0713448. The views and findings are the  der treatment for depression; She was a patient in a clinic author's alone.  in Arizona undergoing treatment for anxiety and depression; In an Arizona mental health facility ; A clinic in Arizona.; A Example reading comprehension  In a clinic being treated for anxiety and depression.; at an Arizona clinic  These answers were judged to be incorrect: Locklear  Actress Heather Locklear arrested for driving under the Ms.Locklaer were laid off after a treatment out of the clinic influence of drugs  The actress Heather Locklear, Amanda on the popular series Melrose Place, was arrested this weekend in Santa Barbara (California) after driving under the influence of drugs. A witness saw her performing inappropriate maneuvers while References  trying to take her car out of a parking space in Montecito, as Bogdan Babych and Anthony Hartley. 2004. Extend-revealed to People magazine by a spokesman for the Californian Highway Police. The witness stated that around 4.30pm ing the Bleu MT evaluation method with frequency  Ms. Locklear hit the accelerator very roughly, making ex-weightings. In Proceedings of ACL.  cessive noise and trying to take the car out from the parking space with abrupt back and forth maneuvers. While re-Chris Callison-Burch, Miles Osborne, and Philipp  versing, she passed several times in front of his sunglasses.  Koehn. 2006. Re-evaluating the role of Bleu in  maShortly after, the witness, who at first, apparently had not rec-chine translation research. In Proceedings of EACL.  ognized the actress, saw Ms. Locklear stopping in a nearby street and leaving the vehicle.  Chris Callison-Burch, Cameron Fordyce, Philipp  It was this person who alerted the emergency services, be-Koehn, Christof Monz, and Josh Schroeder. 2008.  cause he was concerned about Ms. Locklear's life. When Further meta-evaluation of machine translation. In  the patrol arrived, the police found the actress sitting inside Proceedings of the Third Workshop on Statistical  her car, which was partially blocking the road. She seemed Machine Translation (WMT08).  confused, so the policemen took her to a specialized centre for drugs and alcohol and submitted her a test. According to a Chris Callison-Burch, Philipp Koehn, Christof Monz,  spokesman for the police, the actress was cooperative and ex-and Josh Schroeder. 2009. Findings of the 2009  cessive alcohol was ruled out from the beginning, even if as the officers initially observed, we believe Ms. Locklear was Workshop on Statistical Machine Translation. In  under the influences drugs. Ms. Locklear was arrested under Proceedings of the Fourth Workshop on Statistical  suspicion of driving under the influence of some unspecified Machine Translation (WMT09), March.  substance, and imprisoned in the local jail at 7.00pm, to be released some hours later. Two months ago, Ms. Locklear was David Chiang, Steve DeNeefe, Yee Seng Chan, and  released from a specialist clinic in Arizona where she was Hwee Tou Ng. 2008. Decomposability of trans-treated after an episode of anxiety and depression.  lation metrics for improved evaluation and efficient  4 questions were selected  algorithms. In Proceedings of EMNLP.  translation quality by testing English speakers with  a new defense language proficiency test for Arabic.  In Proceedings of the 2005 International Conference  on Intelligence Analysis.  Assessment of fluency and adequacy in translations.  Evgeny Matusov, Nicola Ueffing, and Hermann Ney.  2006. Computing consensus translation for multiple  machine translation systems using enhanced  hypothesis alignment. In Proceedings of EACL.  NIST and LDC. 2007. Post editing guidelines for  developed by the National Institute of Standards and  Technology (NIST), and the Linguistic Data  ConsorJing Zhu. 2002. Bleu: A method for automatic  evaluation of machine translation. In Proceedings  of ACL.  Michael Paul. 2006. Overview of the IWSLT 2006  evaluation campaign. In Proceedings of  International Workshop on Spoken Language Translation.  Dorr. 2007. Combining outputs from multiple  machine translation systems. In Proceedings of  Markus Schulze. 2003. A new monotonic and  cloneindependent single-winner election method. Voting  Matters, (17), October.  nea Micciulla, and John Makhoul. 2006. A study of  translation edit rate with targeted human annotation.  In Proceedings of AMTA.  Rion Snow, Brendan O'Connor, Daniel Jurafsky, and  Andrew Y. Ng. 2008. Cheap and fast but is it  good? Evaluating non-expert annotations for natural  language tasks. In Proceedings of EMNLP.  Omar F. Zaidan and Chris Callison-Burch. 2009.  Feasibility of human-in-the-loop minimum error rate  training. In Proceedings of EMNLP. 