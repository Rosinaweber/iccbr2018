 Extracting Product Features and Opinions from Reviews Ana-Maria Popescu and Oren Etzioni  Department of Computer Science and Engineering  University of Washington  Seattle, WA 98195-2350  {amp, etzioni}@cs.washington.edu  We decompose the problem of review mining into the  Consumers are often forced to wade  following main subtasks:  through many on-line reviews in  I. Identify product features.  order to make an informed  prodII. Identify opinions regarding product features.  uct choice.  This paper introduces  III. Determine the polarity of opinions.  OPINE, an unsupervised  informationIV. Rank opinions based on their strength.  extraction system which mines  reThis paper introduces  views in order to build a model of  imOPINE, an unsupervised  information extraction system that embodies a solution to each portant product features, their evalu-of the above subtasks.  ation by reviewers, and their relative  OPINE is built on top of the  KnowItAll Web information-extraction system (Etzioni et al., quality across products.  2005) as detailed in Section 3.  Compared to previous work, OPINE  achieves 22% higher precision (with  Given a particular product and a corresponding set of  only 3% lower recall) on the feature  reviews, OPINE solves the opinion mining tasks outlined  extraction task.  above and outputs a set of product features, each accom-OPINE's novel use of  relaxation labeling for finding the  sepanied by a list of associated opinions which are ranked mantic orientation of words in con-based on strength ( e.g. , abominable is stronger than text leads to strong performance on  bad). This output information can then be used to  genthe tasks of finding opinion phrases  erate various types of opinion summaries.  and their polarity.  This paper focuses on the first 3 review mining  subtasks and our contributions are as follows:  Introduction  1. We introduce OPINE, a review-mining system whose  The Web contains a wealth of opinions about products,  novel components include the use of relaxation labeling politicians, and more, which are expressed in newsgroup  to find the semantic orientation of words in the context of posts, review sites, and elsewhere. As a result, the prob-given product features and sentences.  lem of opinion mining has seen increasing attention  2. We compare OPINE with the most relevant previous  over the last three years from (Turney, 2002; Hu and Liu, review-mining system (Hu and Liu, 2004) and find that  2004) and many others. This paper focuses on product  OPINE's precision on the feature extraction task is 22%  reviews, though our methods apply to a broader range of  better though its recall is 3% lower on Hu's data sets. We opinions.  show that 1/3 of this increase in precision comes from  Product reviews on Web sites such as amazon.com  using OPINE's feature assessment mechanism on review and elsewhere often associate meta-data with each review data while the rest is due to Web PMI statistics.  indicating how positive (or negative) it is using a 5-star 3. While many other systems have used extracted opin-scale, and also rank products by how they fare in the re-ion phrases in order to determine the polarity of sentences views at the site. However, the reader's taste may differ or documents, OPINE is the first to report its precision and from the reviewers'. For example, the reader may feel  recall on the tasks of opinion phrase extraction and opin-strongly about the quality of the gym in a hotel, whereas ion phrase polarity determination in the context of known many reviewers may focus on other aspects of the ho-product features and sentences. On the first task, OPINE  tel, such as the decor or the location. Thus, the reader is has a precision of 79% and a recall of 76%. On the sec-forced to wade through a large number of reviews looking ond task, OPINE has a precision of 86% and a recall of  for information about particular features of interest.  Input: product class C, reviews R.  to find explicit product features (E). OPINE's Feature As-Output: set of [feature, ranked opinion list] tuples  sessor and its use of Web PMI statistics are vital for the R' parseReviews(R);  extraction of high-quality features (see 3.2). OPINE then E findExplicitFeatures(R', C);  identifies opinion phrases associated with features in E  O findOpinions(R', E);  and finds their polarity. OPINE's novel use of relaxation-CO clusterOpinions(O);  labeling techniques for determining the semantic  orienI findImplicitFeatures(CO, E);  tation of potential opinion words in the context of given features and sentences leads to high precision and recall RO rankOpinions(CO);  on the tasks of opinion phrase extraction and opinion  phrase polarity extraction (see 3.3).  In this paper, we only focus on the extraction of  explicit features, identifying corresponding customer opinThe remainder of this paper is organized as follows:  ions about these features and determining their polarity.  Section 2 introduces the basic terminology, Section 3  We omit the descriptions of the opinion clustering,  imgives an overview of OPINE, describes and evaluates its  plicit feature generation and opinion ranking algorithms.  main components, Section 4 describes related work and  The KnowItAll System.  Terminology  OPINE is built on top of KnowItAll, a Web-based,  domain-independent information extraction system  (EtA product class ( e.g. , Scanner) is a set of products ( e.g. , zioni et al., 2005). Given a set of relations of interest, Epson1200). OPINE extracts the following types of prod-KnowItAll instantiates relation-specific generic  extracuct features: properties, parts, features of product parts, tion patterns into extraction rules which find candidate related concepts, parts and properties of related concepts facts. KnowItAll's Assessor then assigns a probability to (see Table 1 for examples of such features in the Scan-each candidate. The Assessor uses a form of Point-wise ner domains). Related concepts are concepts relevant to Mutual Information (PMI) between phrases that is esti-the customers' experience with the main product ( e.g. , mated from Web search engine hit counts (Turney, 2001).  the company that manufactures a scanner). The  relationIt computes the PMI between each fact and automatically ships between the main product and related concepts are  generated discriminator phrases ( e.g. , is a scanner for typically expressed as verbs ( e.g. , Epson manufactures the isA() relationship in the context of the Scanner  scanners ) or prepositions ( scanners from Epson ). Fea-class). Given fact f and discriminator d, the computed  tures can be explicit ( good scan quality ) or im-PMI score is:  plicit ( good scans implies good ScanQuality).  Hits(d + f )  OPINE also extracts opinion phrases, which are adjec-Hits(d) Hits(f )  tive, noun, verb or adverb phrases representing customer The PMI scores are converted to binary features for a  opinions. Opinions can be positive or negative and vary Naive Bayes Classifier, which outputs a probability asso-in strength ( e.g. , fantastic is stronger than good ).  ciated with each fact (Etzioni et al., 2005).  Finding Explicit Features  This section gives an overview of  OPINE extracts explicit features for the given product OPINE (see Figure 1)  and describes its components and their experimental eval-class from parsed review data. First, the system  recursively identifies both the parts and the properties of the given product class and their parts and properties, in turn, Goal Given product class C with instances I and re-continuing until no candidates are found. Then, the  sysviews R, OPINE's goal is to find a set of (feature, opin-tem finds related concepts as described in (Popescu et ions) tuples {(f, oi, ...oj)} s.t. f F and oi, ...oj O, al., 2004) and extracts their parts and properties. Table 1  where:  shows that each feature type contributes to the set of final a) F is the set of product class features in R.  features (averaged over 7 product classes).  b) O is the set of opinion phrases in R.  c) f is a feature of a particular product instance.  d) o is an opinion about f in a particular sentence.  d) the opinions associated with each feature f are  ranked based on their strength.  Features of Parts  BatteryLife  Solution The steps of our solution are outlined in Fig-Related Concepts  OPINE parses the reviews using  MINIRelated Concepts' Features  module to parsed review data. OPINE then uses the data  Table 1: Explicit Feature Information  In order to find parts and properties, OPINE first ex-Data  Explicit Feature Extraction: Precision  tracts the noun phrases from reviews and retains those  Hu+A/R+W  with frequency greater than an experimentally set thresh-D1  old. OPINE's Feature Assessor, which is an instantia-D2  tion of KnowItAll's Assessor, evaluates each noun phrase D3  by computing the PMI scores between the phrase and  meronymy discriminators associated with the product D5  class ( e.g. , of scanner , scanner has , scanner comes Avg  with , etc.  for the Scanner class).  guishes parts from properties using WordNet's IS-A  hiTable 2: Precision Comparison on the Explicit Feature-erarchy (which enumerates different kinds of properties) Extraction Task. OPINE's precision is 22% better than Hu's precision; Web PMI statistics are responsible for 2/3 of the pre-and morphological cues ( e.g. , -iness , -ity suffixes).  cision increase. All results are reported with respect to Hu's.  In our experiments we use sets of reviews for 7  product classes (1621 total reviews) which include the  pubHu+A/R+W  licly available data sets for 5 product classes from (Hu D1  and Liu, 2004). Hu's system is the review mining  system most relevant to our work. It uses association rule  mining to extract frequent review noun phrases as fea-D4  tures. Frequent features are used to find potential opin-D5  ion words (only adjectives) and the system uses Word-Avg  Net synonyms/antonyms in conjunction with a set of seed  words in order to find actual opinion words. Finally, opin-Table 3:  Recall Comparison on the Explicit  Featureion words are used to extract associated infrequent fea-Extraction Task. OPINE's recall is 3% lower than the recall tures. The system only extracts explicit features.  of Hu's original system (precision level = 0.8). All results are reported with respect to Hu's.  On the 5 datasets in (Hu and Liu, 2004), OPINE's  precision is 22% higher than Hu's at the cost of a 3%  recall drop. There are two important differences between  thermore, the annotators extracted explicit features from OPINE and Hu's system: a) OPINE's Feature Assessor  800 review sentences (400 for each domain). The  interuses PMI assessment to evaluate each candidate feature  annotator agreement was 82%. OPINE's recall on the  and b) OPINE incorporates Web PMI statistics in addition set of 179 features on which both annotators agreed was  to review data in its assessment. In the following, we  quantify the performance gains from a) and b).  a) In order to quantify the benefits of OPINE's Feature  Finding Opinion Phrases and Their Polarity  Assessor, we use it to evaluate the features extracted by Hu's algorithm on review data (Hu+A/R). The Feature This subsection describes how OPINE extracts potential  Assessor improves Hu's precision by 6%.  opinion phrases, distinguishes between opinions and  nonb) In order to evaluate the impact of using Web PMI  opinions, and finds the polarity of each opinion in the statistics, we assess OPINE's features first on reviews  context of its associated feature in a particular review  sen(OP/R) and then on reviews in conjunction with the tence.  Web (the corresponding methods are Hu+A/R+W and  Extracting Potential Opinion Phrases  OPINE). Web PMI statistics increase precision by an average of 14.5%.  OPINE uses explicit features to identify potential  opinOverall, 1/3 of OPINE's precision increase over Hu's ion phrases. Our intuition is that an opinion phrase as-system comes from using PMI assessment on reviews and  sociated with a product feature will occur in its vicinity.  the other 2/3 from the use of the Web PMI statistics.  This idea is similar to that of (Kim and Hovy, 2004) and In order to show that OPINE's performance is robust  (Hu and Liu, 2004), but instead of using a window of size across multiple product classes, we used two sets of re-k or the output of a noun phrase chunker, OPINE takes  views downloaded from tripadvisor.com for  Hoadvantage of the syntactic dependencies computed by the  tels and amazon.com for Scanners. Two annotators  laMINIPAR parser. Our intuition is embodied by 10  exbeled a set of unique 450 OPINE extractions as correct traction rules, some of which are shown in Table 4. If or incorrect. The inter-annotator agreement was 86%.  an explicit feature is found in a sentence, OPINE applies The extractions on which the annotators agreed were used the extraction rules in order to find the heads of potential to compute OPINE's precision, which was 89%. Fur-opinion phrases. Each head word together with its  modifiers is returned as a potential opinion phrase1.  3. Given the set of SO labels for (w, f ) pairs, OPINE  finds a SO label for each (w, f , s) input tuple.  Each of these subtasks is cast as an unsupervised col-if (M, N P = f ) po = M  lective classification problem and solved using the same if (S = f, P, O) po = O  mechanism. In each case, OPINE is given a set of  obI (hate) this scanner  jects (words, pairs or tuples) and a set of labels (SO la-if (S = f, P, O) po = P  program (crashed)  bels); OPINE then searches for a global assignment of labels to objects. In each case, OPINE makes use of local Table 4:  Examples of Domain-independent Rules for  constraints on label assignments ( e.g. , conjunctions and the Extraction of Potential Opinion Phrases.  disjunctions constraining the assignment of SO labels to tion:  S=subject, P=predicate, O=object. Extracted phrases are en-words (Hatzivassiloglou and McKeown, 1997)).  closed in parentheses. Features are indicated by the typewriter A key insight in OPINE is that the problem of searching  font. The equality conditions on the left-hand side use po's for a global SO label assignment to words, pairs or tuples head.  while trying to satisfy as many local constraints on assignments as possible is analogous to labeling problems  Rule Templates  in computer vision ( e.g. , model-based matching). OPINE  uses a well-known computer vision technique, relaxation  the three subtasks described above.  Table 5: Dependency Rule Templates For Finding Words  Relaxation Labeling Overview  w, w' with Related SO Labels .  OPINE instantiates these  Relaxation labeling is an unsupervised classification  templates in order to obtain extraction rules.  Notation:  technique which takes as input:  a) a set of objects ( e.g. , words)  OPINE examines the potential opinion phrases in order  to identify the actual opinions. First, the system finds the c) initial probabilities for each object's possible labels semantic orientation for the lexical head of each poten-d) the definition of an object o's neighborhood (a set of tial opinion phrase. Every phrase whose head word has a  other objects which influence the choice of o's label)  positive or negative semantic orientation is then retained e) the definition of neighborhood features  as an opinion phrase. In the following, we describe how f) the definition of a support function for an object label OPINE finds the semantic orientation of words.  The influence of an object o's neighborhood on its  label L is quantified using the support function. The sup-3.3.2  Word Semantic Orientation  port function computes the probability of the label L be-OPINE finds the semantic orientation of a word w in  ing assigned to o as a function of o's neighborhood fea-the context of an associated feature f and sentence s. We tures. Examples of features include the fact that a certain restate this task as follows:  local constraint is satisfied ( e.g. , the word nice partic-Task Given a set of semantic orientation (SO) labels ipates in the conjunction and together with some other ({positive, negative, neutral}), a set of reviews and a  word whose SO label is estimated to be positive).  set of tuples (w, f , s), where w is a potential opinion Relaxation labeling is an iterative procedure whose  word associated with feature f in sentence s, assign a SO  output is an assignment of labels to objects. At each itera-label to each tuple (w, f , s).  tion, the algorithm uses an update equation to reestimate For example, the tuple ( sluggish, driver, I am not the probability of an object label based on its previous happy with this sluggish driver ) would be assigned a  probability estimate and the features of its neighborhood.  The algorithm stops when the global label assignment  Note: We use word to refer to a potential opinion stays constant over multiple consecutive iterations.  word w and feature to refer to the word or phrase which We employ relaxation labeling for the following rea-represents the explicit feature f .  sons: a) it has been extensively used in computer-vision Solution OPINE uses the 3-step approach below:  with good results b) its formalism allows for many types 1. Given the set of reviews, OPINE finds a SO label for  of constraints on label assignments to be used  simultaneously. As mentioned before, constraints are integrated 2. Given the set of reviews and the set of SO labels for into the algorithm as neighborhood features which influ-words w, OPINE finds a SO label for each (w, f ) pair.  ence the assignment of a particular label to a particular 1The (S,P,O) tuples in Table 4 are automatically generated object.  from MINIPAR's output.  OPINE uses the following sources of constraints:  a) conjunctions and disjunctions in the review text for a subset S of the words.  b) manually-supplied syntactic dependency rule  temscore so(w) for each w in S as the difference between plates (see Table 5). The templates are automatically in-the PMI of w with positive keywords ( e.g. , excellent ) stantiated by our system with different dependency re-and the PMI of w with negative keywords ( e.g. , awful ).  lationships (premodifier, postmodifier, subject, etc.) in When so(w) is small, or w rarely co-occurs with the key-order to obtain syntactic dependency rules which find  words, w is classified as neutral. If so(w) > 0, then words with related SO labels.  w is positive, otherwise w is negative. OPINE then uses c) automatically derived morphological relationships the labeled S set in order to compute prior probabilities ( e.g. , wonderful and wonderfully are likely to have P (l(w) = L), L {pos, neg, neutral} by computing  the ratio between the number of words in S labeled L  d) WordNet-supplied synonymy, antonymy, IS-A and and |S|. Such probabilities are used as initial probabil-morphological relationships between words. For exam-ity estimates associated with the labels of the remaining ple, clean and neat are synonyms and so they are likely words.  to have similar SO labels.  Support Function The support function computes the Each of the SO label assignment subtasks previously  probability of each label for word w based on the labels identified is solved using a relaxation labeling step. In the of objects in w's neighborhood N .  following, we describe in detail how relaxation labeling Let Ak = {(wj, Lj)|wj N } , 0 < k 3|N| rep-is used to find SO labels for words in the given review  resent one of the potential assignments of labels to the sets.  words in N . Let P (Ak)(m) denote the probability of this 3.3.4  particular assignment at iteration m. The support for label L of word w at iteration m is :  For many words, a word sense or set of senses is used  throughout the review corpus with a consistently positive, X  etc.). Thus, in many cases, a word w's SO label in the  We assume that the labels of w's neighbors are  indecontext of a feature f and sentence s will be the same as pendent of each other and so the formula becomes:  its SO label in the context of other features and sentences.  In the following, we describe how  beling mechanism is used to find a word's dominant SO  Every P (l(w  label in a set of reviews.  j ) = Lj )(m) term is the estimate for the  probability that l(w  For this task, a word's neighborhood is defined as j ) = Lj (which was computed at iteration m using the RL update equation).  the set of words connected to it through conjunctions,  The P (l(w) = L|A  disjunctions and all other relationships previously intro-k )(m) term quantifies the influence  of a particular label assignment to w's neighborhood over duced as sources of constraints.  w's label. In the following, we describe how we estimate RL uses an update equation to re-estimate the prob-this term.  ability of a word label based on its previous  probabilNeighborhood Features  ity estimate and the features of its neighborhood (see  Each type of word relationship which constrains the  Neighborhood Features). At iteration m, let q(w, L)(m) assignment of SO labels to words (synonymy, antonymy,  denote the support function for label L of w and let  etc.) is mapped by OPINE to a neighborhood feature. This P (l(w) = L)(m) denote the probability that L is the label mapping allows OPINE to use simultaneously use multi-of w. P (l(w) = L)(m+1) is computed as follows:  ple independent sources of constraints on the label of a RL Update Equation (Rangarajan, 2000)  particular word. In the following, we formalize this map-P (l(w) = L)  Let T denote the type of a word relationship in R  (synL0 P (l(w) = L0)(m)(1 + q(w, L0)(m))onym, antonym, etc.) and let Ak,T represent the labels where L0 {pos, neg, neutral} and > 0 is an  assigned by Ak to neighbors of a word w which are  conexperimentally set constant keeping the numerator and  nected to w through a relationship of type T . We have  probabilities positive. RL's output is an assignment of  k,T and  dominant SO labels to words.  In the following, we describe in detail the initialization T  For each relationship type T , OPINE defines a  step, the derivation of the support function formula and neighborhood feature fT (w, L, Ak,T ) which computes the use of neighborhood features.  P (l(w) = L|Ak,T ), the probability that w's label is L  RL Initialization Step OPINE uses a version of Tur-given Ak,T (see below). P (l(w) = L| S A  ney's PMI-based approach (Turney, 2003) in order to  deestimated combining the information from various  fearive the initial probability estimates (P (l(w) = L)(0)) tures about w's label using the sigmoid function ():  relationships between words and, respectively, features  in order to update the SO labels when necessary. For  where c0, ...cj are weights whose sum is 1 and which  example, in the sentence I hated the big, drafty room  reflect OPINE 's confidence in each type of feature.  because I ended up freezing. , big and hate satisfy  Given word w, label L, relationship type T and  neighcondition 2 in Table 5 and therefore OPINE expects them  borhood label assignment Ak, let NT represent the subset to have similar SO labels. Since hate has a strong neg-of w's neighbors connected to w through a type T  relaative connotation, big acquires a negative SO label in tionship. The feature fT computes the probability that  this context.  w's label is L given the labels assigned by Ak to words  In order to correctly update SO labels in this last step, in NT . Using Bayes's Law and assuming that these la-OPINE takes into consideration the presence of negation bels are independent given l(w), we have the following  modifiers. For example, in the sentence I don't like a formula for fT at iteration m:  large scanner either , OPINE first replaces the positive (w, f ) pair (like, scanner) with the negative labeled pair  (not like, scanner) and then infers that large is likely to T (w, L, Ak,T )(m) = P (l(w) = L)(m)  have a negative SO label in this context.  P (Lj|l(w) = L) is the probability that word wj has label 3.3.7  Identifying Opinion Phrases  Lj if wj and w are linked by a relationship of type T and w has label L. We make the simplifying assumption that  After OPINE has computed the most likely SO labels  this probability is constant and depends only of T , L and for the head words of each potential opinion phrase in the L0, not of the particular words w  context of given features and sentences, OPINE can  exj and w. For each tuple  tract opinion phrases and establish their polarity. Phrases j ), L, Lj {pos, neg, neutral}, OPINE builds  a probability table using a small set of bootstrapped pos-whose head words have been assigned positive or nega-itive, negative and neutral words.  tive labels are retained as opinion phrases. Furthermore, 3.3.5  Finding (Word, Feature) SO Labels  the polarity of an opinion phrase o in the context of a feature f and sentence s is given by the SO label assigned to This subtask is motivated by the existence of frequent  the tuple (head(o), f, s) (3.3.6 shows how  words which change their SO label based on associated  features, but whose SO labels in the context of the respective features are consistent throughout the reviews ( e.g. , 3.4  in the Hotel domain, hot water has a consistently posi-In this section we evaluate OPINE's performance on the  tive connotation, whereas hot room has a negative one).  following tasks: finding SO labels of words in the  conIn order to solve this task, OPINE first assigns each  text of known features and sentences ( SO label  extrac(w, f ) pair an initial SO label which is w's SO label. The tion); distinguishing between opinion and non-opinion system then executes a relaxation labeling step during  phrases in the context of known features and sentences  which syntactic relationships between words and,  respec( opinion phrase extraction); finding the correct polarity tively, between features, are used to update the default of extracted opinion phrases in the context of known fea-SO labels whenever necessary. For example, (hot, room) tures and sentences ( opinion phrase polarity extraction).  appears in the proximity of (broken, fan). If room and While other systems, such as (Hu and Liu, 2004;  Tur fan are conjoined by and, this suggests that hot and ney, 2002), have addressed these tasks to some degree,  broken have similar SO labels in the context of their  OPINE is the first to report results. We first ran OPINE on respective features. If broken has a strongly negative 13841 sentences and 538 previously extracted features.  semantic orientation, this fact contributes to OPINE's be-OPINE searched for a SO label assignment for 1756  diflief that hot may also be negative in this context. Since ferent words in the context of the given features and  sen(hot, room) occurs in the vicinity of other such phrases tences. We compared OPINE against two baseline  meth( e.g. , stifling kitchen), hot acquires a negative SO label ods, PMI++ and Hu++.  in the context of room .  PMI++ is an extended version of (Turney, 2002)'s  Finding (Word, Feature, Sentence) SO Labels  method for finding the SO label of a phrase (as an  atThis subtask is motivated by the existence of (w,f )  tempt to deal with context-sensitive words). For a given pairs ( e.g. , (big, room)) for which w's orientation changes (word, feature, sentence) tuple, PMI++ ignores the sen-based on the sentence in which the pair appears ( e.g. , I tence, generates a phrase based on the word and the fea-hated the big, drafty room because I ended up freezing.  ture ( e.g. , (clean, room): clean room ) and finds its SO  vs. We had a big, luxurious room .)  label using PMI statistics. If unsure of the label, PMI++  In order to solve this subtask, OPINE first assigns each tries to find the orientation of the potential opinion word (w, f, s) tuple an initial label which is simply the SO la-instead. The search engine queries use domain-specific  bel for the (w, f ) pair. The system then uses syntactic keywords ( e.g. , scanner ), which are dropped if they  lead to low counts.  or negative opinion words, which account for the  majorHu++ is a WordNet-based method for finding a word's ity of opinion instances. The method's loss in recall is context-independent semantic orientation.  It extends  due to not recognizing words absent from WordNet ( e.g. , Hu's adjective labeling method in a number of ways in  depth-adjustable ) or not having enough information to  order to handle nouns, verbs and adverbs in addition to  classify some words in WordNet.  adjectives and in order to improve coverage. Hu's method PMI++ typically does well in the presence of strongly starts with two sets of positive and negative words and  positive or strongly negative words. Its high recall is  iteratively grows each one by including synonyms and  correlated with decreased precision, but overall this sim-antonyms from WordNet. The final sets are used to  preple approach does well. PMI++'s main shortcoming is dict the orientation of an incoming word.  misclassifying terms such as basic or visible which  change orientation based on context.  Experiments: Opinion Phrases  In order to evaluate OPINE on the tasks of opinion  phrase extraction and opinion phrase polarity extraction nn  in the context of known features and sentences, we used a vb  set of 550 sentences containing previously extracted fea-adv  tures. The sentences were annotated with the opinion  phrases corresponding to the known features and with the opinion polarity. We compared OPINE with PMI++ and Table 6: Finding SO Labels of Potential Opinion Words Hu++ on the tasks of interest. We found that OPINE had in the Context of Given Product Features and Sentences.  the highest precision on both tasks at a small loss in re-OPINE's precision is higher than that of PMI++ and Hu++.  call with respect to PMI++. OPINE's ability to identify All results are reported with respect to PMI++ . Notation: adj=adjectives, nn=nouns, vb=verbs, adv=adverbs  a word's SO label in the context of a given feature and  sentence allows the system to correctly extract opinions expressed by words such as big or small , whose se-3.4.1  mantic orientation varies based on context.  On the task of finding SO labels for words in the context of given features and review sentences, OPINE obtains Measure  higher precision than both baseline methods at a small  OP Extraction: Precision  loss in recall with respect to PMI++. As described be-OP Extraction: Recall  low, this result is due in large part to OPINE's ability to OP Polarity: Precision  handle context-sensitive opinion words.  OP Polarity: Recall  We randomly selected 200 (word, feature, sentence)  tuples for each word type (adjective, adverb, etc.) and  Table 7: Extracting Opinion Phrases and Opinion Phrase obtained a test set containing 800 tuples. Two annota-Polarity Corresponding to Known Features and Sentences.  OPINE's precision is higher than that of PMI++ and of Hu++.  tors assigned positive, negative and neutral labels to each All results are reported with respect to PMI++.  tuple (the inter-annotator agreement was 78%). We  retained the tuples on which the annotators agreed as the  gold standard. We ran PMI++ and Hu++ on the test data 4  Related Work  and compared the results against OPINE's results on the  The key components of OPINE described in this paper are  the PMI feature assessment which leads to high-precision In order to quantify the benefits of each of the three  feature extraction and the use of relaxation-labeling in or-steps of our method for finding SO labels, we also  comder to find the semantic orientation of potential opinion pared OPINE with a version which only finds SO la-words. The review-mining work most relevant to our  rebels for words and a version which finds SO labels for  search is that of (Hu and Liu, 2004) and (Kobayashi et  words in the context of given features, but doesn't take al., 2004). Both identify product features from reviews, into account given sentences. We have learned from this  but OPINE significantly improves on both. (Hu and Liu,  comparison that OPINE's precision gain over PMI++ and 2004) doesn't assess candidate features, so its precision Hu++ is mostly due to to its ability to handle context-is lower than OPINE's. (Kobayashi et al., 2004) employs  sensitive words in a large number of cases.  an iterative semi-automatic approach which requires  huAlthough Hu++ does not handle context-sensitive SO  man input at every iteration. Neither model explicitly ad-label assignment, its average precision was reasonable  dresses composite (feature of feature) or implicit features.  (75%) and better than that of PMI++. Finding a word's Other systems (Morinaga et al., 2002; Kushal et al., 2003) SO label is good enough in the case of strongly positive also look at Web product reviews but they do not extract  opinions about particular product features. OPINE's use O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu, of meronymy lexico-syntactic patterns is similar to that T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Un-of many others, from (Berland and Charniak, 1999) to  supervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91 134.  (Almuhareb and Poesio, 2004).  Recognizing the subjective character and polarity of  V. Hatzivassiloglou and K. McKeown. 1997. Predicting the se-words, phrases or sentences has been addressed by many  mantic orientation of adjectives. In ACL/EACL, pages 174  authors, including (Turney, 2003; Riloff et al., 2003;  Wiebe, 2000; Hatzivassiloglou and McKeown, 1997).  M. Hu and B. Liu. 2004. Mining and Summarizing Customer  Most recently, (Takamura et al., 2005) reports on the  Reviews. In KDD, pages 168 177, Seattle, WA.  use of spin models to infer the semantic orientation of  R.A. Hummel and S.W. Zucker. 1983. On the foundations of words. The paper's global optimization approach and use  relaxation labeling processes. In PAMI, pages 267 287.  of multiple sources of constraints on a word's semantic  orientation is similar to ours, but the mechanism differs S. Kim and E. Hovy. 2004. Determining the sentiment of opinions. In COLING.  and they currently omit the use of syntactic information.  Subjective phrases are used by (Turney, 2002; Pang and  Vaithyanathan, 2002; Kushal et al., 2003; Kim and Hovy,  Collecting Evaluative Expressions for Opinion Extraction.  2004) and others in order to classify reviews or sentences In IJCNLP, pages 596 605.  as positive or negative. So far, OPINE's focus has been on D. Kushal, S. Lawrence, and D. Pennock. 2003. Mining the extracting and analyzing opinion phrases corresponding  peanut gallery: Opinion extraction and semantic classifica-to specific features in specific sentences, rather than on tion of product reviews. In WWW.  determining sentence or review polarity.  D. Lin. 1998. Dependency-based evaluation of MINIPAR. In Workshop on Evaluation of Parsing Systems at ICLRE.  Conclusion  OPINE is an unsupervised information extraction system  2002. Mining product reputations on the web. In KDD.  which extracts fine-grained features, and associated opinions, from reviews. OPINE's use of the Web as a  corLee L. Pang, B and S. Vaithyanathan. 2002. Thumbs up? sen-pus helps identify product features with improved  precitiment classification using machine learning techniques. In sion compared with previous work.  relaxation-labeling technique to determine the semantic  A. Popescu, A. Yates, and O. Etzioni. 2004. Class extraction orientation of potential opinion words in the context of from the World Wide Web. In AAAI-04 Workshop on Adap-the extracted product features and specific review  sentive Text Extraction and Mining, pages 68 73.  tences; this technique allows the system to identify cus-A. Rangarajan. 2000. Self annealing and self annihilation: uni-tomer opinions and their polarity with high precision and fying deterministic annealing and relaxation labeling. In Pat-recall.  tern Recognition, 33:635-649.  Acknowledgments  E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning Subjective Nouns Using Extraction Pattern Bootstrapping. In CoNLL, We would like to thank the KnowItAll project and the  anonymous reviewers for their comments. Michael  GaH. Takamura, T. Inui, and M. Okumura. 2005. Extracting Se-mon, Costas Boulis and Adam Carlson have also  promantic Orientations of Words using Spin Model. In ACL, vided valuable feedback. We thank Minquing Hu and  Bing Liu for providing their data sets and for their comments. Finally, we are grateful to Bernadette Minton and P. D. Turney. 2001. Mining the Web for Synonyms: PMI-IR  versus LSA on TOEFL. In Procs. of the Twelfth European Fetch Technologies for their help in collecting additional Conference on Machine Learning (ECML-2001), pages 491  reviews. This research was supported in part by NSF  ONR grant N00014-02-1-0324 as well as gifts from  P. D. Turney. 2002. Thumbs up or thumbs down? semantic  orientation applied to unsupervised classification of reviews.  Google and the Turing Center.  In Procs. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL'02), pages 417 424.  P. Turney. 2003. Inference of Semantic Orientation from Asso-A. Almuhareb and M. Poesio. 2004. Attribute-based and value-ciation. In CoRR cs. CL/0309034.  based clustering: An evaluation. In EMNLP, pages 158 165.  J. Wiebe. 2000. Learning subjective adjectives from corpora.  M. Berland and E. Charniak. 1999. Finding parts in very large corpora. In ACL, pages 57 64. 