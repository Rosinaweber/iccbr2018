 Building a Discourse-Tagged Corpus in the Framework of Rhetorical Structure Theory  Information Sciences Institute  University of S. California  corpora (Moser and Moore, 1995; Marcu et al., Abstract  In this paper, we recount our experience in developing a discourse-annotated  developing a large resource with discourse-level corpus for community-wide use.  annotation for NLP research. Our main goal in Working in the framework of  undertaking this effort was to create a reference Rhetorical Structure Theory, we were  corpus for community-wide use. Two essential able to create a large annotated  considerations from the outset were that the resource with very high consistency,  corpus needed to be consistently annotated, and using a well-defined methodology and  that it would be made publicly available through protocol. This resource is made  the Linguistic Data Consortium for a nominal publicly available through the  fee to cover distribution costs. The paper Linguistic Data Consortium to enable  describes the challenges we faced in building a researchers to develop empirically  corpus of this level of complexity and scope  grounded, discourse-specific  including selection of theoretical approach, applications.  annotation methodology, training, and quality assurance. The resulting corpus contains 385  Introduction  documents of American English selected from the Penn Treebank (Marcus et al., 1993),  The advent of large-scale collections of  annotated in the framework of Rhetorical  annotated data has marked a paradigm shift in Structure Theory. We believe this resource the research community for natural language holds great promise as a rich new source of text-processing. These corpora, now also common in level information to support multiple lines of many languages, have accelerated development research for language understanding  efforts and energized the community.  applications.  Annotation ranges from broad characterization of document-level information, such as topic or 2  relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a Two principle goals underpin the creation of this wide range of linguistic phenomena. However, discourse-tagged corpus: 1) The corpus should rich theoretical approaches to discourse/text be grounded in a particular theoretical approach, analysis (Van Dijk and Kintsch, 1983; Meyer, and 2) it should be sufficiently large enough to 1985; Grosz and Sidner, 1986; Mann and  offer potential for wide-scale use including Thompson, 1988) have yet to be applied on a linguistic analysis, training of statistical models large scale. So far, the annotation of discourse of discourse, and other computational linguistic structure of documents has been applied  applications. These goals necessitated a number primarily to identifying topical segments of constraints to our approach. The theoretical (Hearst, 1997), inter-sentential relations framework had to be practical and repeatable (Nomoto and Matsumoto, 1999; Ts'ou et al., over a large set of documents in a reasonable 2000), and hierarchical analyses of small amount of time, with a significant level of consistency across annotators. Thus, our  approach contributes to the community quite supporting or background unit of  differently from detailed analyses of specific information.  discourse phenomena in depth, such as  Each node is characterized by a rhetorical anaphoric relations (Garside et al., 1997) or relation that holds between two or more style types (Leech et al., 1997); analysis of a non-overlapping, adjacent text spans.  single text from multiple perspectives (Mann Relations can be of intentional, semantic, or and Thompson, 1992); or illustrations of a textual nature.  theoretical model on a single representative text (Britton and Black, 1985; Van Dijk and Kintsch, Below, we describe the protocol that we used 1983).  to build consistent RST annotations.  Our annotation work is grounded in the  Rhetorical Structure Theory (RST) framework 2.1 Segmenting Texts into Units  (Mann and Thompson, 1988). We decided to  The first step in characterizing the discourse use RST for three reasons:  structure of a text in our protocol is to determine  It is a framework that yields rich annotations the elementary discourse units (EDUs), which that uniformly capture intentional, semantic, are the minimal building blocks of a discourse and textual features that are specific to a tree. Mann and Thompson (1988, p. 244) state given text.  that RST provides a general way to describe  the relations among clauses in a text, whether or Previous research on annotating texts with not they are grammatically or lexically  rhetorical structure trees (Marcu et al., signalled. Yet, applying this intuitive notion to 1999) has shown that texts can be annotated the task of producing a large, consistently by multiple judges at relatively high levels annotated corpus is extremely difficult, because of agreement. We aimed to produce  the boundary between discourse and syntax can annotation protocols that would yield even be very blurry. The examples below, which higher agreement figures.  range from two distinct sentences to a single  Previous research has shown that RST trees clause, all convey essentially the same meaning, can play a crucial role in building natural packaged in different ways:  1. [Xerox Corp.'s third-quarter net income Moore and Paris, 1993; Moore, 1995) and  grew 6.2% on 7.3% higher revenue.] [This  text summarization systems (Marcu, 2000); earned mixed reviews from Wall Street  can be used to increase the naturalness of analysts.]  2000); and can be used to build  essay2. [Xerox Corp's third-quarter net income scoring systems that provide students with grew 6.2% on 7.3% higher revenue,] [which discourse-based feedback (Burstein et al., earned mixed reviews from Wall Street  2001). We suspect that RST trees can be  analysts.]  exploited successfully in the context of  3. [Xerox Corp's third-quarter net income other applications as well.  grew 6.2% on 7.3% higher revenue,]  In the RST framework, the discourse  [earning mixed reviews from Wall Street  structure of a text can be represented as a tree analysts.]  defined in terms of four aspects:  4. [The 6.2% growth of Xerox Corp.'s  third The leaves of the tree correspond to text quarter net income on 7.3% higher revenue fragments that represent the minimal units earned mixed reviews from Wall Street  of the discourse, called elementary  analysts.]  In Example 1, there is a consequential  The internal nodes of the tree correspond to relation between the first and second sentences.  Ideally, we would like to capture that kind of rhetorical information regardless of the syntactic  Each node is characterized by its nuclearity form in which it is conveyed. However, as  a nucleus indicates a more essential unit of examples 2-4 illustrate, separating rhetorical information, while a satellite indicates a  from syntactic analysis is not always easy. It is a relaxation of controls on exports to the inevitable that any decision on how to bracket Soviet bloc, ] [is questioning ]wsj_2326  elementary discourse units necessarily involves Finally, a small number of phrasal EDUs are some compromises.  allowed, provided that the phrase begins with a Reseachers in the field have proposed a  strong discourse marker, such as because, in number of competing hypotheses about what spite of, as a result of, according to. We opted constitutes an elementary discourse unit. While for consistency in segmenting, sacrificing some some take the elementary units to be clauses potentially discourse-relevant phrases in the (Grimes, 1975; Givon, 1983; Longacre, 1983), process.  others take them to be prosodic units  (Hirschberg and Litman, 1993), turns of talk 2.2 Building up the Discourse Structure (Sacks, 1974), sentences (Polanyi, 1988), intentionally defined discourse segments (Grosz Once the elementary units of discourse have and Sidner, 1986), or the contextually indexed been determined, adjacent spans are linked representation of information conveyed by a together via rhetorical relations creating a semiotic gesture, asserting a single state of hierarchical structure. Relations may be  affairs or partial state of affairs in a discourse mononuclear or multinuclear. Mononuclear  world, (Polanyi, 1996, p.5). Regardless of their relations hold between two spans and reflect the theoretical stance, all agree that the elementary situation in which one span, the nucleus, is more discourse units are non-overlapping spans of salient to the discourse structure, while the other text.  span, the satellite, represents supporting Our goal was to find a balance between  information. Multinuclear relations hold among granularity of tagging and ability to identify two or more spans of equal weight in the  units consistently on a large scale. In the end, discourse structure. A total of 53 mononuclear we chose the clause as the elementary unit of and 25 multinuclear relations were used for the discourse, using lexical and syntactic clues to tagging of the RST Corpus. The final inventory help determine boundaries:  of rhetorical relations is data driven, and is based on extensive analysis of the corpus.  5. [Although Mr. Freeman is retiring,] [he will Although this inventory is highly detailed, continue to work as a consultant for  annotators strongly preferred keeping a higher American Express on a project basis.]wsj_1317  level of granularity in the selections available to 6. [Bond Corp., a brewing, property, media them during the tagging process. More extensive and resources company, is selling many of analysis of the final tagged corpus will  its assets] [to reduce its debts.]wsj_0630  demonstrate the extent to which individual relations that are similar in semantic content However, clauses that are subjects, objects, were distinguished consistently during the or complements of a main verb are not treated as tagging process.  The 78 relations used in annotating the  7. [Making computers smaller often means corpus can be partitioned into 16 classes that sacrificing memory.]wsj_2387  share some type of rhetorical meaning:  Attribution, Background, Cause, Comparison, 8. [Insurers could see claims totaling nearly Condition, Contrast, Elaboration, Enablement, $1 billion from the San Francisco  Evaluation, Explanation, Joint, Manner-Means, earthquake. ]wsj_0675  Topic-Comment, Summary, Temporal, Topic-Relative clauses, nominal postmodifiers, or Change. For example, the class Explanation clauses that break up other legitimate EDUs, are includes the relations evidence, explanation-treated as embedded discourse units:  argumentative, and reason, while Topic-9. [The results underscore Sears's difficulties]  [ in implementing the everyday low  answer, statement-response, topic-comment, and pricing strategy ]  comment-topic. In addition, three relations are wsj_1105  used to impose structure on the tree: textual-10. [The Bush Administration,] [ trying to blunt organization, span, and same-unit (used to link growing demands from Western Europe for  parts of units separated by embedded units or at each stage of the discourse annotation spans).  process, and because decisions made at one stage influenced the decisions made during 3  Discourse Annotation Task  subsequent stages, we could not apply Wiebe et al.'s method. Our methodology for determining Our methodology for annotating the RST  the best guidelines was much more of a Corpus builds on prior corpus work in the consensus-building process, taking into  Rhetorical Structure Theory framework by  consideration multiple factors at each step. The Marcu et al. (1999). Because the goal of this final tagging manual, over 80 pages in length, effort was to build a high-quality, consistently contains extensive examples from the corpus to annotated reference corpus, the task required illustrate text segmentation, nuclearity, selection that we employ people as annotators whose of relations, and discourse cues. The manual can primary professional experience was in the area be downloaded from the following web site: of language analysis and reporting, provide http://www.isi.edu/~marcu/discourse.  extensive annotator training, and specify a The actual tagging of the corpus progressed rigorous set of annotation guidelines.  in three developmental phases. During the initial phase of about four months, the team created a 3.1 Annotator Profile and Training  preliminary corpus of 100 tagged documents.  This was followed by a one-month reassessment The annotators hired to build the corpus were all phase, during which we measured consistency professional language analysts with prior across the group on a select set of documents, experience in other types of data annotation.  and refined the annotation rules. At this point, They underwent extensive hands-on training, we decided to proceed by pre-segmenting all of which took place roughly in three phases.  the texts on hard copy, to ensure a higher overall During the orientation phase, the annotators quality to the final corpus. Each text was pre-were introduced to the principles of Rhetorical segmented by two annotators; discrepancies Structure Theory and the discourse-tagging tool were resolved by the author of the tagging used for the project (Marcu et al., 1999). The guidelines. In the final phase (about six months) tool enables an annotator to segment a text into all 100 documents were re-tagged with the new units, and then build up a hierarchical structure approach and guidelines. The remainder of the of the discourse. In this stage of the training, the corpus was tagged in this manner.  focus was on segmenting hard copy texts into EDUs, and learning the mechanics of the tool.  In the second phase, annotators began to  explore interpretations of discourse structure, by Annotators developed different strategies for independently tagging a short document, based analyzing a document and building up the  on an initial set of tagging guidelines, and then corresponding discourse tree. There were two meeting as a group to compare results. The basic orientations for document analysis hard initial focus was on resolving segmentation copy or graphical visualization with the tool.  differences, but over time this shifted to Hard copy analysis ranged from jotting of notes addressing issues of relations and nuclearity.  in the margins to marking up the document into These exploratory sessions led to enhancements discourse segments. Those who preferred a in the tagging guidelines. To reinforce new graphical orientation performed their analysis rules, annotators re-tagged the document.  simultaneously with building the discourse During this process, we regularly tracked inter-structure, and were more likely to build the annotator agreement (see Section 4.2). In the discourse tree in chunks, rather than  final phase, the annotation team concentrated on incrementally.  ways to reduce differences by adopting some We observed a variety of annotation styles heuristics for handling higher levels of the for the actual building of a discourse tree. Two discourse structure. Wiebe et al. (1999) present of the more representative styles are illustrated a method for automatically formulating a single below.  best tag when multiple judges disagree on 1. The annotator segments the text one unit at selecting between binary features. Because our a time, then incrementally builds up the annotators had to select among multiple choices  discourse tree by immediately attaching the Corp.]18 [This is in part because of the effect]19  current node to a previous node. When  [of having to average the number of shares building the tree in this fashion, the  outstanding,]20 [she said.]21 [In addition,]22 [Mrs.  annotator must anticipate the upcoming  Lidgerwood said,]23 [Norfolk is likely to draw discourse structure, possibly for a large down its cash initially]24 [to finance the span. Yet, often an appropriate choice of purchases]25 [and thus forfeit some interest relation for an unseen segment may not be income.]26 wsj_1111  obvious from the current (rightmost) unit that needs to be attached. That is why  The discourse sub-tree for this text fragment annotators typically used this approach on is given in Figure 1. Using Style 1 the annotator, short documents, but resorted to other  upon segmenting unit [17], must anticipate the strategies for longer documents.  upcoming example relation, which spans units  [17-26]. However, even if the annotator selects 2. The annotator segments multiple units at a an incorrect relation at that point, the tool allows time, then builds discourse sub-trees for great flexibility in changing the structure of the each sentence. Adjacent sentences are then tree later on.  linked, and larger sub-trees begin to Using Style 2, the annotator segments each emerge. The final tree is produced by sentence, and builds up corresponding sub-trees linking major chunks of the discourse for spans [16], [17-18], [19-21] and [22-26]. The example  elaboration-additional  explanation-argumentative  attribution  attribution  +attribution-embedded  *elaboration-object-attribute-embedded  Figure 1: Discourse sub-tree for multiple sentences structure. This strategy allows the annotator second and third sub-trees are then linked via an to see the emerging discourse structure more explanation-argumentative relation, after which, globally; thus, it was the preferred approach the fourth sub-tree is linked via an elaboration-for longer documents.  additional relation. The resulting span [17-26] is finally attached to node [16] as an example Consider the text fragment below, consisting satellite.  of four sentences, and 11 EDUs:  Quality Assurance  [Still, analysts don't expect the buy-back to significantly affect per-share earnings in the short term.]16 [The impact won't be that great,]17  A number of steps were taken to ensure the quality of the final discourse corpus. These  [said Graeme Lidgerwood of First Boston  Table 1: Inter-annotator agreement periodic results for three taggers Taggers  Units  Nuclearity  Relations  No. of  Relations  (Apr 00)  involved two types of tasks: checking the judgments. The strengths and shortcomings of validity of the trees and tracking inter-annotator the approach are also discussed in detail there.  consistency.  Researchers in content analysis (Krippendorff, 1980) suggest that values of kappa > 0.8 reflect 4.1 Tree Validation Procedures  very high agreement, while values between 0.6  and 0.8 reflect good agreement.  Annotators reviewed each tree for syntactic and Table 1 shows average kappa statistics  semantic validity. Syntactic checking involved reflecting the agreement of three annotators at ensuring that the tree had a single root node and various stages of the tasks on selected  comparing the tree to the document to check for documents. Different sets of documents were missing sentences or fragments from the end of chosen for each stage, with no overlap in the text. Semantic checking involved reviewing documents. The statistics measure annotation nuclearity assignments, as well as choice of reliability at four levels: elementary discourse relation and level of attachment in the tree. All units, hierarchical spans, hierarchical nuclearity trees were checked with a discourse parser and and hierarchical relation assignments.  tree traversal program which often identified At the unit level, the initial (April 00) scores errors undetected by the manual validation and final (January 01) scores represent  process. In the end, all of the trees worked agreement on blind segmentation, and are  successfully with these programs.  shown in boldface. The interim June and  November scores represent agreement on hard 4.2 Measuring Consistency  copy pre-segmented texts. Notice that even with We tracked inter-annotator agreement during pre-segmenting, the agreement on units is not each phase of the project, using a method 100% perfect, because of human errors that developed by Marcu et al. (1999) for computing occur in segmenting with the tool. As Table 1  kappa statistics over hierarchical structures. The shows, all levels demonstrate a marked  kappa coefficient (Siegel and Castellan, 1988) improvement from April to November (when  has been used extensively in previous empirical the final corpus was completed), ranging from studies of discourse (Carletta et al., 1997; about 0.77 to 0.92 at the span level, from 0.70 to Flammia and Zue, 1995; Passonneau and  0.88 at the nuclearity level, and from 0.60 to Litman, 1997). It measures pairwise agreement 0.79 at the relation level. In particular, when among a set of coders who make category  relations are combined into the 16 rhetorically-judgments, correcting for chance expected related classes discussed in Section 2.2, the agreement. The method described in Marcu et November results of the annotation process are al. (1999) maps hierarchical structures into sets extremely good. The Fewer-Relations column of units that are labeled with categorial shows the improvement in scores on assigning  Table 2: Inter-annotator agreement final results fox six taggers Taggers  Units  Nuclearity  Relations  No. of  Relations  relations when they are grouped in this manner, This tension between task complexity and  with November results ranging from 0.78 to guideline under-specification resulted from the 0.82. In order to see how much of the  practical application of a theoretical model on a improvement had to do with pre-segmenting, we broad scale. While other discourse theoretical asked the same three annotators to annotate five approaches posit distinctly different treatments previously unseen documents in January,  for various levels of the discourse (Van Dijk and without reference to a pre-segmented document.  Kintsch, 1983; Meyer, 1985), RST relies on a The results of this experiment are given in the standard methodology to analyze the document last row of Table 1, and they reflect only a small at all levels. The RST relation set is rich and the overall decline in performance from the  concept of nuclearity, somewhat interpretive.  November results. These scores reflect very This gave our annotators more leeway in  strong agreement and represent a significant interpreting the higher levels of the discourse improvement over previously reported results on structure, thus introducing some stylistic annotating multiple texts in the RST framework differences, which may prove an interesting (Marcu et al., 1999).  avenue of future research.  Table 2 reports final results for all pairs of taggers who double-annotated four or more 5  documents, representing 30 out of the 53  documents that were double-tagged. Results are The RST Corpus consists of 385 Wall Street based on pre-segmented documents.  Journal articles from the Penn Treebank,  Our team was able to reach a significant  representing over 176,000 words of text. In level of consistency, even though they faced a order to measure inter-annotator consistency, 53  number of challenges which reflect differences of the documents (13.8%) were double-tagged.  in the agreement scores at the various levels.  The documents range in size from 31 to 2124  While operating under the constraints typical of words, with an average of 458.14 words per any theoretical approach in an applied  document. The final tagged corpus contains environment, the annotators faced a task in 21,789 EDUs with an average of 56.59 EDUs which the complexity increased as support from per document. The average number of words per the guidelines tended to decrease. Thus, while EDU is 8.1.  rules for segmenting were fairly precise, The articles range over a variety of topics, annotators relied on heuristics requiring more including financial reports, general interest human judgment to assign relations and  stories, business-related news, cultural reviews, nuclearity. Another factor is that the cognitive editorials, and letters to the editor. In selecting challenge of the task increases as the tree takes these documents, we partnered with the  shape. It is relatively straightforward for the Linguistic Data Consortium to select Penn annotator to make a decision on assignment of Treebank texts for which the syntactic  nuclearity and relation at the inter-clausal level, bracketing was known to be of high caliber.  but this becomes more complex at the inter-Thus, the RST Corpus provides an additional sentential level, and extremely difficult when level of linguistic annotation to supplement linking large segments.  existing annotated resources.  For details on obtaining the corpus, EDU level is likely to need further refinement annotation software, tagging guidelines, and for higher-level text spans along the lines of related documentation and resources, see: other work which posits a few macro-level http://www.isi.edu/~marcu/discourse.  relations for text segments, such as Ferrari (1998) or Meyer (1985). Moreover, using the 6  RST approach, the resultant tree structure, like a traditional outline, imposed constraints that A growing number of groups have developed or other discourse representations (e.g., graph) are developing discourse-annotated corpora for would not. In combination with the tree  text. These can be characterized both in terms of structure, the concept of nuclearity also guided the kinds of features annotated as well as by the an annotator to capture one of a number of scope of the annotation. Features may include possible stylistic interpretations. We ourselves specific discourse cues or markers, coreference are eager to explore these aspects of the RST, links, identification of rhetorical relations, etc.  and expect new insights to appear through The scope of the annotation refers to the levels analysis of the corpus.  of analysis within the document, and can be We anticipate that the RST Corpus will be characterized as follows:  multifunctional and support a wide range of  language engineering applications. The added sentential: annotation of features at the value of multiple layers of overt linguistic intra-sentential or inter-sentential level, at a phenomena enhancing the Penn Treebank  single level of depth (Sundheim, 1995;  information can be exploited to advance the Tsou et al., 2000; Nomoto and Matsumoto,  study of discourse, to enhance language  technologies such as text summarization,  hierarchical: annotation of features at machine translation or information retrieval, or multiple levels, building upon lower levels to be a testbed for new and creative natural of analysis at the clause or sentence level language processing techniques.  (Moser and Moore, 1995; Marcu, et al.  document-level: broad characterization of document structure such as identification of Bruce Britton and John Black. 1985.  topical segments (Hearst, 1997), linking of Understanding Expository Text. Hillsdale, NJ: large text segments via specific relations Lawrence Erlbaum Associates.  Jill Burstein, Daniel Marcu, Slava Andreyev, defining text objects with a text architecture and Martin Chodorow. 2001. Towards  (Pery-Woodley and Rebeyrolle, 1998).  automatic identification of discourse elements in Developing corpora with these kinds of rich essays. In Proceedings of the 39th Annual annotation is a labor-intensive effort. Building Meeting of the Association for Computational the RST Corpus involved more than a dozen Linguistics, Toulouse, France.  people on a full or part-time basis over a one-Jean Carletta, Amy Isard, Stephen Isard,  year time frame (Jan. Dec. 2000). Annotation Jacqueline Kowtko, Gwyneth Doherty-Sneddon, of a single document could take anywhere from and Anne Anderson. 1997. The reliability of a 30 minutes to several hours, depending on the dialogue structure coding scheme.  length and topic. Re-tagging of a large number Computational Linguistics 23(1): 13-32.  of documents after major enhancements to the Giacomo Ferrari. 1998. Preliminary steps  annotation guidelines was also time consuming.  toward the creation of a discourse and text In addition, limitations of the theoretical resource. In Proceedings of the First approach became more apparent over time.  International Conference on Language  Because the RST theory does not differentiate Resources and Evaluation (LREC 1998), between different levels of the tree structure, a Granada, Spain, 999-1001.  fairly fine-grained set of relations operates Giovanni Flammia and Victor Zue. 1995.  between EDUs and EDU clusters at the macro-Empirical evaluation of human performance and level. The procedural knowledge available at the agreement in parsing discourse constituents in  spoken dialogue. In Proceedings of the 4th Daniel Marcu, Estibaliz Amorrortu, and  European Conference on Speech  Communication and Technology, Madrid, Spain, constructing a corpus of discourse trees. In vol. 3, 1965-1968.  Proceedings of the ACL Workshop on Standards Roger Garside, Steve Fligelstone and Simon and Tools for Discourse Tagging, College Park, Botley. 1997. Discourse Annotation: Anaphoric MD, 48-57.  Relations in Corpora. In Corpus annotation: Daniel Marcu, Lynn Carlson, and Maki  Linguistic information from computer text Watanabe. 2000. The automatic translation of corpora, edited by R. Garside, G. Leech, and T.  discourse structures. Proceedings of the First McEnery. London: Longman, 66-84.  Annual Meeting of the North American Chapter Talmy Givon. 1983. Topic continuity in  of the Association for Computational  discourse. In Topic Continuity in Discourse: a Linguistics, Seattle, WA, 9-17.  Quantitative Cross-Language Study.  Mitchell Marcus, Beatrice Santorini, and  Joseph Evans Grimes. 1975 . The Thread of large annotated corpus of English: the Penn Discourse. The Hague, Paris: Mouton.  Treebank, Computational Linguistics 19(2), Barbara Grosz and Candice Sidner. 1986.  Attentions, intentions, and the structure of Bonnie Meyer. 1985. Prose Analysis:  discourse. Computational Linguistics, 12(3): Purposes, Procedures, and Problems. In  Understanding Expository Text, edited by B.  Marti Hearst. 1997. TextTiling: Segmenting Britton and J. Black. Hillsdale, NJ: Lawrence text into multi-paragraph subtopic passages.  Computational Linguistics 23(1): 33-64.  Johanna Moore. 1995. Participating in Julia Hirschberg and Diane Litman. 1993.  Explanatory Dialogues: Interpreting and Empirical studies on the disambiguation of cue Responding to Questions in Context.  phrases. Computational Linguistics 19(3): 501-Cambridge, MA: MIT Press.  Johanna Moore and Cecile Paris. 1993.  Eduard Hovy. 1993. Automated discourse  Planning text for advisory dialogues: capturing generation using discourse structure relations.  intentional and rhetorical information.  Computational Linguistics 19(4): 651-694.  Klaus Krippendorff. 1980. Content Analysis: Megan Moser and Johanna Moore. 1995.  An Introduction to its Methodology. Beverly Investigating cue selection and placement in Hills, CA: Sage Publications.  tutorial discourse. Proceedings of the 33rd Geoffrey Leech, Tony McEnery, and Martin  Annual Meeting of the Association for Wynne. 1997. Further levels of annotation. In Computational Linguistics, Cambridge, MA, Corpus Annotation: Linguistic Information from 130-135.  Computer Text Corpora, edited by R. Garside, Tadashi Nomoto and Yuji Matsumoto. 1999.  G. Leech, and T. McEnery. London: Longman, Learning discourse relations with active data 85-101.  selection. In Proceedings of the Joint SIGDAT  Robert Longacre. 1983 . The Grammar of Conference on Empirical Methods in Natural Discourse. New York: Plenum Press.  Language Processing and Very Large Corpora, William Mann and Sandra Thompson. 1988.  College Park, MD, 158-167.  Rhetorical structure theory. Toward a functional Rebecca Passonneau and Diane Litman.  theory of text organization. Text, 8(3): 243-281.  1997. Discourse segmentation by human and William Mann and Sandra Thompson, eds.  automatic means. Computational Linguistics 1992. Discourse Description: Diverse Linguistic 23(1): 103-140.  Analyses of a Fund-raising Text.  Marie-Paule Pery-Woodley and Josette  Rebeyrolle. 1998. Domain and genre in  Daniel Marcu. 2000. The Theory and  sublanguage text: definitional microtexts in Practice of Discourse Parsing and  three corpora. In Proceedings of the First Summarization. Cambridge, MA: The MIT  International Conference on Language  Press.  Resources and Evaluation (LREC-1998), Granada, Spain, 987-992.  Livia Polanyi. 1988. A formal model of the structure of discourse. Journal of Pragmatics 12: 601-638.  Livia Polanyi. 1996. The linguistic structure of discourse. Center for the Study of Language and Information. CSLI-96-200.  Harvey Sacks, Emmanuel Schegloff, and  Gail Jefferson. 1974. A simple systematics for the organization of turntaking in conversation.  Sidney Siegal and N.J. Castellan. 1988.  Nonparametric Statistics for the Behavioral Sciences. New York: McGraw-Hill.  Beth Sundheim. 1995. Overview of results of the MUC-6 evaluation. In Proceedings of the Sixth Message Understanding Conference (MUC-6), Columbia, MD, 13-31.  2000. Enhancement of Chinese discourse  marker tagger with C.4.5. In Proceedings of the Second Chinese Language Processing  Strategies of Discourse Comprehension. New York: Academic Press.  The Eighth Text Retrieval Conference (TREC-8). NIST Special Publication 500-246.  Charles Wayne. 2000. Multilingual topic  detection and tracking: successful research enabled by corpora and evaluation. In  Proceedings of the Second International Conference on Language Resources and  Evaluation (LREC-2000), Athens, Greece, 1487-1493.  Janyce Wiebe, Rebecca Bruce, and Thomas  O'Hara. 1999. Development and use of a gold-standard data set for subjectivity classifications.  In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.  College Park, MD, 246-253. 