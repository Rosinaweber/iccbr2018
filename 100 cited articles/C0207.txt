 A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts  Bo Pang and Lillian Lee  Department of Computer Science  Cornell University  Ithaca, NY 14853-7501  classifier to the resulting extract. This can prevent the polarity classifier from considering irrelevant or Sentiment analysis seeks to identify the view-even potentially misleading text: for example, al-point(s) underlying a text span; an example appli-though the sentence The protagonist tries to procation is classifying a movie review as thumbs up  tect her good name contains the word good , it  or thumbs down . To determine this sentiment po-tells us nothing about the author's opinion and in larity, we propose a novel machine-learning method fact could well be embedded in a negative movie  that applies text-categorization techniques to just review. Also, as mentioned above, subjectivity ex-the subjective portions of the document. Extracting tracts can be provided to users as a summary of the these portions can be implemented using efficient sentiment-oriented content of the document.  techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence Our results show that the subjectivity extracts  we create accurately represent the sentiment  information of the originating documents in a much  Introduction  more compact form: depending on choice of  downstream polarity classifier, we can achieve highly sta-The computational treatment of opinion, sentiment, tistically significant improvement (from 82.8% to and subjectivity has recently attracted a great deal 86.4%) or maintain the same level of performance  of attention (see references), in part because of its for the polarity classification task while retaining potential applications. For instance, information-only 60% of the reviews' words.  Also, we  exextraction and question-answering systems could  plore extraction methods based on a minimum cut flag statements and queries regarding opinions  formulation, which provides an efficient, intuitive, rather than facts (Cardie et al., 2003).  Also, it  and effective means for integrating inter-sentence-has proven useful for companies, recommender  syslevel contextual information with traditional bag-of-tems, and editorial sites to create summaries of peo-words features.  ple's experiences and opinions that consist of subjective expressions extracted from reviews (as is 2  commonly done in movie ads) or even just a  review's polarity positive ( thumbs up ) or neg-2.1  Architecture  ative ( thumbs down ).  Document polarity classification poses a  signifification to be just a special (more difficult) case cant challenge to data-driven methods, resisting tra-of text categorization with rather than ditional text-categorization techniques (Pang, Lee, topic-based categories. Hence, standard machine-and Vaithyanathan, 2002). Previous approaches  folearning classification techniques, such as support cused on selecting indicative lexical features (e.g., vector machines (SVMs), can be applied to the en-the word good ), classifying a document  accordtire documents themselves, as was done by Pang,  ing to the number of such features that occur any-Lee, and Vaithyanathan (2002). We refer to such  where within it. In contrast, we propose the follow-classification techniques as default polarity classi-ing process: (1) label the sentences in the document fiers.  as either subjective or objective, discarding the lat-However, as noted above, we may be able to  imter; and then (2) apply a standard machine-learning prove polarity classification by removing objective  sentences (such as plot summaries in a movie re-we propose an alternative that avoids the need for view). We therefore propose, as depicted in Figure such feature engineering: we use an efficient and 1, to first employ a subjectivity detector that deter-intuitive graph-based formulation relying on find-mines whether each sentence is subjective or not: ing minimum cuts.  Our approach is inspired by  discarding the objective ones creates an extract that Blum and Chawla (2001), although they focused on  should better represent a review's subjective content similarity between items (the motivation being to to a default polarity classifier.  combine labeled and unlabeled data), whereas we  are concerned with physical proximity between the subjective  n sentence review  items to be classified; indeed, in computer vision, (m<=n)  s1  modeling proximity information via graph cuts has s2  led to very effective classification (Boykov, Veksler, s1  s3  s4  s4  default polarity classifier  Cut-based classification  Figure 2 shows a worked example of the concepts  subjectivity  in this section.  subjectivity extraction  Suppose we have n items x1, . . . , xn to divide  into two classes C1 and C2, and we have access to Figure 1: Polarity classification via subjectivity detec-two types of information:  tion.  Individual scores indj(xi): non-negative estimates of each xi's preference for being in Cj based To our knowledge, previous work has not in-on just the features of xi alone; and  tegrated sentence-level subjectivity detection with  Association scores assoc(xi, xk): non-negative document-level sentiment polarity. Yu and Hatzi-estimates of how important it is that xi and xk be in vassiloglou (2003) provide methods for sentence-the same class.1  level analysis and for determining whether a  docWe would like to maximize each item's net  hapument is subjective or not, but do not combine these piness : its individual score for the class it is as-two types of algorithms or consider document polar-signed to, minus its individual score for the other ity classification. The motivation behind the single-class. But, we also want to penalize putting tightly-sentence selection method of Beineke et al. (2004) associated items into different classes. Thus, after is to reveal a document's sentiment polarity, but they some algebra, we arrive at the following optimiza-do not evaluate the polarity-classification accuracy tion problem: assign the xis to C1 and C2 so as to that results.  minimize the partition cost  Context and Subjectivity Detection  ind2(x)+  ind1(x)+  As with document-level polarity classification, we x C1  could perform subjectivity detection on individual xk C2  sentences by applying a standard classification algorithm on each sentence in isolation. However, mod-The problem appears intractable, since there are  eling proximity relationships between sentences  2n possible binary partitions of the xi's.  would enable us to leverage coherence: text spans ever, suppose we represent the situation in the fol-occurring near each other (within discourse bound-lowing manner. Build an undirected graph G with  aries) may share the same subjectivity status, other vertices {v1, . . . , vn, s, t}; the last two are, respec-things being equal (Wiebe, 1994).  tively, the source and sink. Add n edges (s, vi), each We would therefore like to supply our algorithms  with weight ind1(xi), and n edges (vi, t), each with with pair-wise interaction information, e.g., to spec-weight ind  ify that two particular sentences should ideally re-each with weight assoc(xi, xk). Then, cuts in G  ceive the same subjectivity label but not state which are defined as follows:  label this should be. Incorporating such  informaDefinition 1 A cut (S, T ) of G is a partition of its tion is somewhat unnatural for classifiers whose in-nodes into sets S = {s} S0 and T = {t} T 0 , put consists simply of individual feature vectors, where s 6 S0, t 6 T 0 . Its cost cost(S, T ) is the sum such as Naive Bayes or SVMs, precisely because  of the weights of all edges crossing from S to T . A such classifiers label each test item in isolation.  minimum cut of G is one of minimum cost.  One could define synthetic features or feature vectors to attempt to overcome this obstacle. However, 1Asymmetry is allowed, but we used symmetric scores.  ind (Y) [.8]  ind (Y) [.2]  Association  Cost  assoc(Y,M) [1.0]  assoc(Y,N) [.1]  s ind (M)[.5]  ind (M) [.5]  assoc(M,N) [.2]  Figure 2: Graph for classifying three items. Brackets enclose example values; here, the individual scores happen to be probabilities. Based on individual scores alone, we would put Y ( yes ) in C1, N ( no ) in C2, and be undecided about M ( maybe ). But the association scores favor cuts that put Y and M in the same class, as shown in the table.  Thus, the minimum cut, indicated by the dashed line, places M together with Y in C1.  Observe that every cut corresponds to a partition of 3  Evaluation Framework  the items and has cost equal to the partition cost.  Our experiments involve classifying movie reviews Thus, our optimization problem reduces to finding as either positive or negative, an appealing task for minimum cuts.  several reasons. First, as mentioned in the introduction, providing polarity information about  rePractical advantages As we have noted, formulat-views is a useful service: witness the popularity of ing our subjectivity-detection problem in terms of www.rottentomatoes.com. Second, movie reviews  graphs allows us to model item-specific and  pairare apparently harder to classify than reviews of wise information independently. Note that this is other products (Turney, 2002; Dave, Lawrence, and a very flexible paradigm. For instance, it is per-Pennock, 2003). Third, the correct label can be ex-fectly legitimate to use knowledge-rich algorithms tracted automatically from rating information (e.g., employing deep linguistic knowledge about sen-number of stars). Our data4 contains 1000 positive timent indicators to derive the individual scores.  and 1000 negative reviews all written before 2002, And we could also simultaneously use knowledge-with a cap of 20 reviews per author (312 authors  lean methods to assign the association scores. In-total) per category. We refer to this corpus as the terestingly, Yu and Hatzivassiloglou (2003) com-polarity dataset.  pared an individual-preference classifier against a relationship-based method, but didn't combine the Default polarity classifiers We tested support vec-two; the ability to coordinate such algorithms is tor machines (SVMs) and Naive Bayes (NB). Fol-precisely one of the strengths of our approach.  lowing Pang et al. (2002), we use unigram-presence But a crucial advantage specific to the utilization features: the ith coordinate of a feature vector is of a minimum-cut-based approach is that we can use 1 if the corresponding unigram occurs in the input maximum-flow algorithms with polynomial asymp-text, 0 otherwise. (For SVMs, the feature vectors totic running times and near-linear running times are length-normalized).  in practice to exactly compute the minimum-level polarity classifier is trained and tested on the cost cut(s), despite the apparent intractability of extracts formed by applying one of the sentence-the optimization problem (Cormen, Leiserson, and  level subjectivity detectors to reviews in the polarity Rivest, 1990; Ahuja, Magnanti, and Orlin, 1993).2  In contrast, other graph-partitioning problems that Subjectivity dataset  To train our detectors, we  have been previously used to formulate NLP  clasneed a collection of labeled sentences. Riloff and sification problems3 are NP-complete (Hatzivassi-Wiebe (2003) state that It is [very hard] to  obloglou and McKeown, 1997; Agrawal et al., 2003;  tain collections of individual sentences that can be Joachims, 2003).  easily identified as subjective or objective ; the polarity-dataset sentences, for example, have not 2Code available at http://www.avglab.com/andrew/soft.html.  3Graph-based approaches to general clustering problems 4Available   www.cs.cornell.edu/people/pabo/movieare too numerous to mention here.  review-data/ (review corpus version 2.0).  been so annotated.5  Fortunately, we were able  non-increasing function f (d) specifies how the in-to mine the Web to create a large,  automaticallyfluence of proximal sentences decays with respect to labeled sentence corpus6.  To gather subjective  distance d; in our experiments, we tried f (d) = 1, sentences (or phrases), we collected 5000 movie-e1 d, and 1/d2. The constant c controls the relative review snippets (e.g., bold, imaginative, and im-influence of the association scores: a larger c makes possible to resist ) from www.rottentomatoes.com.  the minimum-cut algorithm more loath to put  proxTo obtain (mostly) objective data, we took 5000 sen-imal sentences in different classes. With these in tences from plot summaries available from the In-hand8, we set (for j > i)  ternet Movie Database (www.imdb.com). We only  selected sentences or snippets at least ten words def n f (j i) c  assoc(s  long and drawn from reviews or plot summaries of  otherwise.  movies released post-2001, which prevents overlap 4  with the polarity dataset.  Below, we report average accuracies computed by  Subjectivity detectors As noted above, we can use ten-fold cross-validation over the polarity dataset.  our default polarity classifiers as basic sentence-Section 4.1 examines our basic subjectivity extrac-level subjectivity detectors (after retraining on the tion algorithms, which are based on individual-subjectivity dataset) to produce extracts of the orig-sentence predictions alone. Section 4.2 evaluates inal reviews. We also create a family of cut-based the more sophisticated form of subjectivity extrac-subjectivity detectors; these take as input the set of tion that incorporates context information via the sentences appearing in a single document and de-minimum-cut paradigm.  termine the subjectivity status of all the sentences As we will see, the use of subjectivity extracts  simultaneously using per-item and pairwise  relacan in the best case provide satisfying  improvetionship information. Specifically, for a given doc-ment in polarity classification, and otherwise can ument, we use the construction in Section 2.2 to  at least yield polarity-classification accuracies indis-build a graph wherein the source s and sink t cor-tinguishable from employing the full review. At the respond to the class of subjective and objective sen-same time, the extracts we create are both smaller tences, respectively, and each internal node vi cor-on average than the original document and more  responds to the document's ith sentence si. We can effective as input to a default polarity classifier set the individual scores ind1(si) to P rNB(s  than the same-length counterparts produced by stan-ind2(si) to 1 P rNB(s  i), as shown in Figure 3,  dard summarization tactics (e.g., or last-N sen-where P rNB(s) denotes Naive Bayes' estimate of  tences). We therefore conclude that subjectivity ex-the probability that sentence s is subjective; or, we traction produces effective summaries of document can use the weights produced by the SVM classi-sentiment.  fier instead.7 If we set all the association scores to zero, then the minimum-cut classification of the 4.1  Basic subjectivity extraction  sentences is the same as that of the basic subjectiv-As noted in Section 3, both Naive Bayes and SVMs  ity detector. Alternatively, we incorporate the de-can be trained on our subjectivity dataset and then gree of proximity between pairs of sentences, con-used as a basic subjectivity detector. The former has trolled by three parameters. The threshold T spec-somewhat better average ten-fold cross-validation ifies the maximum distance two sentences can be  performance on the subjectivity dataset (92% vs.  separated by and still be considered proximal. The 90%), and so for space reasons, our initial discus-sions will focus on the results attained via NB sub-5We therefore could not directly evaluate sentence-jectivity detection.  classification accuracy on the polarity dataset.  Employing Naive Bayes as a subjectivity   detecwww.cs.cornell.edu/people/pabo/movietor ( Extract  NB) in conjunction with a Naive Bayes  7We converted SVM output d  document-level polarity classifier achieves 86.4%  i, which is a signed distance  (negative=objective) from the separating hyperplane, to non-accuracy.9 This is a clear improvement over the  negative numbers by  82.8% that results when no extraction is applied  Parameter training is driven by optimizing the performance 1(si) =  of the downstream polarity classifier rather than the detector 0  itself because the subjectivity dataset's sentences come from and ind2(si) = 1 ind1(si). Note that scaling is employed different reviews, and so are never proximal.  only for consistency; the algorithm itself does not require prob-9This result and others are depicted in Figure 5; for now, abilities for individual scores.  consider only the y-axis in those plots.  n sentence review  s1  Pr (s1)  1 Pr (s1)  s2  s  s  s3  s1  s4  s4  Pr individual subjectivity probability link edge crossing the cut  proximity link  Figure 3: Graph-cut-based creation of subjective extracts.  ( Full review); indeed, the difference is highly sta-line to compare against, we take the canonical sum-tistically significant (p < 0.01, paired t-test). With marization standard of extracting the first N sen-SVMs as the polarity classifier instead, the Full re-tences in general settings, authors often  beview performance rises to 87.15%, but comparison gin documents with an overview.  We also  convia the paired t-test reveals that this is statistically sider the last N sentences: in many documents, indistinguishable from the 86.4% that is achieved by concluding material may be a good summary, and  running the SVM polarity classifier on ExtractNB  www.rottentomatoes.com tends to select snippets  input. (More improvements to extraction  perforfrom the end of movie reviews (Beineke et al.,  mance are reported later in this section.)  2004). Finally, as a sanity check, we include results These findings indicate10 that the extracts pre-from the N least subjective sentences according to serve (and, in the NB polarity-classifier case, appar-Naive Bayes.  ently clarify) the sentiment information in the orig-Figure 4 shows the polarity classifier results as inating documents, and thus are good summaries  N ranges between 1 and 40. Our first observation  from the polarity-classification point of view. Fur-is that the NB detector provides very good bang  ther support comes from a flipping experiment:  for the buck : with subjectivity extracts containing if we give as input to the default polarity classifier as few as 15 sentences, accuracy is quite close to an extract consisting of the sentences labeled ob-what one gets if the entire review is used. In fact, jective, accuracy drops dramatically to 71% for NB  for the NB polarity classifier, just using the 5 most and 67% for SVMs. This confirms our hypothesis  subjective sentences is almost as informative as the that sentences discarded by the subjectivity extrac-Full review while containing on average only about tion process are indeed much less indicative of sen-22% of the source reviews' words.  timent polarity.  Also, it so happens that at N = 30, performance  Moreover, the subjectivity extracts are much  is actually slightly better than (but statistically in-more compact than the original documents (an  imdistinguishable from) Full review even when the portant feature for a summary to have): they contain SVM default polarity classifier is used (87.2% vs.  on average only about 60% of the source reviews'  87.15%).12 This suggests potentially effective ex-words. (This word preservation rate is plotted along traction alternatives other than using a fixed proba-the x-axis in the graphs in Figure 5.) This prompts bility threshold (which resulted in the lower accu-us to study how much reduction of the original doc-racy of 86.4% reported above).  uments subjectivity detectors can perform and still Furthermore, we see in Figure 4 that the N most-accurately represent the texts' sentiment informa-subjective-sentences method generally outperforms tion.  the other baseline summarization methods (which  We can create subjectivity extracts of varying  perhaps suggests that sentiment summarization can-lengths by taking just the N most subjective sen-not be treated the same as topic-based  summarizatences11 from the originating review. As one base-ities exceed 50% and so would actually be classified as subjective by Naive Bayes. For reviews with fewer than N sentences, 10Recall that direct evidence is not available because the pothe entire review will be returned.  larity dataset's sentences lack subjectivity labels.  12Note that roughly half of the documents in the polarity 11These are the N sentences assigned the highest probability dataset contain more than 30 sentences (average=32.3, standard by the basic NB detector, regardless of whether their probabil-deviation 15).  Accuracy for N-sentence abstracts (def = NB) Accuracy for N-sentence abstracts (def = SVM)  Average accuracy  Average accuracy  first N sentences  first N sentences  least subjective N sentences  least subjective N sentences  Full review  Full review  Figure 4: Accuracies using N-sentence extracts for NB (left) and SVM (right) default polarity classifiers.  Accuracy for subjective abstracts (def = NB)  Accuracy for subjective abstracts (def = SVM)  Full Review  difference in accuracy  difference in accuracy  not statistically significant  not statistically significant  SVM+Prox  Average accuracy  Average accuracy  indicates statistically significant  indicates statistically significant  improvement in accuracy  improvement in accuracy  Full Review  % of words extracted  % of words extracted  Figure 5: Word preservation rate vs. accuracy, NB (left) and SVMs (right) as default polarity classifiers.  Also indicated are results for some statistical significance tests.  tion, although this conjecture would need to be veri-proximity information.  ExtractNB+Prox and  fied on other domains and data). It's also interesting ExtractSVM+Prox are the graph-based subjectivity to observe how much better the last N sentences are detectors using Naive Bayes and SVMs, respec-than the first N sentences; this may reflect a (hardly tively, for the individual scores; we depict the  surprising) tendency for movie-review authors to  best performance achieved by a single setting of  place plot descriptions at the beginning rather than the three proximity-related edge-weight parameters the end of the text and conclude with overtly opin-over all ten data folds13 (parameter selection was ionated statements.  not a focus of the current work). The two  comparisons we are most interested in are ExtractNB+Prox 4.2  Incorporating context information  versus ExtractNB and ExtractSVM+Prox versus Extract  The previous section demonstrated the value of  subjectivity detection. We now examine whether  We see that the context-aware graph-based  subcontext information, particularly regarding sentence jectivity detectors tend to create extracts that are proximity, can further improve subjectivity extrac-more informative (statistically significant so (paired tion.  As discussed in Section 2.2 and 3,  cont-test) for SVM subjectivity detectors only),  altextual constraints are easily incorporated via the though these extracts are longer than their context-minimum-cut formalism but are not natural inputs  blind counterparts. We note that the performance  for standard Naive Bayes and SVMs.  13Parameters are chosen from T {1, 2, 3}, f(d)  Figure 5 shows the effect of adding in  {1, e1 d, 1/d2}, and c [0, 1] at intervals of 0.1.  enhancements cannot be attributed entirely to the that they are not only shorter, but also cleaner rep-mere inclusion of more sentences regardless of  resentations of the intended polarity.  whether they are subjective or not one  counterWe have also shown that employing the  argument is that Full review yielded substantially minimum-cut framework results in the develop-worse results for the NB default polarity classifier  ment of efficient algorithms for sentiment  analyand at any rate, the graph-derived extracts are still sis. Utilizing contextual information via this frame-substantially more concise than the full texts.  work can lead to statistically significant improve-Now, while incorporating a bias for assigning  ment in polarity-classification accuracy. Directions nearby sentences to the same category into NB and for future research include developing parameter-SVM subjectivity detectors seems to require some  selection techniques, incorporating other sources of non-obvious feature engineering, we also wish  contextual cues besides sentence proximity, and in-to investigate whether our graph-based paradigm  vestigating other means for modeling such informamakes better use of contextual constraints that can tion.  be (more or less) easily encoded into the input of standard classifiers. For illustrative purposes, we Acknowledgments  consider paragraph-boundary information, looking  We thank Eric Breck, Claire Cardie, Rich Caruana, only at SVM subjectivity detection for simplicity's Yejin Choi, Shimon Edelman, Thorsten Joachims,  It seems intuitively plausible that paragraph  boundaries (an approximation to discourse  boundand the anonymous reviewers for helpful comments.  aries) loosen coherence constraints between nearby This paper is based upon work supported in part  sentences. To capture this notion for minimum-cut-by the National Science Foundation under grants  based classification, we can simply reduce the as-ITR/IM IIS-0081334 and IIS-0329064, a Cornell  sociation scores for all pairs of sentences that oc-Graduate Fellowship in Cognitive Studies, and by  cur in different paragraphs by multiplying them by an Alfred P. Sloan Research Fellowship. Any opin-a cross-paragraph-boundary weight w [0, 1]. For ions, findings, and conclusions or recommendations standard classifiers, we can employ the trick of hav-expressed above are those of the authors and do not ing the detector treat paragraphs, rather than sen-necessarily reflect the views of the National Science tences, as the basic unit to be labeled. This en-Foundation or Sloan Foundation.  ables the standard classifier to utilize coherence between sentences in the same paragraph; on the other References  hand, it also (probably unavoidably) poses a hard Agrawal, Rakesh, Sridhar Rajagopalan, Ramakrish-constraint that all of a paragraph's sentences get the nan Srikant, and Yirong Xu. 2003. Mining news-same label, which increases noise sensitivity.14 Our groups using networks arising from social behav-experiments reveal the graph-cut formulation to be ior. In WWW, pages 529 535.  the better approach: for both default polarity clas-Ahuja,  and  sifiers (NB and SVM), some choice of parameters  James B. Orlin. 1993. Network Flows: Theory,  (including w) for ExtractSVM+Prox yields statisti-Algorithms, and Applications. Prentice Hall.  cally significant improvement over its  paragraphExploring sentiment summarization. In AAAI  Spring Symposium on Exploring Attitude and Af-5  fect in Text: Theories and Applications (AAAI We examined the relation between subjectivity de-tech report SS-04-07).  tection and polarity classification, showing that sub-Blum, Avrim and Shuchi Chawla. 2001. Learning  jectivity detection can compress reviews into much from labeled and unlabeled data using graph min-shorter extracts that still retain polarity information cuts. In Intl. Conf. on Machine Learning (ICML), at a level comparable to that of the full review. In pages 19 26.  fact, for the Naive Bayes polarity classifier, the sub-Boykov, Yuri, Olga Veksler, and Ramin Zabih.  jectivity extracts are shown to be more effective in-1999. Fast approximate energy minimization via  put than the originating document, which suggests graph cuts. In Intl. Conf. on Computer Vision (ICCV), pages 377 384. Journal version in IEEE  14For example, in the data we used, boundaries may have Trans. Pattern Analysis and Machine Intelligence been missed due to malformed html.  Cardie, Claire, Janyce Wiebe, Theresa Wilson, and extraction patterns for subjective expressions. In Diane Litman. 2003. Combining low-level and  summary representations of opinions for  multiRiloff, Ellen, Janyce Wiebe, and Theresa Wilson.  perspective question answering. In AAAI Spring 2003. Learning subjective nouns using extraction  Symposium on New Directions in Question  Anpattern bootstrapping. In Conf. on Natural Lanswering, pages 20 27.  guage Learning (CoNLL), pages 25 32.  Cormen, Thomas H., Charles E. Leiserson, and  Subasic, Pero and Alison Huettner. 2001.  AfRonald L. Rivest. 1990. Introduction to  Algofect analysis of text using fuzzy semantic typing.  rithms. MIT Press.  Das, Sanjiv and Mike Chen. 2001. Yahoo! for  Tong, Richard M. 2001. An operational system for  Amazon: Extracting market sentiment from stock  detecting and tracking opinions in on-line discus-message boards. In Asia Pacific Finance Associ-sion. SIGIR Wksp. on Operational Text  Classifiation Annual Conf. (APFA).  cation.  Dave, Kushal, Steve Lawrence, and David M.  PenTurney, Peter. 2002. Thumbs up or thumbs down?  nock. 2003. Mining the peanut gallery: Opinion  Semantic orientation applied to unsupervised  extraction and semantic classification of product classification of reviews. In ACL, pages 417 424.  reviews. In WWW, pages 519 528.  Wiebe, Janyce M. 1994. Tracking point of view in  narrative. Computational Linguistics, 20(2):233  ion classification through information extraction.  In Intl. Conf. on Data Mining Methods and  Databases for Engineering, Finance and Other  and Wayne Niblack. 2003. Sentiment analyzer:  Extracting sentiments about a given topic using  Warner. 2003. A system for affective rating of  Intl. Conf. on Data Mining (ICDM).  texts. In KDD Wksp. on Operational Text Classi-Yu, Hong and Vasileios Hatzivassiloglou. 2003.  fication Systems (OTC-3).  Towards answering opinion questions:  Separating facts from opinions and identifying the polar-Keown. 1997. Predicting the semantic  orientaity of opinion sentences. In EMNLP.  tion of adjectives. In 35th ACL/8th EACL, pages 174 181.  Joachims, Thorsten. 2003. Transductive learning  via spectral graph partitioning. In Intl. Conf. on Machine Learning (ICML).  Liu, Hugo, Henry Lieberman, and Ted Selker.  2003. A model of textual affect sensing using  real-world knowledge. In Intelligent User Inter-faces (IUI), pages 125 132.  and Alexander Gelbukh. 1999. Text mining as a  social thermometer. In IJCAI Wksp. on Text Mining, pages 103 107.  Morinaga, Satoshi, Kenji Yamanishi, Kenji Tateishi, and Toshikazu Fukushima. 2002. Mining product reputations on the web. In KDD, pages 341  349. Industry track.  and  Thumbs up?  ment classification using machine learning  tors. 2004. AAAI Spring Symposium on  Exploring Attitude and Affect in Text: Theories and Applications. AAAI technical report SS-04-07.  Riloff, Ellen and Janyce Wiebe. 2003. Learning 