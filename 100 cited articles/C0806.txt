 Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables  National Institute of Information and Communications Technology  Tohoku University  Kyoto University  2002), p.168). For example, when a document  contains some domain-specific words, the document  In this paper, we present a dependency  treebased method for sentiment classification of  will probably belong to the domain. However, in  Japanese and English subjective sentences  using conditional random fields with hidden  reversed. For example, let us consider the sentence  Subjective sentences often  con The medicine kills cancer cells. While the phrase  tain words which reverse the sentiment  pocancer cells has negative polarity, the word kills re-larities of other words.  Therefore,  interacverses the polarity, and the whole sentence has  postions between words need to be considered  itive polarity. Thus, in sentiment classification, a  in sentiment classification, which is difficult  sentence which contains positive (or negative)  polarto be handled with simple bag-of-words  approaches, and the syntactic dependency  strucity words does not necessarily have the same  polartures of subjective sentences are exploited in  ity as a whole, and we need to consider interactions  our method. In the method, the sentiment  polarity of each dependency subtree in a  sendently.  tence, which is not observable in training data,  Recently, several methods have been proposed to  is represented by a hidden variable. The  pocope with the problem (Zaenen, 2004; Ikeda et al.,  larity of the whole sentence is calculated in  consideration of interactions between the  hid2008). However, these methods are based on flat  bag-of-features representation, and do not consider  tion is used for inference. Experimental  resyntactic structures which seem essential to infer  sults of sentiment classification for Japanese  the polarity of a whole sentence. Other methods  and English subjective sentences showed that  have been proposed which utilize composition of  the method performs better than other  methsentences (Moilanen and Pulman, 2007; Choi and  ods based on bag-of-features.  Cardie, 2008; Jia et al., 2009), but these methods  use rules to handle polarity reversal, and whether  poIntroduction  larity reversal occurs or not cannot be learned from  Sentiment classification is a useful technique for  anlabeled data. Statistical machine learning can learn  alyzing subjective information in a large number of  useful information from training data and generally  texts, and many studies have been conducted (Pang  robust for noisy data, and using it instead of rigid  and Lee, 2008). A typical approach for sentiment  rules seems useful. Wilson et al. (2005) proposed  classification is to use supervised machine learning  a method for sentiment classification which utilizes  algorithms with bag-of-words as features (Pang et  head-modifier relation and machine learning.  Howal., 2002), which is widely used in topic-based text  ever, the method is based on bag-of-features and  poIn the approach, a subjective  senlarity reversal occurred by content words is not  hantence is represented as a set of words in the  sendled. One issue of the approach to use sentence  tence, ignoring word order and head-modifier  relacomposition and machine learning is that only the  tion between words. However, sentiment  classifiwhole sentence is labeled with its polarity in  gencation is different from traditional topic-based text  eral corpora for sentiment classification, and each  classification. Topic-based text classification is  gencomponent of the sentence is not labeled, though  erally a linearly separable problem ((Chakrabarti,  such information is necessary for supervised  maHuman Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 786 794, Los Angeles, California, June 2010. c  2010 Association for Computational Linguistics  Whole Dependency Tree  s 0  s 1  s 2  s 3  s 4  It  and heart disease.  It  and heart disease.  Figure 2: Probabilistic Model based on Dependency Tree  Polarities of Dependency Subtrees  s 0  s 1  s 2  s 3  s 4  and heart disease.  and heart disease.  Figure 1: Polarities of Dependency Subtrees  Figure 3: Factor Graph  chine learning to infer the sentence polarity from its  ties. However, the polarities are reversed by  modifying the word prevents, and the dependency subtree In this paper, we propose a dependency tree-based  prevents cancer and heart disease has positive  pomethod for Japanese and English sentiment  classifilarity. As a result, the whole dependency tree It  cation using conditional random fields (CRFs) with  prevents cancer and heart disease. has positive  pohidden variables. In the method, the sentiment  polarity (Figure 1). In such a way, we can consider  larity of each dependency subtree, which is not  obthe sentiment polarity for each dependency subtree  servable in training data, is represented by a hidden  of a subjective sentence. Note that we use phrases as  variable. The polarity of the whole sentence is  cala basic unit instead of words in this study, because  culated in consideration of interactions between the  phrases are useful as a meaningful unit for sentiment  classification1. In this paper, a dependency subtree  The rest of this paper is organized as follows:  Secmeans the subtree of a dependency tree whose root  tion 2 describes a dependency tree-based method  node is one of the phrases in the sentence.  for sentiment classification using CRFs with  hidWe use a probabilistic model as shown in  Figden variables, and Section 3 shows experimental  reure 2. We consider that each phrase in the subjective  sults on Japanese and English corpora. Section 4  sentence has a random variable (indicated by a  cirdiscusses related work, and Section 5 gives  conclucle in Figure 2). The random variable represents the  polarity of the dependency subtree whose root node  is the corresponding phrase. Two random variables  Dependency Tree-based Sentiment  are dependent (indicated by an edge in Figure 2) if  Classification using CRFs with Hidden  their corresponding phrases have head-modifier  relation in the dependency tree. The node denoted as  In this study, we handle a task to classify the  polar<root> in Figure 2 indicates a virtual phrase which ities (positive or negative) of given subjective sen-represents the root node of the sentence, and we  retences. In the rest of this section, we describe a  probgard that the random variable of the root node is the  abilistic model for sentiment classification based on  polarity of the whole sentence. In usual annotated  dependency trees, methods for inference and  paramcorpora for sentiment classification, only each  seneter estimation, and features we use.  tence is labeled with its polarity, and each phrase  (dependency subtree) is not labeled, so all the  ranA Probabilistic Model based on  dom variables except the one for the root node are  1From an empirical view, in our preliminary experiments  Let us consider the subjective sentence It prevents  with the proposed method, phrase-based processing performed cancer and heart disease. In the sentence, cancer  better than word-based processing in accuracy and in computa-and heart disease have themselves negative polari-tional efficiency.  hidden variables that cannot be observed in labeled where Kn and Ke respectively represent the sets of data (indicated by gray circles in Figure 2). With  indices of node features and edge features.  properties such that phrases which contain positive  Classification of Sentiment Polarity  (or negative) words tend to have positive (negative)  Let us consider how to infer the sentiment polarity  polarities, and two phrases with head-modifier  relap {+1 , 1 }, given a subjective sentence w and tion tend to have opposite polarities if the head con-its dependency tree h. The polarity of the root node tains a word which reverses sentiment polarity.  ( s 0) is regarded as the polarity of the whole sentence, Next, we define the probabilistic model as shown  and p can be calculated as follows:  in Figure 2 in detail. Let n denote the number of phrases in a subjective sentence, wi the i-th phrase, p=argmax P ( p0|w , h) , (5)  and hi the head index of the i-th phrase. Let si de-p0  note the random variable which represents the  poP (s |w , h) .  larity of the dependency subtree whose root is the  s: s 0= p  i-th phrase ( si {+1 , 1 }), and let p denote the polarity of the whole sentence ( p {+1 , 1 }). We That is, the polarity of the subjective sentence is ob-regard the 0-th phrase as a virtual phrase which  reptained as the marginal probability of the root node  resents the root of the sentence. w , h , s respectively polarity, by summing the probabilities for all the  denote the sequence of w  possible configurations of hidden variables.  However, enumerating all the possible configurations of  w = w 1 wn, h = h 1 hn, s = s 0 sn, hidden variables is computationally hard, and we use  p = s 0 .  sum-product belief propagation (MacKay, 2003) for  the calculation.  For the example sentence in Figure 1, w 1 = It, Belief propagation enables us to efficiently calcu-w 2 = prevents, w 3 = cancer, w 4 = and heart dis-late marginal probabilities. In this study, the  graphease. , h 1 = 2, h 2 = 0, h 3 = 2, h 4 = 2. We define ical model to be solved has a tree structure (identi-the joint probability distribution of the sentiment  pocal to the syntactic dependency tree) which has no  larities of dependency subtrees s, given a subjective loops, and an exact solution can be obtained us-sentence w and its dependency tree h, using log-ing belief propagation. Dependencies among  random variables in Figure 2 are represented by a factor  graph in Figure 3. The factor graph consists of  varii indicated by circles, and factor  (feature) nodes gi indicated by squares. In the  example in Figure 3, gi(1 i 4) correspond to the Z (w , h)=  node features in Equation (4), and g  s  correspond to the edge features. In belief  propagation, marginal distribution is calculated by  passing messages (beliefs) among the variables and  factors connected by edges in the factor graph (Refer  where = { 1 , , K} is the set of parameters to (MacKay, 2003) for detailed description of belief  of the model. fk( i, w , h , s) is the feature function propagation).  of the i-th phrase, and is classified to node feature which considers only the corresponding node, or  Parameter Estimation  edge feature which considers both the correspond-Let us consider how to estimate model parameters ,  ing node and its head, as follows:  In this study, we use the maximum a posteriori  es, s )  timation with Gaussian priors for parameter  estimation. We define the following objective function L , 788  and calculate the parameters  which maximize the  s  s  Edge Features  are as follows:  s  s  P (s |w l, h l, pl) Fk(w l, h l, s)  s  s  s  s  Table 1: Features Used in This Study  fine-grained POS tag of the k-th word in the i-th The model parameters can be calculated with the  phrase.  L-BFGS quasi-Newton method (Liu and Nocedal,  We used the morphological analysis system  JU1989) using the objective function and its partial  MAN and the dependency parser KNP2 for  proWhile the partial derivatives contain  cessing Japanese data, and the POS tagger  MXsummation over all the possible configurations of  POST (Ratnaparkhi, 1996) and the dependency  hidden variables, it can be calculated efficiently  usparser MaltParser3 for English data. KNP outputs  ing belief propagation as explained in Section 2.2.  phrase-based dependency trees, but MaltParser  outThis parameter estimation method is same to one  puts word-based dependency trees, and we  conused for Latent-Dynamic Conditional Random Field  verted the word-based ones to phrase-based ones  us(Morency et al., 2007). Note that the objective  funcing simple heuristic rules explained in Appendix A.  The prior polarity of a phrase q  is not convex, and there is no guarantee for  global optimality. The estimated model parameters  the innate sentiment polarity of a word contained in  depend on the initial values of the parameters, and  the phrase, which can be obtained from sentiment  the setting of the initial values of model parameters  polarity dictionaries. We used sentiment polarity  will be explained in Section 2.4.  dictionaries made by Kobayashi et al. (2007) and  HiFeatures  (The resulting dictionary contains 6,974 positive  exTable 1 shows the features used in this study.  Feapressions and 8,428 negative expressions), and a  dictures (a) (h) in Table 1 are used as the node  feationary made by Wilson et al. (2005)5 for English  tures (Equation (4)) for the i-th phrase, and fea-experiments (The dictionary contains 2,289 positive  tures (A) (E) are used as the edge features for the  expressions and 4,143 negative expressions). When  i-th and j-th phrases ( j= hi). In Table 1, si denotes a phrase contains the words registered in the dictio-the hidden variable which represents the polarity of  naries, its prior polarity is set to the registered  pothe dependency subtree whose root node is the  ilarity, otherwise the prior polarity is set to 0. When  th phrase, qi denotes the prior polarity of the i-th a phrase contains multiple words in the dictionaries,  phrase (explained later), ri denotes the polarity re-the registered polarity of the last (nearest to the end  versal of the i-th phrase (explained later), mi de-2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/  notes the number of words in the i-th phrase, ui,k, 3http://maltparser.org/  bi,k, ci,k, fi,k respectively denote the surface form, 4http://cl.naist.jp/ inui/research/EM/sentiment-lexicon.html base form, coarse-grained part-of-speech (POS) tag,  of the sentence) word is used.  preliminary experiments, L-BFGS often did not  conThe polarity reversal of a phrase ri { 0 , 1 } rep-verge and classification accuracy was unstable when  resents whether it reverses the polarities of other  the initial values of parameters were randomly set.  phrases (1) of not (0). We prepared polarity  reversTherefore, in later experiments, we set the initial  ing word dictionaries, and the polarity reversal of  values in the following way. For the feature (A) in  a phrase is set to 1 if the phrase contains a word  Table 1 in which si and sj are equal, we set the ini-in the dictionaries, otherwise set to 0.  We  contial parameter i of the feature to a random number structed polarity reversing word dictionaries which  in [0 . 9 , 1 . 1], otherwise we set to a random number in contain such words as decrease and vanish that  re[ 0 . 1 , 0 . 1]7. By setting such initial values, the initial verse sentiment polarity. A Japanese polarity revers-model parameters have a property that two phrases  ing word dictionary was constructed from an  autowith head-modifier relation tend to have the same  matically constructed corpus, and the construction  polarity, which is intuitively reasonable.  procedure is described in Appendix B (The  dictionary contains 219 polarity reversing words).  English polarity reversing word dictionary was  constructed from the General Inquirer dictionary6 in the  We conducted experiments of sentiment  classificasame way as Choi and Cardie (2008), by collecting  tion on four Japanese corpora and four English  corwords which belong to either NOTLW or DECREAS  categories (The dictionary contains 121 polarity  reChoi and Cardie (2008) categorized polarity  reWe used four corpora for experiments of Japanese  versing words into two categories: function-word  sentiment classification: the Automatically  Connegators such as not and content-word negators such structed Polarity-tagged corpus (ACP) (Kaji and  as eliminate. The polarity reversal of a phrase ri ex-Kitsuregawa, 2006), the Kyoto University and NTT  plained above handles only the content-word  negaBlog corpus (KNB) 8, the NTCIR Japanese opinion  tors, and function-word negators are handled in  another way, since the scope of a function-word  negathe 50 Topics Evaluative Information corpus (50  tor is generally limited to the phrase containing it in  Topics) (Nakagawa et al., 2008). The ACP corpus  Japanese, and the number of function-word negators  is an automatically constructed corpus from HTML  is small. The prior polarity qi and the polarity rever-documents on the Web using lexico-syntactic  patsal ri of a phrase are changed to the following q0i and terns and layout structures. The size of the corpus  r0i, if the phrase contains a function-word negator (in is large (it consists of 650,951 instances), and we  Japanese) or if the phrase is modified by a  functionused 1 / 100 of the whole corpus. The KNB corpus  word negator (in English):  consists of Japanese blogs, and is manually  annotated. The NTC-J corpus consists of Japanese  newspaper articles. There are two NTCIR Japanese  opinion corpora available, the NTCIR-6 corpus and the  NTCIR-7 corpus; and we combined the two  corIn this paper, unless otherwise noted, the word  popora. The 50 Topics corpus is collected from various  larity reversal is used to indicate polarity reversing pages on the Web, and is manually annotated.  caused by content-word negators, and function-word  We used four corpora for experiments of English  negators are assumed to be applied to qi and ri in the sentiment classification: the Customer Review data  above way beforehand.  7The values of most learned parameters distributed between As described in Section 2.3, there is no  guaran-1.0 and 1.0 in our preliminary experiments. Therefore, we de-tee of global optimality for estimated parameters,  cided to give values around the upper bound (1.0) and the mean since the objective function is not convex. In our  (0.0) to the features in order to incorporate minimal prior knowl-edge into the model.  (CR)9, the MPQA Opinion corpus (MPQA)10, the reversal phrases in their ancestors are reversed  Movie Review Data (MR) 11, and the NTCIR  English opinion corpus (NTC-E) (Seki et al., 2007;  Seki et al., 2008). The CR corpus consists of  re( 1) rj + 0 . 5 p 0 . (13) view articles about products such as digital cameras  and cellular phones. There are two customer review  datasets, the 5 products dataset and the 9 products  Rule The polarity of a subjective sentence is  deterdataset, and we combined the two datasets. In the  ministically decided basing on rules, by  conMPQA corpus, sentiment polarities are attached not  sidering the sentiment polarities of dependency  to sentences but expressions (sub-sentences), and we  subtrees. The polarity of the dependency  subregarded the expressions as sentences and classified  tree whose root is the i-th phrase is decided by  the polarities. There are two NTCIR English  corvoting the prior polarity of the i-th phrase and  pora available, the NTCIR-6 corpus and the  NTCIRthe polarities of the dependency subtrees whose  7 corpus, and we combined the two corpora.  root nodes are the modifiers of the i-th phrase.  The statistical information of the corpora we used  The polarities of the modifiers are reversed if  is shown in Table 2. We randomly split each corpus  their head phrase has a reversal word. The  deinto 10 portions, and conducted 10-fold cross  validacision rule is applied from leaf nodes in the  detion. Accuracy of sentiment classification was  calpendency tree, and the polarity of the root node  culated as the number of correctly predicted labels  is decided at the last.  (polarities) divided by the number of test examples.  s  s  Compared Methods  We compared our method to 6 baseline methods,  and this section describes them. In the following,  p 0 {+1 , 1 } denotes the major polarity in train-Bag-of-Features with No Dictionaries The  polaring data, H i denotes the set consisting of all the an-ity of a subjective sentence is classified  uscestor nodes of the i-th phrase in the dependency ing Support Vector Machines. Surface forms,  tree, and sgn( x) is defined as below:  base forms, coarse-grained POS tags and  finegrained POS tags of word unigrams and  bigrams in the subjective sentence are used as  features12. The second order polynomial  kernel is used and the cost parameter C is set to  Voting without Polarity Reversal The polarity of  1 . 0. No prior polarity information (dictionary) a subjective sentence is decided by voting of  is used.  each phrase's prior polarity. In the case of a  Bag-of-Features without Polarity Reversal Same  tie, the major polarity in the training data is  to Bag-of-Features with No Dictionaries,  exadopted.  cept that the voting result of prior polarities  (one of positive, negative or tie) is also used  as a feature.  Bag-of-Features with Polarity Reversal Same to  Voting with Polarity Reversal Same  to  Bag-of-Features without Polarity Reversal,  exwithout Polarity Reversal, except that the  pocept that the polarities of phrases which have  larities of phrases which have odd numbers of  12In experiments on English corpora, only the features of un-9http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html  igrams are used and those of bigrams are not used, since the 10http://www.cs.pitt.edu/mpqa/  bigram features decreased accuracies in our preliminary  experi-11http://www.cs.cornell.edu/People/pabo/movie-reviewments as reported in previous work (Andreevskaia and Bergler, data/  Corpus  Number of Instances  (Positive / Negative)  ACP  NTC-J  CR  English  NTC-E  Table 2: Statistical Information of Corpora  English  ACP  NTC-J  CR  NTC-E  Voting-w/o Rev.  Tree-CRF  (* indicates statistical significance at p < 0 . 05) Table 3: Accuracy of Sentiment Classification  odd numbers of reversal phrases in their  ancesprior polarity dictionaries and the polarity  reverstors are reversed before voting.  ing word dictionaries), but the latter performed  better than the former. Our error analysis showed that  Tree-CRF The proposed method based on  depenBoF-w/ Rev. was not robust for erroneous words in  dency trees using CRFs, described in Section 2.  the prior polarity dictionaries. BoF-w/ Rev. uses the  voting result of the prior polarities as a feature, and  the feature is sensitive to the errors in the dictionary, The experimental results are shown in Table 3. The  while Tree-CRF uses several information as well as  proposed method Tree-CRF obtained the best  acthe prior polarities to decide the polarities of  depencuracies for all the four Japanese corpora and the  dency subtrees, and was robust to the dictionary  erfour English corpora, and the differences against  rors. We investigated the trained model parameters  the second best methods were statistically  signifiof Tree-CRF, and found that the features (E) in  Tacant ( p < 0 . 05) with the paired t-test for the six ble 1, in which the head and the modifier have op-of the eight corpora. Tree-CRF performed better  posite polarities and the head word is such as  profor the Japanese corpora than for the English  cortect and withdraw, have large positive weights. Al-pora. For both the Voting methods and the  Bag-ofthough these words were not included in the  polarFeatures methods, the methods with polarity  reverity reversing word dictionary, the property that these  sal performed better than those without it13.  words reverse polarities of other words seems to be  Both BoF-w/ Rev. and Tree-CRF use supervised  learned with the model.  machine learning and the same dictionaries (the  13The Japanese polarity reversing word dictionary was con-4  Related Work  structed from the ACP corpus as described in Appendix B, and it is not reasonable to compare the methods with and without polarity reversal on the ACP corpus. However, the tendency Various studies on sentiment classification have  can be seen on the other 7 corpora.  been conducted, and there are several methods  proposed for handling reversal of polarities. In this pa-of a subjective sentence is represented by a  hidper, our method was not directly compared with the  den variable. The values of the hidden variables  other methods, since it is difficult to completely  imare calculated in consideration of interactions  beplement them or conduct experiments with exactly  tween variables whose nodes have head-modifier  rethe same settings.  lation in the dependency tree.  The value of the  Choi and Cardie (2008) proposed a method to  hidden variable of the root node is identified with  classify the sentiment polarity of a sentence  basthe polarity of the whole sentence. Experimental  ing on compositional semantics. In their method,  results showed that the proposed method performs  the polarity of the whole sentence is determined  better for Japanese and English data than the  basefrom the prior polarities of the composing words by  line methods which represents subjective sentences  pre-defined rules, and the method differs from ours  as bag-of-features.  which uses the probabilistic model to handle  interactions between hidden variables. Syntactic structures  were used in the studies of Moilanen and Pulman  Rules for Converting Word Sequence to  (2007) and, Jia et al. (2009), but their methods are  based on rules and supervised learning was not used  to handle polarity reversal. As discussed in  SecLet v 1 , , vN denote an English word sequence, yi tion 1, Wilson et al. (2005) studied a bag-of-features  the part-of-speech of the i-th word, and zi the head based statistical sentiment classification method in-index of the i-th word. The word sequence was concorporating head-modifier relation.  verted to a phrase sequence as follows, by applying  Ikeda et al. (2008) proposed a machine learning  rules which combine two adjacent words:  approach to handle sentiment polarity reversal. For  each word with prior polarity, whether the polarity is  P P { IN , RP , TO , DT , PDT , PRP , WDT , WP , WP$ , WRB }  reversed or not is learned with a statistical learning  algorithm using its surrounding words as features.  The method can handle only words with prior  polarif xi and xi+1 are not yet combined  ities, and does not use syntactic dependency  strucConditional random fields with hidden variables  have been studied so far for other tasks.  LatentCombine the words vi and vi+1  until No rules are applied  Dynamic Conditional Random Fields (LDCRF)  Construction of Japanese Polarity  abilistic models with hidden variables for  sequenReversing Word Dictionary  tial labeling, and belief propagation is used for  inference. Out method is similar to the models, but  We constructed a Japanese polarity reversing word  there are several differences. In our method, only  dictionary from the Automatically Constructed  one variable which represents the polarity of the  Polarity-tagged corpus (Kaji and Kitsuregawa,  whole sentence is observable, and dependency  re2006). First, we collected sentences, each of which  lation among random variables is not a linear chain  contains just one phrase having prior polarity, and  but a tree structure which is identical to the syntactic  the phrase modifies a phrase which modifies the root  node. Among them, we selected sentences in which  the prior polarity is not equal to the polarity of the  Conclusion  whole sentence. We extracted all the words in the  head phrase, and manually checked them whether  In this paper, we presented a dependency tree-based  they should be put into the dictionary or not. The  ramethod for sentiment classification using  conditionale behind the procedure is that the prior polarity  tional random fields with hidden variables. In this  can be considered to be reversed by a certain word  method, the polarity of each dependency subtree  in the head phrase.  Sadao Kurohashi. 2008. Extracting Subjective and  Objective Evaluative Expressions from the Web. In  Specialists and Generalists Work Together:  OvercomProceedings of the 2nd International Symposium on  Universal Communication.  Proceedings of the 46th Annual Meeting of the  AssoBo Pang and Lillian Lee. 2008. Opinion Mining and  ciation for Computational Linguistics: Human  LanSentiment Analysis. Foundations and Trends in  InforMining the Web:  Discovering Knowledge from Hypertext Data.  MorganThumbs up? Sentiment Classification using  Yejin Choi and Claire Cardie.  Learning with  Machine Learning Techniques. In Proceedings of the  Compositional Semantics as Structural Inference for  2002 Conference on Empirical Methods in Natural  Subsentential Sentiment Analysis. In Proceedings of  Language Processing, pages 79 86.  the 2008 Conference on Empirical Methods in Natural  Adwait Ratnaparkhi. 1996. A Maximum Entropy Model  Language Processing, pages 793 801.  for Part-of-Speech Tagging.  In Proceedings of the  1996 Conference on Empirical Methods in Natural  sumoto. 2008. Acquiring Noun Polarity Knowledge  Language Processing Conference, pages 133 142.  Using Selectional Preferences. In Proceedings of the  14th Annual Meeting of the Association for Natural  Chen, Noriko Kando, and Chin-Yew Lin.  Language Processing, pages 584 587. (in Japanese).  Overview of Opinion Analysis Pilot Task at  NTCIR6. In Proceedings of the 6th NTCIR Workshop, pages Manabu Okumura. 2008. Learning to Shift the Po-265 278.  larity of Words for Sentiment Classification. In  Proceedings of the 3rd International Joint Conference on Hsin-Hsi Chen, and Noriko Kando. 2008. Overview  Natural Language Processing, pages 296 303.  of Multilingual Opinion Analysis Task at NTCIR-7. In  Lifeng Jia, Clement Yu, and Weiyi Meng. 2009. The  EfProceedings of the 7th NTCIR Workshop.  fect of Negation on Sentiment Analysis and Retrieval  Effectiveness. In Proceeding of the 18th ACM  Conand Jun'ichi Tsujii. 2008. Modeling Latent-Dynamic  ference on Information and Knowledge Management,  in Shallow Parsing: A Latent Conditional Model with  Improved Inference. In Proceedings of the 22nd  International Conference on Computational Linguistics, matic Construction of Polarity-Tagged Corpus from  In Proceedings of the  COLTheresa Wilson, Janyce Wiebe, and Paul Hoffmann.  ING/ACL 2006 Main Conference Poster Sessions,  Recognizing Contextual Polarity in  PhraseLevel Sentiment Analysis. In Proceedings of the 2005  Joint Conference on Human Language Technology and  2007. Opinion Mining from Web Documents:  ExtracEmpirical Methods in Natural Language Processing, tion and Structurization. Journal of the Japanese So-pages 347 354.  ciety for Artificial Intelligence, 22(2):227 238.  Dong C. Liu and Jorge Nocedal. 1989. On the limited  Valence Shifters. In Proceedings of the AAAI Spring  memory BFGS method for large scale optimization.  Symposium on Exploring Attitude and Affect in Text.  Mathematical Programming, 45(3):503 528.  David J. C. MacKay. 2003. Information Theory,  Inference, and Learning Algorithms. Cambridge  University Press.  Composition. In Proceedings of the Recent Advances  in Natural Language Processing International  ConferLouis-Philippe Morency, Ariadna Quattoni, and Trevor  Darrell. 2007. Latent-Dynamic Discriminative  Models for Continuous Gesture Recognition. In  Proceedings of the 2007 IEEE Conference on Computer Vision  and Pattern Recognition, pages 1 8. 