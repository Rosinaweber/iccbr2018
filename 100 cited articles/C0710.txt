 Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences Hong Yu  Department of Computer Science  Department of Computer Science  Columbia University  Columbia University  New York, NY 10027, USA  New York, NY 10027, USA  trieving only editorials in favor of a particular policy Opinion question answering is a challenging task  for natural language processing. In this paper, we Our motivation for building the opinion detec-discuss a necessary component for an opinion question answering system: separating opinions from  tion and classification system described in this pa-fact, at both the document and sentence level. We per is the need for organizing information in the present a Bayesian classifier for discriminating be-context of question answering for complex  questween documents with a preponderance of opinions  such as editorials from regular news stories, and tions.  Unlike questions like Who was the first  describe three unsupervised, statistical techniques man on the moon? which can be answered with  for the significantly harder task of detecting opin-a simple phrase, more intricate questions such as ions at the sentence level. We also present a first model for classifying opinion sentences as positive  What are the reasons for the US-Iraq war? require or negative in terms of the main perspective be-long answers that must be constructed from  multiing expressed in the opinion. Results from a large ple sources. In such a context, it is imperative that collection of news stories and a human evaluation of 400 sentences are reported, indicating that we the question answering system can discriminate be-achieve very high performance in document  classitween opinions and facts, and either use the appro-fication (upwards of 97% precision and recall), and respectable performance in detecting opinions and priate type depending on the question or combine  classifying them at the sentence level as positive, them in a meaningful presentation. Perspective in-negative, or neutral (up to 91% accuracy).  formation can also help highlight contrasts and con-tradictions between different sources there will be significant disparity in the material collected for the 1  Introduction  question mentioned above between Fox News and  Newswire articles include those that mainly present the Independent, for example.  opinions or ideas, such as editorials and letters to Fully analyzing and classifying opinions involves the editor, and those that mainly report facts such as tasks that relate to some fairly deep semantic and daily news articles. Text materials from many other syntactic analysis of the text. These include not only sources also contain mixed facts and opinions. For recognizing that the text is subjective, but also de-many natural language processing applications, the termining who the holder of the opinion is, what the ability to detect and classify factual and opinion sen-opinion is about, and which of many possible posi-tences offers distinct advantages in deciding what in-tions the holder of the opinion expresses regarding formation to extract and how to organize and present that subject. In this paper, we are presenting three this information. For example, information extrac-of the components of our opinion detection and ortion applications may target factual statements rather ganization subsystem, which have already been in-than subjective opinions, and summarization  systegrated into our larger question-answering system.  tems may list separately factual information and ag-These components deal with the initial tasks of clas-gregate opinions according to distinct perspectives.  sifying articles as mostly subjective or objective, At the document level, information retrieval systems finding opinion sentences in both kinds of articles, can target particular types of articles and even utilize and determining, in general terms and without refer-perspectives in focusing queries (e.g., filtering or re-ence to a specific subject, if the opinions are positive  or negative. The three modules of the system dis-human emotions. Hatzivassiloglou and McKeown  cussed here provide the basis for ongoing work for (1997) described an unsupervised learning method  further classification of opinions according to sub-for obtaining positively and negatively oriented adject and opinion holder and for refining the original jectives with accuracy over 90%, and demonstrated positive/negative attitude determination.  that this semantic orientation, or polarity, is a con-We review related work in Section 2, and then  sistent lexical property with high inter-rater agree-present our document-level classifier for opinion or ment.  Turney (2002) showed that it is possible  factual articles (Section 3), three implemented tech-to use only a few of those semantically oriented  niques for detecting opinions at the sentence level words (namely, excellent and poor ) to label  (Section 4), and our approach for rating an opinion other phrases co-occuring with them as positive or as positive or negative (Section 5). We have evalu-negative. He then used these phrases to  automatiated these methods using a large collection of news cally separate positive and negative movie and prod-articles without additional annotation (Section 6) uct reviews, with accuracy of 66 84%. Pang et al.  and an evaluation corpus of 400 sentences  anno(2002) adopted a more direct approach, using supertated for opinion classifications (Section 7). The vised machine learning with words and n-grams as  results, presented in Section 8, indicate that we features to predict orientation at the document level achieve very high performance (more than 97%) at  with up to 83% precision.  document-level classification and respectable per-Our approach to document and sentence  classiformance (86 91%) at detecting opinion sentences  fication of opinions builds upon the earlier work and classifying them according to orientation.  by using extended lexical models with additional  features. Unlike the work cited above, we do not  Related Work  rely on human annotations for training but only on weak metadata provided at the document level. Our Much of the earlier research in automated opinion sentence-level classifiers introduce additional crite-detection has been performed by Wiebe and  colria for detecting subjective material (opinions), in-leagues (Bruce and Wiebe, 1999; Wiebe et al., 1999; cluding methods based on sentence similarity within Hatzivassiloglou and Wiebe, 2000; Wiebe, 2000;  a topic and an approach that relies on multiple clas-Wiebe et al., 2002), who proposed methods for dissifiers. At the document level, our classifier uses the criminating between subjective and objective text at same document labels that the method of (Wiebe et the document, sentence, and phrase levels. Bruce  al., 2002) does, but automatically detects the words and Wiebe (1999) annotated 1,001 sentences as sub-and phrases of importance without further  analysis of the text. For determining whether an  opinscribed a sentence-level Naive Bayes classifier using ion sentence is positive or negative, we have used as features the presence or absence of particular synseed words similar to those produced by (Hatzivas-tactic classes (pronouns, adjectives, cardinal num-siloglou and McKeown, 1997) and extended them to  bers, modal verbs, adverbs), punctuation, and sen-construct a much larger set of semantically oriented tence position. Subsequently, Hatzivassiloglou and words with a method similar to that proposed by  Wiebe (2000) showed that automatically detected  (Turney, 2002). Our focus is on the sentence level, gradable adjectives are a useful feature for subjec-unlike (Pang et al., 2002) and (Turney, 2002); we tivity classification, while Wiebe (2000) introduced employ a significantly larger set of seed words, and lexical features in addition to the presence/absence we explore as indicators of orientation words from of syntactic categories. More recently, Wiebe et al.  syntactic classes other than adjectives (nouns, verbs, (2002) report on document-level subjectivity classi-and adverbs).  fication, using a k-nearest neighbor algorithm based on the total count of subjective words and phrases 3  Document Classification  within each document.  Psychological studies (Bradley and Lang, 1999)  To separate documents that contain primarily opin-found measurable associations between words and  ions from documents that report mainly facts, we  applied Naive Bayes1, a commonly used supervised  machine-learning algorithm.  This approach  preal., 2001), a state-of-the-art system for measuring supposes the availability of at least a collection of ar-sentence similarity based on shared words, phrases, ticles with pre-assigned opinion and fact labels at the and WordNet synsets. To measure the overall simi-document level; fortunately, Wall Street Journal ar-larity of a sentence to the opinion or fact documents, ticles contain such metadata by identifying the type we first select the documents that are on the same of each article as Editorial, Letter to editor, Business topic as the sentence in question. We obtain topics and News. These labels are used only to provide the as the results of IR queries (for example, by search-correct classification labels during training and eval-ing our document collection for welfare reform ).  uation, and are not included in the feature space. We We then average its SIMFINDER-provided similari-used as features single words, without stemming or ties with each sentence in those documents. Then  stopword removal. Naive Bayes assigns a document  we assign the sentence to the category for which the to the class  that maximizes  by applying  average is higher (we call this approach the score  and assuming  convariant). Alternatively, for the frequency variant, ditional independence of the features.  we do not use the similarity scores themselves but Although Naive Bayes can be outperformed in  instead we count how many of them, for each  catetext classification tasks by more complex methods gory, exceed a predetermined threshold (empirically such as SVMs, Pang et al. (2002) report similar per-set to 0.65).  formance for Naive Bayes and other machine  learnNaive Bayes Classifier  ing techniques for a similar task, that of distinguishing between positive and negative reviews at the  Our second method trains a Naive Bayes classifier document level. Further, we achieved such high  per(see Section 3), using the sentences in opinion and formance with Naive Bayes (see Section 8) that ex-fact documents as the examples of the two  cateploring additional techniques for this task seemed gories. The features include words, bigrams, and  trigrams, as well as the parts of speech in each sentence. In addition, the presence of semantically ori-4  Finding Opinion Sentences  ented (positive and negative) words in a sentence is an indicator that the sentence is subjective (Hatzi-We developed three different approaches to  clasvassiloglou and Wiebe, 2000). Therefore, we  insify opinions from facts at the sentence level. To clude in our features the counts of positive and neg-avoid the need for obtaining individual sentence anative words in the sentence (which are obtained with notations for training and evaluation, we rely in-the method of Section 5.1), as well as counts of  stead on the expectation that documents classified the polarities of sequences of semantically oriented as opinion on the whole (e.g., editorials) will tend to words (e.g., ++ for two consecutive positively ori-have mostly opinion sentences, and conversely docented words). We also include the counts of parts uments placed in the factual category will tend to of speech combined with polarity information (e.g., have mostly factual sentences. Wiebe et al. (2002)  JJ+ for positive adjectives), as well as features en-report that this expectation is borne out 75% of the coding the polarity (if any) of the head verb, the time for opinion documents and 56% of the time for main subject, and their immediate modifiers. Syn-factual documents.  tactic structure was obtained with Charniak's statistical parser (Charniak, 2000). Finally, we used as 4.1  Similarity Approach  one of the features the average semantic orientation Our first approach to classifying sentences as opin-score of the words in the sentence.  ions or facts explores the hypothesis that, within a given topic, opinion sentences will be more simi-4.3  Multiple Naive Bayes Classifiers  lar to other opinion sentences than to factual sen-Our designation of all sentences in opinion or factual 1Using the Rainbow implementation, available from articles as opinion or fact sentences is an approxima-www.  cs.cmu.edu/ mccallum/bow/rainbow.  tion. To address this, we apply an algorithm using  multiple classifiers, each relying on a different sub-whether multiple seed words have a positive effect set of our features. The goal is to reduce the training in detection performance. We experimented with  set to the sentences that are most likely to be cor-seed sets containing 1, 20, 100 and over 600 positive rectly labeled, thus boosting classification accuracy.  and negative pairs of adjectives. For a given seed set Given separate sets of features  size, we denote the set of positive seeds as ADJ  we train separate Naive Bayes classifiers  and the set of negative seeds as ADJ . We then cal-corresponding to each feature set.  Assumculate a modified log-likelihood ratio  ing as ground truth the information provided by the for a word  with part of speech POS ( can be  document labels and that all sentences inherit the adjective, adverb, noun or verb) as the ratio of its status of their document as opinions or facts, we collocation frequency with ADJ and ADJ within a first train  on the entire training set, then use the  resulting classifier to predict labels for the training  set. The sentences that receive a label different from Freq 2'  the assumed truth are then removed, and we train  on the remaining sentences. This process is  repeated iteratively until no more sentences can be re-where Freq  represents the  collomoved. We report results using five feature sets, cation frequency of all words  all of part of speech  starting from words alone and adding in bigrams, tri-POS with ADJ  and  is a smoothing constant  grams, part-of-speech, and polarity.  in our case). We used Brill's tagger (Brill,  1995) to obtain part-of-speech information.  Identifying the Polarity of Opinion  Sentence Polarity Tagging  As our measure of semantic orientation across an  Having distinguished whether a sentence is a fact or opinion, we separate positive, negative, and neutral entire sentence we used the average per word log-likelihood scores defined in the preceding section.  opinions into three classes. We base this decision on the number and strength of semantically oriented To determine the orientation of an opinion sentence, all that remains is to specify cutoffs  and  words (either positive or negative) in the sentence.  We first discuss how such words are automatically that sentences for which the average log-likelihood score exceeds  are classified as positive opinions,  found by our system, and then describe the method  by which we aggregate this information across the sentences with scores lower than  are classified  as negative opinions, and sentences with in-between scores are treated as neutral opinions. Optimal val-5.1  Semantically Oriented Words  and  are obtained from the training data  via density estimation using a small, hand-labeled To determine which words are semantically ori-subset of sentences we estimate the proportion of ented, in what direction, and the strength of their sentences that are positive or negative. The values orientation, we measured their co-occurrence with of the average log-likelihood score that correspond words from a known seed set of semantically ori-to the appropriate tails of the score distribution are ented words. The approach is based on the hypothe-then determined via Monte Carlo analysis of a much sis that positive words co-occur more than expected larger sample of unlabeled training data.  by chance, and so do negative words; this hypothesis was validated, at least for strong positive/negative 6  words, in (Turney, 2002). As seed words, we used  subsets of the 1,336 adjectives that were manually We used the TREC2 8, 9, and 11 collections, which classified as positive (657) or negative (679) by consist of more than 1.7 million newswire arti-Hatzivassiloglou and McKeown (1997). In earlier  The aggregate collection covers six  differwork (Turney, 2002) only singletons were used as  ent newswire sources including 173,252 Wall Street seed words; varying their number allows us to test 2http://trec.nist.gov/.  Journal (WSJ) articles from 1987 to 1992. Some  of the WSJ articles have structured headings that Fact  include Editorial, Letter to editor, Business, and Opinion  News (2,877, 1,695, 2,009 and 3,714 articles, re-Uncertain  spectively). We randomly selected 2,000 articles3  Breakdown of opinion labels  from each category so that our data set was approx-Positive  imate evenly divided between fact and opinion  arNegative  ticles. Those articles were used for both document No orientation  and sentence level opinion/fact classification.  Mixed orientation  Uncertain orientation  Evaluation Metrics and Gold Standard  Table 1: Statistics of gold standards A and B.  For classification tasks (i.e., classifying between facts and opinions and identifying the semantic ori-documents from the same topic were retrieved to  entation of sentences), we measured our system's  supply the missing sentences. The resulting  performance by standard recall and precision. We  sentences were then interleaved so  evaluated the quality of semantically oriented words  that successive sentences came from different top-by mapping the extracted words and labels to an exics and documents and divided into ten 50-sentence ternal gold standard. We took the subset of our out-blocks. Each block shares ten sentences with the  put containing words that appear in the standard, and preceding and following block (the last block is con-measured the accuracy of our output as the portion sidered to precede the first one), so that 100 of the of that subset that was assigned the correct label.  400 sentences appear in two blocks. Each of ten hu-A gold standard for document-level classification man evaluators (all with graduate training in com-is readily available, since each article in our Wall putational linguistics) was presented with one block Street Journal collection comes with an article type and asked to select a label for each sentence among label (see Section 6). We mapped article types News the following: fact , positive opinion , negative and Business to facts, and article types Editorial and opinion , neutral opinion , sentence contains both Letter to the Editor to opinions. We cannot auto-positive and negative opinions , opinion but cannot matically select a sentence-level gold standard dis-determine orientation , and uncertain .5  criminating between facts and opinions, or between Since we have one judgment for 300 sentences  positive and negative opinions. We therefore asked and two judgments for 100 sentences, we created  human evaluators to classify a set of sentences be-two gold standards for sentence classification. The tween facts and opinions as well as determine the first (Standard A) includes the 300 sentences with type of opinions.  one judgment and a single judgment for the remain-Since we have implemented our methods in an  ing 100 sentences.6 The second standard (Standard opinion question answering system, we selected four B) contains the subset of the 100 sentences for which different topics ( gun control, illegal aliens, social we obtained identical labels. Statistics of these two security, and welfare reform). For each topic, we standards are given in Table 1. We measured the  randomly selected 25 articles from the entire com-pairwise agreement among the 100 sentences that  bined TREC corpus (not just the WSJ portion); these were judged by two evaluators, as the ratio of sen-were articles matching the corresponding topical  tences that receive a label  from both evaluators  phrase given above as determined by the Lucene  divided by the total number of sentences receiving search engine.4 From each of these documents we  from any evaluator. The agreement across  randomly selected four sentences. If a document  happened to have less than four sentences, additional 5The full instructions can be viewed online at http:  //www1.cs.columbia.edu/ hongyu/research/  3Except for Letters to Editor, for which we included all instructions.html.  6In order to assign a unique label, we arbitrarily chose the 4http://www.jguru.com/faq/Lucene.  first evaluator for those sentences.  Class  Standard A  News vs. Editorial  Fact  News+Business vs. Editorial+Letter  Opinion  Fact  Table 2: Document-level fact/opinion classification Frequency  Opinion  by Naive Bayes algorithm.  Table 3: Recall, precision of similarity classifier.  the 100 sentences for all seven choices was 55%;  if we group together the five subtypes of opinion is also consistent with a high inter-rater agreement sentences, the overall agreement rises to 82%. The ( kappa=0.95) for document-level fact/opinion anno-low agreement for some labels was not surprising  tation (Wiebe et al., 2002). Note that we trained and because there is much ambiguity between facts and evaluated only on WSJ articles for which we can ob-opinions. An example of an arguable sentence is A tain article class metadata, so the classifier may per-lethal guerrilla war between poachers and wardens form less accurately when used for other newswire now rages in central and eastern Africa , which one articles.  rater classified as fact and another rater classified as opinion .  Sentence Classification  Table 3 shows the  reFinally, for evaluating the quality of extracted  call and precision of the similarity-based approach, words with semantic orientation labels, we used two while Table 4 lists the recall and precision of naive distinct manually labeled collections as gold stan-Bayes (single and multiple classifiers) for sentence-dards. One set consists of the previously described level opinion/fact classification. In both cases, the 657 positive and 679 negative adjectives (Hatzi-results are better when we evaluate against  Stanvassiloglou and McKeown, 1997). We also used  dard B, containing the sentences for which two hu-the ANEW list which was constructed during  psymans assign the same label; obviously, it is easier for cholinguistic experiments (Bradley and Lang, 1999) the automatic system to produce the correct label in and contains 1,031 words of all four open classes.  these more clear-cut cases.  As described in (Bradley and Lang, 1999), humans  Our Naive Bayes classifier has a higher recall and assigned valence scores to each score according to precision (80 90%) for detecting opinions than for dimensions such as pleasure, arousal, and domi-facts (around 50%). While words and n-grams had  nance; following heuristics proposed in psycholin-little performance effect for the opinion class, they guistics7 we obtained 284 positive and 272 negative increased the recall for the fact class around five fold words from the valence scores.  compared to the approach by Wiebe et al. (1999).  In general, the additional features helped the classi-8  Results and Discussion  fier; the best performance is achieved when words, bigrams, trigrams, part-of-speech, and polarity are Document Classification  We trained our Bayes  included in the feature set. Further, using multiple classifier for documents on 4,000 articles from the classifiers to automatically identify an appropriate WSJ portion of our combined TREC collection, and  subset of the data for training slightly increases per-evaluated on 4,000 other articles also from the WSJ  formance.  part. Table 2 lists the F-measure scores (the har-monic mean of precision and recall) of our Bayesian Polarity Classification  Using the method of  Secclassifier for document-level opinion/fact classification 5.1, we automatically identified a total of 39,652  tion. The results show the classifier achieved 97%  (65,773), 3,128 (4,426), 144,238 (195,984), and  F-measure, which is comparable or higher than the 22,279 (30,609) positive (negative) adjectives, ad-93% accuracy reported by (Wiebe et al., 2002),  verbs, nouns, and verbs, respectively. Extracted pos-who evaluated their work based on a similar set of itive words include inspirational, truly, luck, and WSJ articles. The high classification performance achieve.  Negative ones include depraved, disas-7http://www.sci.sdsu.edu/CAL/wordlist/.  trously, problem, and depress. Figure 1 plots the  Standard A  Features  Class  Fact  Features from (Wiebe et al., 1999)  Opinion  Fact  Words only  Opinion  Fact  Words and Bigrams  Opinion  Fact  Words, Bigrams, and Trigrams  Opinion  Words, Bigrams, Trigrams,  Fact  and Part-of-Speech  Opinion  Words, Bigrams, Trigrams,  Fact  Part-of-Speech, and Polarity  Opinion  Recall, precision of opinion/fact sentence classification using different features and either a single or multiple (data cleaning) classifiers.  Parts-of-speech Used  Adjectives  Nouns  Adjectives and Adverbs  Adjectives, Adverbs, and Verbs  Adjectives, Adverbs, Nouns,  Table 5: Accuracy of sentence polarity tagging on gold standards A and B for different sets of parts-of-speech.  Figure 1: Recall and precision (1,336 manually labeled positive and negative adjectives as gold standard) of extracted adjectives using 1, 20, and 100  polarity to opinion sentences. Table 5 lists the accu-positive and negative adjective pairs as seeds.  racy of our sentence-level tagging process. We experimented with different combinations of part-of-speech classes for calculating the aggregate polarity recall and precision of extracted adjectives by us-scores, and found that the combined evidence from ing randomly selected seed sets of 1, 20, and 100  adjectives, adverbs, and verbs achieves the highest pairs of positive and negative adjectives from the list accuracy (90% over a baseline of 48%). As in the  of (Hatzivassiloglou and McKeown, 1997). Both  recase of sentence-level classification between opin-call and precision increase as the seed set becomes ion and fact, we also found the performance to be larger. We obtained similar results with the ANEW  higher on Standard B, for which humans exhibited  list of adjectives (Section 7). As an additional ex-consistent agreement.  periment, we tested the effect of ignoring sentences with negative particles, obtaining a small increase in 9  precision and recall.  We subsequently used the automatically extracted  We presented several models for distinguishing be-polarity score for each word to assign an aggregate tween opinions and facts, and between positive and  negative opinions. At the document level, a fairly Eric Brill.  straightforward Bayesian classifier using lexical in-learning and natural language processing: A case study in part of speech tagging. Computational Linguistics.  formation can distinguish between mostly factual  and mostly opinion documents with very high  preRebecca Bruce and Janyce Wiebe. 1999. Recognizing cision and recall (F-measure of 97%). The task is subjectivity: A case study in manual tagging. Natural much harder at the sentence level. For that case, Language Engineering, 5(2).  we described three novel techniques for opinion/fact Eugene Charniak. 2000. A maximum-entropy inspired classification achieving up to 91% precision and re-parser. In Proceedings of NAACL-2000.  call on the detection of opinion sentences. We also Vasileios Hatzivassiloglou and Kathleen R. McKeown.  examined an automatic method for assigning  polar1997. Predicting the semantic orientation of adjec-ity information to single words and sentences, accu-tives.  In Proceedings of the 35th Annual Meeting  rately discriminating between positive, negative, and of the ACL and the 8th Conference of the European neutral opinions in 90% of the cases.  Chapter of the ACL, pages 174 181, Madrid, Spain, July. Association for Computational Linguistics.  Our work so far has focused on characterizing  opinions and facts in a generic manner, without ex-Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-amining who the opinion holder is or what the opin-fects of adjective orientation and gradability on sentence subjectivity. In Conference on Computational ion is about. While we have found presenting in-Linguistics (COLING-2000).  formation organized in separate opinion and fact  classes useful, our goal is to introduce further analy-Vasileios Hatzivassiloglou, Judith Klavans, Melissa Hol-sis of each sentence so that opinion sentences can be combe, Regina Barzilay, Min-Yen Kan, and Kathleen McKeown. 2001. SIMFINDER: A flexible clustering  linked to particular perspectives on a specific sub-tool for summarization. In Proceedings of the Work-ject. We intend to cluster together sentences from shop on Summarization in NAACL-01.  the same perspective and present them in summary  form as answers to subjective questions.  2002. Thumps up? Sentiment classification using machine learning techniques. In Proceedings of the 2002  Acknowledgments  Conference on Empirical Methods in Natural Language Processing (EMNLP-02), Philadelphia, Penn-We wish to thank Eugene Agichtein, Sasha  Blairhadad, Kathy McKeown, Becky Passonneau, and  Peter Turney. 2002. Thumps up or thumbs down?  Semantic orientation applied to unsupervised classifica-the anonymous reviewers for valuable input on eartion of reviews. In Proceedings of the 40th Annual lier versions of this paper. We are grateful to the Meeting of the Association for Computational Linguis-graduate students at Columbia University who  participated in our evaluation of sentence-level opin-Janyce Wiebe, Rebecca Bruce, and Thomas O'Hara.  ions. This work was supported by ARDA under  1999. Development and use of a gold standard data AQUAINT project MDA908-02-C-0008. Any opin-set for subjectivity classifications. In Proceedings of ions, findings, or recommendations are those of  the 37th Annual Meeting of the Association for Com-the authors and do not necessarily reflect ARDA's putational Linguistics (ACL-99), pages 246 253.  Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and M. Martin.  Learning subjective  language. Technical Report TR-02-100, Department  of Computer Science, University of Pittsburgh, Pittsburgh, Pennsylvania.  M. M. Bradley and P. J. Lang. 1999. Affective norms for English words (ANEW): Stimuli, instruction man-Janyce Wiebe.  Learning subjective adjectives  ual and affective ratings. Technical Report C-1, The from corpora. In Proceedings of the 17th National Center for Research in Psychophysiology, University Conference on Artificial Intelligence (AAAI-2000), of Florida, Gainesville, Florida.  Austin, Texas. 