 Seeing stars when there aren't many stars: Graph-based semi-supervised learning for sentiment categorization Andrew B. Goldberg  Computer Sciences Department  Computer Sciences Department  University of Wisconsin-Madison  University of Wisconsin-Madison  Madison, W.I. 53706  Madison, W.I. 53706  ment analysis attempts to identify the subjective sentiment expressed (or implied) in documents, such as We present a graph-based semi-supervised  consumer product or movie reviews. In particular  learning algorithm to address the  sentiPang and Lee proposed the rating-inference problem ment analysis task of rating inference.  (2005). Rating inference is harder than binary posi-Given a set of documents (e.g., movie  tive / negative opinion classification. The goal is to reviews) and accompanying ratings (e.g.,  infer a numerical rating from reviews, for example  4 stars ), the task calls for inferring  nuthe number of stars that a critic gave to a movie.  merical ratings for unlabeled documents  Pang and Lee showed that supervised machine learn-based on the perceived sentiment  exing techniques (classification and regression) work pressed by their text.  In particular, we  well for rating inference with large amounts of train-are interested in the situation where  labeled data is scarce. We place this task  However, review documents often do not come  in the semi-supervised setting and  demonwith numerical ratings. We call such documents un-strate that considering unlabeled reviews  labeled data. Standard supervised machine learning in the learning process can improve rating-algorithms cannot learn from unlabeled data.  Asinference performance. We do so by  creatsigning labels can be a slow and expensive process ing a graph on both labeled and unlabeled  because manual inspection and domain expertise are data to encode certain assumptions for this  needed. Often only a small portion of the documents task. We then solve an optimization prob-can be labeled within resource constraints, so most lem to obtain a smooth rating function  documents remain unlabeled. Supervised learning  over the whole graph. When only  limalgorithms trained on small labeled sets suffer in ited labeled data is available, this method  performance. Can one use the unlabeled reviews to achieves significantly better predictive ac-improve rating-inference? Pang and Lee (2005) sug-curacy over other methods that ignore the  gested that doing so should be useful.  unlabeled examples during training.  We demonstrate that the answer is Yes.' Our  approach is graph-based semi-supervised learning.  Semi-supervised learning is an active research area 1  Introduction  in machine learning. It builds better classifiers or regressors using both labeled and unlabeled data, Sentiment analysis of text documents has received under appropriate assumptions (Zhu, 2005; Seeger, considerable attention recently (Shanahan et al., 2001). This paper contains three contributions:  2005; Turney, 2002; Dave et al., 2003; Hu and  Liu, 2004; Chaovalit and Zhou, 2005). Unlike  tra We present a novel adaptation of graph-based  ditional text categorization based on topics, senti-semi-supervised learning (Zhu et al., 2003)  Workshop on TextGraphs, at HLT-NAACL 2006, pages 45 52, New York City, June 2006. c  2006 Association for Computational Linguistics  to the sentiment analysis domain, extending 2. Optionally, we are given numerical rating pre-past supervised learning work by Pang and  for  in We design a special graph which encodes  (Joachims, 1999; Smola and Sch lkopf, 2004)  our assumptions for rating-inference problems  used by (Pang and Lee, 2005).  This acts  (section 2), and present the associated  optias an extra knowledge source for our  semimization problem in section 3;  supervised learning framework to improve  upon. We note our framework is general and  We show the benefit of semi-supervised  learnworks without the separate learner, too. (For  ing for rating inference with extensive  experithis to work in practice, a reliable similarity  mental results in section 4.  measure is required.)  A Graph for Sentiment Categorization  We now describe our graph for the  semiThe semi-supervised rating-inference problem is  supervised rating-inference problem.  We do this  formalized as follows. There are n review  docupiece by piece with reference to Figure 1. Our undi-ments x  rected graph G = (V, E) has 2n nodes V , and  1 . . . xn, each represented by some standard  feature representation (e.g., word-presence vectors).  weighted edges E among some of the nodes.  Without loss of generality, let the first l n documents be labeled with ratings y  Each document is a node in the graph (open  cir1 . . . yl C . The  remaining documents are unlabeled. In our  expercles, e.g., xi and xj). The true ratings of these iments, the unlabeled documents are also the test nodes f (x) are unobserved. This is true even  documents, a setting known as transduction. The  for the labeled documents because we allow for  set of numerical ratings are C = {c  noisy labels. Our goal is to infer f (x) for the  1, . . . , cC }, with  1 < . . . < cC R. For example, a one-star to four-star movie rating system has C = {0, 1, 2, 3}.  We seek a function f : x 7 R that gives a  contin Each labeled document (e.g., xj) is connected  uous rating f (x) to a document x. Classification is to an observed node (dark circle) whose value  done by mapping f (x) to the nearest discrete rating is the given rating yj. The observed node is  in C. Note this is ordinal classification, which dif-a dongle' (Zhu et al., 2003) since it connects  fers from standard multi-class classification in that only to xj. As we point out later, this serves  C is endowed with an order. In the following we use to pull f (xj) towards yj. The edge weight  be review' and document,' rating' and label' inter-tween a labeled document and its dongle is a  changeably.  large number M . M represents the influence  We make two assumptions:  of yj: if M then f (xj) = yj becomes a  1. We are given a similarity measure wij 0  Similarly each unlabeled document (e.g., xi) is be computable from features, so that we can  also connected to an observed dongle node  measure similarities between any documents,  whose value is the prediction of the separate  including unlabeled ones.  learner. Therefore we also require that f (xi)  plies that the two documents tend to express  is close to  yi. This is a way to incorporate  multhe same sentiment (i.e., rating). We  experitiple learners in general. We set the weight  bement with positive-sentence percentage (PSP) tween an unlabeled node and its dongle arbi-based similarity which is proposed in (Pang and  trarily to 1 (the weights are scale-invariant oth-Lee, 2005), and mutual-information modulated  erwise). As noted earlier, the separate learner  word-vector cosine similarity. Details can be  is optional: we can remove it and still carry out found in section 4.  graph-based semi-supervised learning.  neighbors  Summing over all edges in the graph, we obtain the reviews  (un)smoothness L(f ) over the whole graph. We call b w  (f ) the energy or loss, which should be minimized.  and unlabeled review indices, respectively. With the graph in Figure 1, the loss L(f ) can be written as a wij  reviews  Figure 1: The graph for semi-supervised rating in-X  ference.  A small loss implies that the rating of an unlabeled  Each unlabeled document xi is connected to  review is close to its labeled peers as well as its un-kN NL(i), its k nearest labeled documents.  labeled peers. This is how unlabeled data can par-Distance is measured by the given similarity  ticipate in learning. The optimization problem is measure w. We want f (xi) to be consistent  minf L(f ). To understand the role of the  paramewith its similar labeled documents. The weight  ters, we define = ak + bk0 and = b , so that  L(f ) can be written as  Each unlabeled document is also connected to  M (f (x  U (i), its k0 nearest unlabeled documents  (excluding itself). The weight between xi and  f (xi) to be consistent with its similar  unlabeled neighbors. We allow potentially different  numbers of neighbors (k and k0), and different  weight coefficients (a and b). These parameters  are set by cross validation in experiments.  Thus controls the relative weight between labeled neighbors and unlabeled neighbors; is roughly  The last two kinds of edges are the key to  semithe relative weight given to semi-supervised (nonsupervised learning:  They connect unobserved  nodes and force ratings to be smooth throughout the We can find the closed-form solution to the opti-graph, as we discuss in the next section.  mization problem. Defining an n n matrix  Graph-Based Semi-Supervised Learning  With the graph defined, there are several algorithms Wij =  one can use to carry out semi-supervised learning  W >) be a symmetrized version  of this matrix. Let D be a diagonal degree matrix The basic idea is the same and is what we use in this with  paper. That is, our rating function f (x) should be n  smooth with respect to the graph. f (x) is not smooth Dii =  if there is an edge with large weight w between  nodes xi and xj, and the difference between f (xi) Note that we define a node's degree to be the sum of and f (xj) is large. The (un)smoothness over the par-its edge weights. Let = D W be the  combinaticular edge can be defined as w f (xi) f (xj)2.  weight matrix with  movie-review-data/ and first used in (Pang and Lee, M,  2005). We chose 4-class instead of 3-class labeling ii =  because it is harder. The dataset is divided into four author-specific corpora, containing 1770, 902, 1307, Let f  yn)>. We can rewrite L(f ) as  ally for each author. Each document is represented  as a {0, 1} word-presence vector, normalized to sum (f y)>C(f y) +  to 1.  We systematically vary labeled set size |L|  This is a quadratic function in f . Setting the gradient  {0.9n, 800, 400, 200, 100, 50, 25, 12, 6} to observe to zero, L(f )/ f = 0 , we find the minimum loss the effect of semi-supervised learning. |L| = 0.9n function  is included to match 10-fold cross validation used  by (Pang and Lee, 2005). For each |L| we run 20  trials where we randomly split the corpus into la-k + k0  beled and test (unlabeled) sets. We ensure that all Because C has strictly positive eigenvalues, the in-four classes are represented in each labeled set. The verse is well defined. All our semi-supervised learn-same random splits are used for all methods, allowing experiments use (7) in what follows.  ing paired t-tests for statistical significance. All re-Before moving on to experiments, we note an  ported results are average test set accuracy.  interesting connection to the supervised learning We compare our graph-based semi-supervised  method in (Pang and Lee, 2005), which formulates  method with two previously studied methods:  rerating inference as a metric labeling problem (Klein-gression and metric labeling as in (Pang and Lee, berg and Tardos, 2002). Consider a special case of 2005).  our loss function (1) when b = 0 and M . It  is easy to show for labeled nodes j L, the optimal value is the given label: f (xj) = yj. Then the We ran linear -insensitive support vector regression optimization problem decouples into a set of one-using Joachims' SVMlight package (1999) with all  dimensional problems, one for each unlabeled node default parameters. The continuous prediction on a i U : Lb=0,M (f (xi)) =  test document is discretized for classification. Regression results are reported under the heading reg.'  Note this method does not use unlabeled data for  The above problem is easy to solve. It corresponds 4.2  Metric labeling  exactly to the supervised, non-transductive version We ran Pang and Lee's method based on metric la-of metric labeling, except we use squared  differbeling, using SVM regression as the initial label ence while (Pang and Lee, 2005) used absolute dif-preference function. The method requires an  itemference. Indeed in experiments comparing the two  similarity function, which is equivalent to our  simi(not reported here), their differences are not statis-larity measure wij. Among others, we experimented tically significant. From this perspective, our semi-with PSP-based similarity.  For consistency with  supervised learning method is an extension with  in(Pang and Lee, 2005), supervised metric labeling re-teracting terms among unlabeled data.  sults with this measure are reported under reg+PSP.'  Note this method does not use unlabeled data for  training either.  We performed experiments using the movie  rePSPi is defined in (Pang and Lee, 2005) as the  view documents and accompanying 4-class (C =  percentage of positive sentences in review xi. The  {0, 1, 2, 3}) labels found in the scale dataset v1.0  similarity between reviews xi, xj is the cosine angle 48  Positive sentence percentage (PSP) statistics sizes in experiments involving PSP. Because c is  Author (a)  fixed, k varies directly with |L| (i.e., when less Author (b)  Author (c)  labeled data is available, our algorithm considers Author (d)  fewer nearby labeled examples). In an attempt to  reproduce the findings in (Pang and Lee, 2005),  we tuned c, with cross validation. Tuning ranges 0.4  are c {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and  The optimal parameters we found are c = 0.2 and  = 1.5. (In section 4.4, we discuss an alternative mean and standard deviation of PSP  similarity measure, for which we re-tuned these  Note that we learned a single set of shared param-fine grain rating  eters for all authors, whereas (Pang and Lee, 2005) Figure 2: PSP for reviews expressing each fine-grain tuned k and on a per-author basis. To demonstrate rating. We identified positive sentences using SVM  that our implementation of metric labeling produces instead of Na ve Bayes, but the trend is qualitatively comparable results, we also determined the optimal the same as in (Pang and Lee, 2005).  author-specific parameters. Table 1 shows the  accuracy obtained over 20 trials with |L| = 0.9n for each author, using SVM regression, reg+PSP using  between the vectors (PSPi, 1 PSPi) and (PSPj, 1  shared c, parameters, and reg+PSP using  authorPSPj). Positive sentences are identified using a bi-specific c, parameters (listed in parentheses). The nary classifier trained on a separate snippet data best result in each row of the table is highlighted in set located at the same URL as above. The snippet bold. We also show in bold any results that cannot data set contains 10662 short quotations taken from be distinguished from the best result using a paired movie reviews appearing on the rottentomatoes.com t-test at the 0.05 level.  Web site. Each snippet is labeled positive or  neg(Pang and Lee, 2005) found that their metric  laative based on the rating of the originating review.  beling method, when applied to the 4-class data we Pang and Lee (2005) trained a Na ve Bayes classi-are using, was not statistically better than regres-fier. They showed that PSP is a (noisy) measure for sion, though they observed some improvement for  comparing reviews reviews with low ratings tend  authors (c) and (d). Using author-specific parame-to receive low PSP scores, and those with higher  ters, we obtained the same qualitative result, but the ratings tend to get high PSP scores. Thus, two re-improvement for (c) and (d) appears even less sig-views with a high PSP-based similarity are expected nificant in our results. Possible explanations for this to have similar ratings. For our experiments we de-difference are the fact that we derived our PSP mea-rived PSP measurements in a similar manner, but us-surements using an SVM classifier instead of an NB  ing a linear SVM classifier. We observed the same classifier, and that we did not use the same range of relationship between PSP and ratings (Figure 2).  parameters for tuning. The optimal shared  parameThe metric labeling method has parameters  ters produced almost the same results as the optimal (the equivalent of k, in our model). Pang and  author-specific parameters, and were used in subse-Lee tuned them on a per-author basis using cross  validation but did not report the optimal parameters.  We were interested in learning a single set of  Semi-Supervised Learning  parameters for use with all authors. In addition, We used the same PSP-based similarity measure  since we varied labeled set size, it is convenient and the same shared parameters c = 0.2, =  to tune c = k/|L|, the fraction of labeled reviews 1.5 from our metric labeling experiments to per-used as neighbors, instead of k.  We then used  form graph-based semi-supervised learning.  The  the same c, for all authors at all labeled set  results are reported as SSL+PSP.' SSL has three  that we used the mutual-information weighted word Author  (shared)  vector similarity instead of PSP whenever a  similarity measure was required. We repeated the  tuning procedures described in the previous sections.  Using this new similarity measure led to the  optimal parameters c = 0.1, = 1.5, k0 = 5, and  = 10.0. The results are reported under reg+WV'  Table 1: Accuracy using shared (c = 0.2, = 1.5) and SSL+WV,' respectively.  vs. author-specific parameters, with |L| = 0.9n.  additional parameters k0, , and M .  We tested the five algorithms for all four authors us-we tuned k0, with cross validation.  ing each of the nine labeled set sizes. The results ranges are k0  {2, 3, 5, 10, 20} and  are presented in table 2. Each entry in the table  rep{0.001, 0.01, 0.1, 1.0, 10.0}. The optimal parameresents the average accuracy across 20 trials for an ters are k0 = 5 and = 1.0. These were used for all author, a labeled set size, and an algorithm. The best authors and for all labeled set sizes. Note that unlike result in each row is highlighted in bold. Any results k = c|L|, which decreases as the labeled set size de-on the same row that cannot be distinguished from creases, we let k0 remain fixed for all |L|. We set M  the best result using a paired t-test at the 0.05 level arbitrarily to a large number 108 to ensure that the are also bold.  ratings of labeled reviews are respected.  The results indicate that the graph-based  semisupervised learning algorithm based on PSP  simiAlternate Similarity Measures  larity (SSL+PSP) achieved better performance than In addition to using PSP as a similarity measure be-all other methods in all four author corpora when tween reviews, we investigated several alternative only 200, 100, 50, 25, or 12 labeled documents  similarity measures based on the cosine of word  were available. In 19 out of these 20 learning sce-vectors. Among these options were the cosine  benarios, the unlabeled set accuracy by the SSL+PSP  tween the word vectors used to train the SVM  realgorithm was significantly higher than all other gressor, and the cosine between word vectors con-methods. While accuracy generally degraded as we  taining only words with high (top 1000 or top 5000) trained on less labeled data, the decrease for the SSL  mutual information values. The mutual information approach was less severe through the mid-range la-is computed with respect to the positive and negative beled set sizes. SSL+PSP remains among the best  classes in the 10662-document snippet data set.  methods with only 6 labeled examples.  Finally, we experimented with using as a similarity Note that the SSL algorithm appears to be quite  measure the cosine between word vectors containing sensitive to the similarity measure used to form the all words, each weighted by its mutual information.  graph on which it is based. In the experiments where We found this measure to be the best among the op-we used mutual-information weighted word vector  tions tested in pilot trial runs using the metric label-similarity (reg+WV and SSL+WV), we notice that  ing algorithm. Specifically, we scaled the mutual in-reg+WV remained on par with reg+PSP at high  laformation values such that the maximum value was  beled set sizes, whereas SSL+WV appears  signifone. Then, we used these values as weights for the icantly worse in most of these cases.  It is clear  corresponding words in the word vectors. For words that PSP is the more reliable similarity measure.  in the movie review data set that did not appear in SSL uses the similarity measure in more ways than the snippet data set, we used a default weight of zero the metric labeling approaches (i.e., SSL's graph is (i.e., we excluded them. We experimented with set-denser), so it is not surprising that SSL's accuracy ting the default weight to one, but found this led to would suffer more with an inferior similarity mea-inferior performance.)  We repeated the experiments described in  secUnfortunately, our SSL approach did not do as  tions 4.2 and 4.3 with the only difference being  well with large labeled set sizes. We believe this 50  word vector  Author  Author  Author  Author  Table 2: 20-trial average unlabeled set accuracy for each author across different labeled set sizes and methods. In each row, we list in bold the best result and any results that cannot be distinguished from it with a paired t-test at the 0.05 level.  is due to two factors: a) the baseline SVM regres-Olivier Delalleau, Yoshua Bengio, and Nicolas Le Roux.  sor trained on a large labeled set can achieve fairly 2005. Efficient non-parametric function induction in semi-supervised learning. In Proceedings of the Tenth high accuracy for this difficult task without consid-International Workshop on Artificial Intelligence and ering pairwise relationships between examples; b) Statistics (AISTAT 2005).  PSP similarity is not accurate enough. Gain in vari-Minqing Hu and Bing Liu. 2004. Mining and  summaance reduction achieved by the SSL graph is offset rizing customer reviews. In Proceedings of KDD '04, by its bias when labeled data is abundant.  the ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168 177. ACM  Press.  We have demonstrated the benefit of using  unlaT. Joachims. 1999. Making large-scale svm learning beled data for rating inference. There are several practical. In B. Sch lkopf, C. Burges, and A. Smola, directions to improve the work: 1. We will inves-editors, Advances in Kernel Methods Support Vector Learning. MIT Press.  tigate better document representations and similarity measures based on parsing and other  linguisT. Joachims. 2003. Transductive learning via spectral tic knowledge, as well as reviews' sentiment pat-graph partitioning. In Proceedings of ICML-03, 20th International Conference on Machine Learning.  terns. For example, several positive sentences followed by a few concluding negative sentences could Jon M. Kleinberg and va Tardos. 2002. Approxima-indicate an overall negative review, as observed in tion algorithms for classification problems with pairwise relationships: metric labeling and markov ran-prior work (Pang and Lee, 2005). 2. Our method  is transductive: new reviews must be added to the graph before they can be classified. We will extend Bo Pang and Lillian Lee. 2005. Seeing stars: exploiting class relationships for sentiment categorization with it to the inductive learning setting based on (Sind-respect to rating scales. In Proceedings of the ACL.  hwani et al., 2005). 3. We plan to experiment with cross-reviewer and cross-domain analysis, such as Matthias Seeger. 2001. Learning with labeled and unlabeled data. Technical report, University of Edinburgh.  using a model learned on movie reviews to help clas-sify product reviews.  James Shanahan, Yan Qu, and Janyce Wiebe, editors.  2005. Computing attitude and affect in text. Springer, Acknowledgment  Dordrecht, The Netherlands.  We thank Bo Pang, Lillian Lee and anonymous  re2005. Beyond the point cloud: from transductive to viewers for helpful comments.  semi-supervised learning. In ICML05, 22nd International Conference on Machine Learning, Bonn, Ger-many.  A. J. Smola and B. Sch lkopf.  A tutorial on  support vector regression. Statistics and Computing, 2005. On manifold regularization. In Proceedings of 14:199 222.  the Tenth International Workshop on Artificial Intelligence and Statistics (AISTAT 2005).  Peter Turney. 2002. Thumbs up or thumbs down?  Semantic orientation applied to unsupervised classifica-A. Blum and S. Chawla. 2001. Learning from labeled tion of reviews. In Proceedings of ACL-02, 40th An-and unlabeled data using graph mincuts. In Proc. 18th nual Meeting of the Association for Computational International Conf. on Machine Learning.  Linguistics, pages 417 424.  view mining: a comparison between supervised and  2003. Semi-supervised learning using Gaussian fields unsupervised classification approaches.  In HICSS.  and harmonic functions. In ICML-03, 20th Interna-IEEE Computer Society.  tional Conference on Machine Learning.  Kushal Dave, Steve Lawrence, and David M. Pennock.  2003. Mining the peanut gallery: opinion extraction erature survey.  Technical Report 1530,  Comand semantic classification of product reviews.  puter Sciences, University of Wisconsin-Madison.  WWW '03: Proceedings of the 12th international con-http://www.cs.wisc.edu/ jerryzhu/pub/ssl survey.pdf.  ference on World Wide Web, pages 519 528. 