 Improving Word Representations via Global Context and Multiple Word Prototypes  Eric H. Huang, Richard Socher , Christopher D. Manning, Andrew Y. Ng Computer Science Department, Stanford University, Stanford, CA 94305, USA  {ehhuang,manning,ang}@stanford.edu, richard@socher.org Abstract  Despite their usefulness, most VSMs share a  common problem that each word is only  repreUnsupervised word representations are very  sented with one vector, which clearly fails to capture useful in NLP tasks both as inputs to learning  homonymy and polysemy. Reisinger and Mooney  algorithms and as extra word features in NLP  (2010b) introduced a multi-prototype VSM where  systems. However, most of these models are  word sense discrimination is first applied by clus-built with only local context and one  representering contexts, and then prototypes are built using tation per word. This is problematic because  words are often polysemous and global  conthe contexts of the sense-labeled words. However, in text can also provide useful information for  order to cluster accurately, it is important to capture learning word meanings. We present a new  both the syntax and semantics of words. While many neural network architecture which 1) learns  approaches use local contexts to disambiguate word word embeddings that better capture the se-meaning, global contexts can also provide useful  mantics of words by incorporating both local  topical information (Ng and Zelle, 1997). Several and global document context, and 2) accounts  studies in psychology have also shown that global for homonymy and polysemy by learning multiple embeddings per word. We introduce a  new dataset with human judgments on pairs of  words in sentential context, and evaluate our  We introduce a new neural-network-based  lanmodel on it, showing that our model  outperguage model that distinguishes and uses both local forms competitive baselines and other neural  and global context via a joint training objective. The language models. 1  model learns word representations that better capture the semantics of words, while still keeping syn-1  Introduction  tactic information. These improved representations can be used to represent contexts for clustering word Vector-space models (VSM) represent word mean-instances, which is used in the multi-prototype ver-ings with vectors that capture semantic and syntac-sion of our model that accounts for words with mul-tic information of words. These representations can tiple senses.  be used to induce similarity measures by computing We evaluate our new model on the standard  distances between the vectors, leading to many use-WordSim-353 (Finkelstein et al., 2001) dataset that ful applications, such as information retrieval (Man-includes human similarity judgments on pairs of  ning et al., 2008), document classification (Sebas-words, showing that combining both local and  tiani, 2002) and question answering (Tellex et al., global context outperforms using only local or  global context alone, and is competitive with state-1The dataset and word vectors can be downloaded at of-the-art methods. However, one limitation of this http://ai.stanford.edu/ ehhuang/.  evaluation is that the human judgments are on pairs  weighted average  he  to  the  Figure 1: An overview of our neural language model. The model makes use of both local and global context to compute a score that should be large for the actual next word (bank in the example), compared to the score for other words.  When word meaning is still ambiguous given local context, information in global context can help disambiguation.  of words presented in isolation, ignoring meaning global context.  variations in context. Since word interpretation in Given a word sequence s and document d in  context is important especially for homonymous and which the sequence occurs, our goal is to discrim-polysemous words, we introduce a new dataset with inate the correct last word in s from other random human judgments on similarity between pairs of  We compute scores g(s, d) and g(sw, d)  words in sentential context. To capture interesting where sw is s with the last word replaced by word w, word pairs, we sample different senses of words us-and g( , ) is the scoring function that represents the ing WordNet (Miller, 1995). The dataset includes  neural networks used. We want g(s, d) to be larger verbs and adjectives, in addition to nouns. We show than g(sw, d) by a margin of 1, for any other word that our multi-prototype model improves upon the  w in the vocabulary, which corresponds to the train-single-prototype version and outperforms other neu-ing objective of minimizing the ranking loss for each ral language models and baselines on this dataset.  (s, d) found in the corpus:  Global Context-Aware Neural Language  Collobert and Weston (2008) showed that this rank-In this section, we describe the training objective of ing approach can produce good word embeddings  our model, followed by a description of the neural that are useful in several NLP tasks, and allows  network architecture, ending with a brief description much faster training of the model compared to op-of our model's training method.  timizing log-likelihood of the next word.  Training Objective  Neural Network Architecture  Our model jointly learns word representations while We define two scoring components that contribute  learning to discriminate the next word given a short to the final score of a (word sequence, document) word sequence (local context) and the document  pair. The scoring components are computed by two  (global context) in which the word sequence occurs.  neural networks, one capturing local context and the Because our goal is to learn useful word representa-other global context, as shown in Figure 1. We now tions and not the probability of the next word given describe how each scoring component is computed.  previous words (which prohibits looking ahead), our The score of local context uses the local word se-model can utilize the entire document to provide  quence s. We first represent the word sequence s as  an ordered list of vectors x = (x1, x2, ..., xm) where The final score is the sum of the two scores:  xi is the embedding of word i in the sequence, which is a column in the embedding matrix L  where |V | denotes the size of the vocabulary. The columns of this embedding matrix L are the word  The local score preserves word order and syntactic vectors and will be learned and updated during train-information, while the global score uses a weighted ing. To compute the score of local context, score average which is similar to bag-of-words features, l,  we use a neural network with one hidden layer:  capturing more of the semantics and topics of the document. Note that Collobert and Weston (2008)'s a  language model corresponds to the network using  only local context.  Learning  where [x1; x2; ...; xm] is the concatenation of the m word embeddings representing sequence s, f is  Following Collobert and Weston (2008), we sample  an element-wise activation function such as tanh, the gradient of the objective by randomly choosing a  a word from the dictionary as a corrupt example for 1 R  is the activation of the hidden layer with  each sequence-document pair, (s, d), and take the 1 R  and W2 R  are respectively the first and second layer weights of derivative of the ranking loss with respect to the pa-the neural network, and b  rameters: weights of the neural network and the em-1, b2 are the biases of each  bedding matrix L. These weights are updated via  For the score of the global context, we represent backpropagation. The embedding matrix L is the  the document also as an ordered list of word  emword representations. We found that word  embeddings move to good positions in the vector space  1, d2, ..., dk). We first compute the  weighted average of all word vectors in the  docufaster when using mini-batch L-BFGS (Liu and  Nocedal, 1989) with 1000 pairs of good and corrupt examples per batch for training, compared to stochas-Pk  Multi-Prototype Neural Language  where w( ) can be any weighting function that cap-Model  tures the importance of word ti in the document. We Despite distributional similarity models' successful use idf-weighting as the weighting function.  applications in various NLP tasks, one major limi-We use a two-layer neural network to compute the  tation common to most of these models is that they global context score, scoreg, similar to the above: assume only one representation for each word. This (g)  single-prototype representation is problematic be-a (g)  f (W  cause many words have multiple meanings, which  can be wildly different.  Using one  representation simply cannot capture the different meanings.  where [c; xm] is the concatenation of the weighted Moreover, using all contexts of a homonymous or  average document vector and the vector of the last polysemous word to build a single prototype could word in s, a (g)  is the activation of  hurt the representation, which cannot represent any (g)  the hidden layer with h(g) hidden nodes, W  one of the meanings well as it is influenced by all h(g) (2n)  and W  are respectively the  meanings of the word.  first and second layer weights of the neural network, Instead of using only one representation per word, (g)  and b  are the biases of each layer. Note that  Reisinger and Mooney (2010b) proposed the  multiinstead of using the document where the sequence  prototype approach for vector-space models, which occurs, we can also specify a fixed k > m that cap-uses multiple representations to capture different tures larger context.  senses and usages of a word. We show how our  model can readily adopt the multi-prototype ap-important, we introduce a new dataset with human  We present a way to use our learned  judgments on similarity of pairs of words in senten-single-prototype embeddings to represent each contial context. Finally, we show that our model outper-text window, which can then be used by clustering to forms other methods on this dataset and also that the perform word sense discrimination (Sch tze, 1998).  multi-prototype approach improves over the single-In order to learn multiple prototypes, we first  prototype approach.  gather the fixed-sized context windows of all occur-We chose Wikipedia as the corpus to train all  rences of a word (we use 5 words before and after models because of its wide range of topics and  the word occurrence). Each context is represented word usages, and its clean organization of docu-by a weighted average of the context words' vectors, ment by topic. We used the April 2010 snapshot of where again, we use idf-weighting as the weighting the Wikipedia corpus (Shaoul and Westbury, 2010), function, similar to the document context represen-with a total of about 2 million articles and 990 mil-tation described in Section 2.2. We then use spheri-lion tokens. We use a dictionary of the 30,000 most cal k-means to cluster these context representations, frequent words in Wikipedia, converted to lower  which has been shown to model semantic relations  case. In preprocessing, we keep the frequent  numwell (Dhillon and Modha, 2001). Finally, each word bers intact and replace each digit of the uncommon occurrence in the corpus is re-labeled to its associ-numbers to DG so as to preserve information such ated cluster and is used to train the word representa-as it being a year (e.g. DGDGDGDG ). The  contion for that cluster.  verted numbers that are rare are mapped to a  NUMSimilarity between a pair of words (w, w0)  usBER token. Other rare words not in the dictionary ing the multi-prototype approach can be computed  are mapped to an UNKNOWN token.  with or without context, as defined by Reisinger and For all experiments,  Mooney (2010b):  of text as the local context, 100 hidden units, and no weight regularization for both neural networks. For AvgSimC(w, w0) =  multi-prototype variants, we fix the number of pro-k  totypes to be 10.  In order to show that our model learns more semantic word representations with global context, we give where p(c, w, i) is the likelihood that word w is in the nearest neighbors of our single-prototype model its cluster i given context c, i(w) is the vector rep-versus C&W's, which only uses local context. The resenting the i-th cluster centroid of w, and d(v, v0) nearest neighbors of a word are computed by com-is a function computing similarity between two vec-paring the cosine similarity between the center word tors, which can be any of the distance functions pre-and all other words in the dictionary. Table 1 shows sented by Curran (2004). The similarity measure can the nearest neighbors of some words. The nearest  be computed in absence of context by assuming uni-neighbors of market that C&W's embeddings give form p(c, w, i) over i.  are more constrained by the syntactic constraint that words in plural form are only close to other words 4  in plural form, whereas our model captures that the In this section, we first present a qualitative analysis singular and plural forms of a word are similar in comparing the nearest neighbors of our model's em-meaning. Other examples show that our model  inbeddings with those of others, showing our  embedduces nearest neighbors that better capture semandings better capture the semantics of words, with the tics.  use of global context. Our model also improves the Table 2 shows the nearest neighbors of our model  correlation with human judgments on a word  simiusing the multi-prototype approach.  We see that  larity task. Because word interpretation in context is the clustering is able to group contexts of different  Corpus  Word  stores  American  Australian,  African  alleged, overseas,  harmful,  prohibited, convicted  Table 1: Nearest neighbors of words based on cosine similarity. Our model is less constrained by syntax and is Pruned tf-idf  Center Word  Nearest Neighbors  corporation, insurance, company  Table 3: Spearman's correlation on WordSim-353, bank 2  shore, coast, direction  showing our model's improvement over previous neural star 1  models for learning word embeddings. C&W* is the star 2  word embeddings trained and provided by C&W. Our Model* is trained without stop words, while Our Model-cell 1  telephone, smart, phone  g uses only global context. Pruned tf-idf (Reisinger and cell 2  pathology, molecular, physiology  Mooney, 2010b) and ESA (Gabrilovich and Markovitch, left 1  2007) are also included.  top, round, right  Table 2: Nearest neighbors of word embeddings learned our re-implementation of C&W's model trained on by our model using the multi-prototype approach based Wikipedia, showing the large effect of using a dif-on cosine similarity. The clustering is able to find the different corpus.  ferent meanings, usages, and parts of speech of the words.  Our model is able to learn more semantic word  embeddings and noticeably improves upon C&W's meanings of a word into separate groups, allowing model. Note that our model achieves higher corre-our model to learn multiple meaningful representalation (64.2) than either using local context alone tions of a word.  (C&W: 55.3) or using global context alone (Our Model-g: 22.8). We also found that correlation can 4.2  be further improved by removing stop words (71.3).  A standard dataset for evaluating vector-space mod-Thus, each window of text (training example)  conels is the WordSim-353 dataset (Finkelstein et al., tains more information but still preserves some syn-2001), which consists of 353 pairs of nouns. Each tactic information as the words are still ordered in pair is presented without context and associated with the local context.  13 to 16 human judgments on similarity and  reNew Dataset: Word Similarity in Context  latedness on a scale from 0 to 10. For example,  (cup,drink) received an average score of 7.25, while The many previous datasets that associate human  (cup,substance) received an average score of 1.92.  judgments on similarity between pairs of words,  Table 3 shows our results compared to previous  such as WordSim-353, MC (Miller and Charles,  methods, including C&W's language model and the 1991) and RG (Rubenstein and Goodenough, 1965),  hierarchical log-bilinear (HLBL) model (Mnih and  have helped to advance the development of  vectorHinton, 2008), which is a probabilistic, linear neu-space models. However, common to all datasets is  ral model. We downloaded these embeddings from  that similarity scores are given to pairs of words in Turian et al. (2010). These embeddings were trained isolation. This is problematic because the mean-on the smaller corpus RCV1 that contains one year ings of homonymous and polysemous words depend  of Reuters English newswire, and show similar cor-highly on the words' contexts. For example, in the relations on the dataset.  We report the result of  two phrases, he swings the baseball bat and the  Word 1  Word 2  Located downtown along the east bank of the Des  This is the basis of all money laundering , a track record Moines River ...  of depositing clean money before slipping through dirty money ...  Inside the ruins , there are bats and a bowl with Pokeys An aggressive lower order batsman who usually bats at that fills with sand over the course of the race , and the No. 11 , Muralitharan is known for his tendency to back music changes somewhat while inside ...  An example of legacy left in the Mideast from these  ... one should not adhere to a particular explanation , nobles is the Krak des Chevaliers ' enlargement by the only in such measure as to be ready to abandon it if it Counts of Tripoli and Toulouse ...  be proved with certainty to be false ...  ... and Andy 's getting ready to pack his bags and head  ... she encounters Ben ( Duane Jones ) , who arrives up to Los Angeles tomorrow to get ready to fly back in a pickup truck and defends the house against another home on Thursday  pack of zombies ...  In practice , there is an unknown phase delay between  ... but Gilbert did not believe that she was dedicated the transmitter and receiver that must be compensated enough , and when she missed a rehearsal , she was by synchronization of the receivers local oscillator dismissed ...  Table 4: Example pairs from our new dataset. Note that words in a pair can be the same word and have different parts of speech.  bat flies , bat has completely different meanings. It speech (noun, verb or adjective), from 1 to 3. We is unclear how this variation in meaning is accounted also group words by their number of synsets: [0,5], for in human judgments of words presented without  ple at most 15 words from each combination in the One of the main contributions of this paper is the Cartesian product of the above groupings.  creation of a new dataset that addresses this issue.  In step 2, for each of the words selected in step The dataset has three interesting characteristics: 1) 1, we want to choose the other word so that the pair human judgments are on pairs of words presented in captures an interesting relationship. Similar to Man-sentential context, 2) word pairs and their contexts andhar et al. (2010), we use WordNet to first ran-are chosen to reflect interesting variations in mean-domly select one synset of the first word, we then ings of homonymous and polysemous words, and 3)  construct a set of words in various relations to the verbs and adjectives are present in addition to nouns.  first word's chosen synset, including hypernyms, hy-We now describe our methodology in constructing  ponyms, holonyms, meronyms and attributes. We  the dataset.  randomly select a word from this set of words as the second word in the pair. We try to repeat the above 4.3.1  Dataset Construction  twice to generate two pairs for each word. In addi-Our procedure of constructing the dataset consists tion, for words with more than five synsets, we allow of three steps: 1) select a list a words, 2) for each the second word to be the same as the first, but with word, select another word to form a pair, 3) for each different synsets. We end up with pairs of words as word in a pair, find a sentential context. We now well as the one chosen synset for each word in the describe each step in detail.  In step 1, in order to make sure we select a diverse In step 3, we aim to extract a sentence from  list of words, we consider three attributes of a word: Wikipedia for each word, which contains the word  frequency in a corpus, number of parts of speech, and corresponds to a usage of the chosen synset.  and number of synsets according to WordNet. For  We first find all sentences in which the word  ocfrequency, we divide words into three groups, top curs. We then POS tag2 these sentences and filter out 2,000 most frequent, between 2,000 and 5,000, and those that do not match the chosen POS. To find the between 5,000 to 10,000 based on occurrences in  Wikipedia. For number of parts of speech, we group 2We used the MaxEnt Treebank POS tagger in the python words based on their number of possible parts of  Evaluations on Word Similarity in  For evaluation, we also compute Spearman  correlation between a model's computed similarity scores tf-idf-S  models' results on this dataset. We compare against Pruned tf-idf-M AvgSim  the following baselines: tf-idf represents words in Pruned tf-idf-M AvgSimC  a word-word matrix capturing co-occurrence counts in all 10-word context windows.  Table 5:  Spearman's correlation on our new  Mooney (2010b) found pruning the low-value tf-idf dataset. Our Model-S uses the single-prototype approach, features helps performance. We report the result  while Our Model-M uses the multi-prototype approach.  AvgSim calculates similarity with each prototype con-of this pruning technique after tuning the thresh-tributing equally, while AvgSimC weighs the prototypes old value on this dataset, removing all but the top according to probability of the word belonging to that 200 features in each word vector.  We tried the  prototype's cluster.  same multi-prototype approach and used spherical  k-means3 to cluster the contexts using tf-idf representations, but obtained lower numbers than single-word usages that correspond to the chosen synset, prototype (55.4 with AvgSimC). We then tried using we first construct a set of related words of the chosen pruned tf-idf representations on contexts with our synset, including hypernyms, hyponyms, holonyms,  clustering assignments (included in Table 5), but still meronyms and attributes. Using this set of related got results worse than the single-prototype version words, we filter out a sentence if the document in of the pruned tf-idf model (60.5 with AvgSimC).  which the sentence appears does not include one of This suggests that the pruned tf-idf representations the related words. Finally, we randomly select one might be more susceptible to noise or mistakes in sentence from those that are left.  context clustering.  Table 4 shows some examples from the dataset.  Note that the dataset also includes pairs of the same forms C&W's vectors and the above baselines on word. Single-prototype models would give the max  this dataset.  With multiple representations per  similarity score for those pairs, which can be prob-word, we show that the multi-prototype approach  lematic depending on the words' contexts.  This  can improve over the single-prototype version with-dataset requires models to examine context when de-out using context (62.8 vs. 58.6). Moreover, using termining word meaning.  AvgSimC4 which takes contexts into account, the  Using Amazon Mechanical Turk, we collected 10  multi-prototype model obtains the best performance human similarity ratings for each pair, as Snow et (65.7).  al. (2008) found that 10 non-expert annotators can achieve very close inter-annotator agreement with 5  Related Work  expert raters. To ensure worker quality, we only  allowed workers with over 95% approval rate to  Neural language models (Bengio et al., 2003; Mnih work on our task. Furthermore, we discarded all  and Hinton, 2007; Collobert and Weston, 2008;  ratings by a worker if he/she entered scores out of Schwenk and Gauvain, 2002; Emami et al., 2003)  the accepted range or missed a rating, signaling low-have been shown to be very powerful at language  quality work.  modeling, a task where models are asked to  acWe obtained a total of 2,003 word pairs and their curately predict the next word given previously  sentential contexts. The word pairs consist of 1,712  seen words. By using distributed representations of unique words. Of the 2,003 word pairs, 1328 are  3We first tried movMF as in Reisinger and Mooney (2010b), noun-noun pairs, 399 verb-verb, 140 verb-noun, 97  but were unable to get decent results (only 31.5).  adjective-adjective, 30 noun-adjective, and 9 verb-4probability of being in a cluster is calculated as the inverse adjective. 241 pairs are same-word pairs.  of the distance to the cluster centroid.  words which model words' similarity, this type of pared to another verb chosen to be either similar or models addresses the data sparseness problem that dissimilar to the intransitive verb in context. The n-gram models encounter when large contexts are  context is short, with only one word, and only verbs used. Most of these models used relative local con-are compared. Erk and Pad (2008), Thater et al.  texts of between 2 to 10 words. Schwenk and  Gau(2011) and Dinu and Lapata (2010) evaluated word  vain (2002) tried to incorporate larger context by similarity in context with a modified task where sys-combining partial parses of past word sequences and tems are to rerank gold-standard paraphrase candi-a neural language model. They used up to 3  previdates given the SemEval 2007 Lexical Substitution ous head words and showed increased performance  Task dataset. This task only indirectly evaluates sim-on language modeling. Our model uses a similar  ilarity as only reranking of already similar words are neural network architecture as these models and uses evaluated.  the ranking-loss training objective proposed by Collobert and Weston (2008), but introduces a new way 6  Conclusion  to combine local and global context to train word We presented a new neural network architecture that embeddings.  learns more semantic word representations by  using both local and global context in learning. These duced by neural language models have been use-learned word embeddings can be used to represent  ful in chunking, NER (Turian et al., 2010), parsing word contexts as low-dimensional weighted average (Socher et al., 2011b), sentiment analysis (Socher et vectors, which are then clustered to form different al., 2011c) and paraphrase detection (Socher et al., meaning groups and used to learn multi-prototype  2011a). However, they have not been directly eval-vectors. We introduced a new dataset with human  uated on word similarity tasks, which are important judgments on similarity between pairs of words in for tasks such as information retrieval and summa-context, so as to evaluate model's abilities to capture rization. Our experiments show that our word em-homonymy and polysemy of words in context. Our  beddings are competitive in word similarity tasks.  Most of the previous vector-space models use a  forms previous neural models and competitive base-single vector to represent a word even though many lines on this new dataset.  words have multiple meanings. The multi-prototype approach has been widely studied in models of cat-Acknowledgments  egorization in psychology (Rosseel, 2002; Griffiths et al., 2009), while Sch tze (1998) used clustering The authors gratefully acknowledges the support of of contexts to perform word sense discrimination.  the Defense Advanced Research Projects Agency  Reisinger and Mooney (2010b) combined the two  approaches and applied them to vector-space  modForce Research Laboratory (AFRL) prime contract  els, which was further improved in Reisinger and  FA8750-09-C-0181, and the DARPA Deep  Mooney (2010a). Two other recent papers (Dhillon  Learning program under contract number  FA865010-C-7020. Any opinions, findings, and conclusions for constructing word representations that deal with or recommendations expressed in this material are context. It would be interesting to evaluate those those of the authors and do not necessarily reflect models on our new dataset.  the view of DARPA, AFRL, or the US government.  Many datasets with human similarity ratings on  pairs of words, such as WordSim-353 (Finkelstein  et al., 2001), MC (Miller and Charles, 1991) and  Yoshua Bengio, R jean Ducharme, Pascal Vincent,  widely used to evaluate vector-space models. Moti-Christian Jauvin, Jaz K, Thomas Hofmann, Tomaso  vated to evaluate composition models, Mitchell and Poggio, and John Shawe-taylor. 2003. A neural prob-Lapata (2008) introduced a dataset where an intran-abilistic language model. Journal of Machine Learn-sitive verb, presented with a subject noun, is com-ing Research, 3:1137 1155.  Ronan Collobert and Jason Weston. 2008. A unified ar-D. C. Liu and J. Nocedal. 1989. On the limited mem-chitecture for natural language processing: deep neu-ory bfgs method for large scale optimization. Math.  ral networks with multitask learning. In Proceedings Program., 45(3):503 528, December.  of the 25th international conference on Machine learn-Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dligach, ing, ICML '08, pages 160 167, New York, NY, USA.  ACM.  14: Word sense induction & disambiguation. Word James Richard Curran. 2004. From distributional to se-Journal Of The International Linguistic Association, mantic similarity. Technical report.  (July):63 68.  Christopher D. Manning, Prabhakar Raghavan, and Hin-Concept decompositions for large sparse text data us-rich Schtze. 2008. Introduction to Information Reing clustering. Mach. Learn., 42:143 175, January.  trieval. Cambridge University Press, New York, NY, Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.  George A Miller and Walter G Charles. 1991. Contextual cca. In Advances in Neural Information Processing correlates of semantic similarity. Language & Cogni-Systems (NIPS), volume 24.  tive Processes, 6(1):1 28.  Georgiana Dinu and Mirella Lapata. 2010. Measuring distributional similarity in context. In Proceedings of George A. Miller. 1995. Wordnet: A lexical database for the 2010 Conference on Empirical Methods in Natural english. Communications of the ACM, 38:39 41.  Jeff Mitchell and Mirella Lapata. 2008. Vector-based Stroudsburg, PA, USA. Association for Computational models of semantic composition. In In Proceedings of Linguistics.  Ahmad Emami, Peng Xu, and Frederick Jelinek. 2003.  Andriy Mnih and Geoffrey Hinton. 2007. Three new  Using a connectionist model in a syntactical based lan-graphical models for statistical language modelling. In guage model. In Acoustics, Speech, and Signal Pro-Proceedings of the 24th international conference on cessing, pages 372 375.  Machine learning, ICML '07, pages 641 648, New  A structured  York, NY, USA. ACM.  vector space model for word meaning in context. In Andriy Mnih and Geoffrey Hinton. 2008. A scalable Proceedings of the Conference on Empirical Meth-hierarchical distributed language model. In In NIPS.  ods in Natural Language Processing, EMNLP '08,  Ht Ng and J Zelle. 1997. Corpus-based approaches to pages 897 906, Stroudsburg, PA, USA. Association  for Computational Linguistics.  Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Siva Reddy, Ioannis Klapaftis, Diana McCarthy, and Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan  Suresh Manandhar. 2011. Dynamic and static  protoRuppin. 2001. Placing search in context: the  contype vectors for semantic composition. In Proceedings cept revisited. In Proceedings of the 10th international of 5th International Joint Conference on Natural Lan-conference on World Wide Web, WWW '01, pages  guage Processing, pages 705 713, Chiang Mai, Thai-406 414, New York, NY, USA. ACM.  land, November. Asian Federation of Natural  LanEvgeniy Gabrilovich and Shaul Markovitch. 2007. Com-guage Processing.  puting semantic relatedness using wikipedia-based explicit semantic analysis.  In Proceedings of the  Joseph Reisinger and Raymond Mooney. 2010a. A mix-20th international joint conference on Artifical intel-ture model with sharing for lexical semantics. In Pro-ligence, IJCAI'07, pages 1606 1611, San Francisco, ceedings of the 2010 Conference on Empirical Meth-CA, USA. Morgan Kaufmann Publishers Inc.  ods in Natural Language Processing, EMNLP '10,  Thomas L Griffiths, Kevin R Canini, Adam N Sanborn, pages 1173 1182, Stroudsburg, PA, USA. Association and Daniel J Navarro. 2009. Unifying rational models for Computational Linguistics.  of categorization via the hierarchical dirichlet process.  Joseph Reisinger and Raymond J. Mooney.  Multi-prototype vector-space models of word  meanDavid J Hess, Donald J Foss, and Patrick Carroll. 1995.  ing. In Human Language Technologies: The 2010  AnEffects of global and local context on lexical process-nual Conference of the North American Chapter of the ing during language comprehension. Journal of Ex-Association for Computational Linguistics, HLT '10, perimental Psychology: General, 124(1):62 82.  pages 109 117, Stroudsburg, PA, USA. Association  Ping Li, Curt Burgess, and Kevin Lund. 2000. The ac-for Computational Linguistics.  quisition of word meaning through global lexical co-Yves Rosseel. 2002. Mixture models of categorization.  Journal of Mathematical Psychology, 46:178 210.  for semi-supervised learning. In Proceedings of the Contextual correlates of synonymy. Commun. ACM,  48th Annual Meeting of the Association for Computa-8:627 633, October.  tional Linguistics, ACL '10, pages 384 394, Strouds-Hinrich Sch tze. 1998. Automatic word sense discrimi-burg, PA, USA. Association for Computational  Linnation. Journal of Computational Linguistics, 24:97  Holger Schwenk and Jean-luc Gauvain. 2002. Connectionist language modeling for large vocabulary con-tinuous speech recognition. In In International Conference on Acoustics, Speech and Signal Processing, pages 765 768.  Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Comput. Surv., 34:1  Cyrus Shaoul and Chris Westbury. 2010. The westbury lab wikipedia corpus.  Rion Snow, Brendan O'Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP  '08, pages 254 263, Stroudsburg, PA, USA.  Association for Computational Linguistics.  Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011a. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems 24.  Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. 2011b. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 26th International Conference on Machine Learning (ICML).  Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning.  Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the 2011  Conference on Empirical Methods in Natural  Language Processing (EMNLP).  Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernan-des, and Gregory Marton. 2003. Quantitative evaluation of passage retrieval algorithms for question answering. In Proceedings of the 26th Annual International ACM SIGIR Conference on Search and  Development in Information Retrieval, pages 41 47. ACM  Press.  2011. Word meaning in context: a simple and effective vector model. In Proceedings of the 5th International Joint Conference on Natural Language Processing, IJCNLP '11.  Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.  Word representations: a simple and general method 