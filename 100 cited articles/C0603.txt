 Mining WordNet for Fuzzy Sentiment:  Sentiment Tag Extraction from WordNet Glosses  Concordia University  {andreev, bergler}@encs.concordia.ca  representation, which regard all members of a  category as equal: no element is more of a  memMany of the tasks required for semantic  ber than any other (Edmonds, 1999). In this  patagging of phrases and texts rely on a list  per, we challenge the applicability of this  assumpof words annotated with some semantic  tion to the semantic category of sentiment, which features.  We present a method for  exconsists of positive, negative and neutral  subcatetracting sentiment-bearing adjectives from  gories, and present a dictionary-based Sentiment  WordNet using the Sentiment Tag  ExtracTag Extraction Program (STEP) that we use to  tion Program (STEP). We did 58 STEP  generate a fuzzy set of English sentiment-bearing runs on unique non-intersecting seed lists  words for the use in sentiment tagging systems 1.  drawn from manually annotated list of  The proposed approach based on the fuzzy logic  positive and negative adjectives and  evalu(Zadeh, 1987) is used here to assign fuzzy  senated the results against other manually  antiment tags to all words in WordNet (Fellbaum,  notated lists. The 58 runs were then  col1998), that is it assigns sentiment tags and a degree lapsed into a single set of 7, 813 unique  of centrality of the annotated words to the  sentiFor each word we computed a  ment category. This assignment is based on  WordNet Overlap Score by subtracting the total  Net glosses. The implications of this approach for  number of runs assigning this word a  negNLP and linguistic research are discussed.  ative sentiment from the total of the runs  that consider it positive. We demonstrate  The Category of Sentiment as a Fuzzy  that Net Overlap Score can be used as a  measure of the words degree of  memberSome semantic categories have clear membership  ship in the fuzzy category of sentiment:  (e.g., lexical fields (Lehrer, 1974) of color, body the core adjectives, which had the high-parts or professions), while others are much more  difficult to define. This prompted the development  most accurately both by STEP and by  huof approaches that regard the transition from  memman annotators, while the words on the  bership to non-membership in a semantic category  periphery of the category had the lowest  as gradual rather than abrupt (Zadeh, 1987; Rosch,  scores and were associated with low rates  1978). In this paper we approach the category of  of inter-annotator agreement.  sentiment as one of such fuzzy categories where 1  Introduction  some words such as good, bad are very central, prototypical members, while other, less  cenMany of the tasks required for effective  semantral words may be interpreted differently by differ-tic tagging of phrases and texts rely on a list of  ent people. Thus, as annotators proceed from the  words annotated with some lexical semantic  feacore of the category to its periphery, word  memtures. Traditional approaches to the development  1Sentiment tagging is defined here as assigning positive, of such lists are based on the implicit assumption  negative and neutral labels to words according to the senti-of classical truth-conditional theories of meaning  ment they express.  bership in this category becomes more ambiguous,  in importance of various sentiment markers have  and hence, lower inter-annotator agreement can be  crystallized in two main approaches: automatic  expected for more peripheral words. Under the  assignment of weights based on some statistical  classical truth-conditional approach the  disagreecriterion ((Hatzivassiloglou and McKeown, 1997;  ment between annotators is invariably viewed as a  Turney and Littman, 2002; Kim and Hovy, 2004),  sign of poor reliability of coding and is eliminated and others) or manual annotation (Subasic and  by training' annotators to code difficult and  amHuettner, 2001). The statistical approaches  usubiguous cases in some standard way. While this  procedure leads to high levels of inter-annotator  nitude of pointwise mutual information in (Turney  agreement on a list created by a coordinated team  and Littman, 2002), goodness-for-fit measure in  of researchers, the naturally occurring differences (Hatzivassiloglou and McKeown, 1997), probabil-in the interpretation of words located on the  peity of word's sentiment given the sentiment if its  riphery of the category can clearly be seen when  synonyms in (Kim and Hovy, 2004), etc.) to  deannotations by two independent teams are  comfine the strength of the sentiment expressed by a  pared. The Table 1 presents the comparison of  GIword or to establish a threshold for the  membership in the crisp sets 3 of positive, negative and  al., 1966)) 2 and HM (from (Hatzivassiloglou and  neutral words. Both approaches have their  limiMcKeown, 1997) study) lists of words manually  tations: the first approach produces coarse results annotated with sentiment tags by two different re-and requires large amounts of data to be reliable,  search teams.  while the second approach is prohibitively  expensive in terms of annotator time and runs the risk of GI-H4  introducing a substantial subjective bias in  annotations.  In this paper we seek to develop an approach  for semantic annotation of a fuzzy lexical  category and apply it to sentiment annotation of all  Tags assigned  WordNet words. The sections that follow (1)  describe the proposed approach used to extract  sentiment information from WordNet entries using  Adj. with  STEP (Semantic Tag Extraction Program)  algorithm, (2) discuss the overall performance of STEP  Intersection  on WordNet glosses, (3) outline the method for  (% intersection)  of GI-H4 adj)  of HM)  defining centrality of a word to the sentiment cate-Agreement on tags  gory, and (4) compare the results of both automatic (STEP) and manual (HM) sentiment annotations  to the manually-annotated GI-H4 list, which was  notations on sentiment tags.  used as a gold standard in this experiment. The  The approach to sentiment as a category with comparisons are performed separately for each of  fuzzy boundaries suggests that the 21.3%  disthe subsets of GI-H4 that are characterized by a  agreement between the two manually annotated  different distance from the core of the lexical cat-lists reflects a natural variability in human  anegory of sentiment.  notators' judgment and that this variability is  reSentiment Tag Extraction from  lated to the degree of centrality and/or relative im-WordNet Entries  portance of certain words to the category of  sentiment. The attempts to address this difference  Word lists for sentiment tagging applications can  2The General Inquirer (GI) list used in this study was be compiled using different methods. Automatic  manually cleaned to remove duplicate entries for words with methods of sentiment annotation at the word level  same part of speech and sentiment. Only the Harvard IV-4  can be grouped into two major categories: (1)  list component of the whole GI was used in this study, since other lists included in GI lack the sentiment annotation. Un-corpus-based approaches and (2) dictionary-based  less otherwise specified, we used the full GI-H4 list including the Neutral words that were not assigned Positiv or Negativ 3We use the term crisp set to refer to traditional, non-annotations.  approaches.  The first group includes methods  ambiguity of some words acquired in pass 1 and  that rely on syntactic or co-occurrence patterns  from the seed list. At this step, we also filter out of words in large texts to determine their senti-all those words that have been assigned  contradictment (e.g., (Turney and Littman, 2002;  Hatzivasing, positive and negative, sentiment values within siloglou and McKeown, 1997; Yu and Hatzivas-the same run.  The performance of STEP was evaluated using  ers). The majority of dictionary-based approaches  GI-H4 as a gold standard, while the HM list was  use WordNet information, especially, synsets and  used as a source of seed words fed into the  syshierarchies, to acquire sentiment-marked words  tem. We evaluated the performance of our  system against the complete list of 1904 adjectives in and Hovy, 2004) or to measure the similarity  GI-H4 that included not only the words that were  between candidate words and sentiment-bearing  marked as Positiv, Negativ, but also those that were words such as good and bad (Kamps et al., 2004).  not considered sentiment-laden by GI-H4  annotaIn this paper, we propose an approach to  sentitors, and hence were by default considered neutral  ment annotation of WordNet entries that was  imin our evaluation. For the purposes of the  evaluaplemented and tested in the Semantic Tag  Extraction we have partitioned the entire HM list into 58  tion Program (STEP). This approach relies both  non-intersecting seed lists of adjectives. The  reon lexical relations (synonymy, antonymy and  hysults of the 58 runs on these non-intersecting seed ponymy) provided in WordNet and on the Word-lists are presented in Table 2. The Table 2 shows  Net glosses. It builds upon the properties of  dicthat the performance of the system exhibits  subtionary entries as a special kind of structured text: stantial variability depending on the composition  such lexicographical texts are built to establish se-of the seed list, with accuracy ranging from 47.6%  mantic equivalence between the left-hand and the  to 87.5% percent (Mean = 71.2%, Standard  Deviright-hand parts of the dictionary entry, and there-ation (St.Dev) = 11.0%).  fore are designed to match as close as possible the components of meaning of the word. They have  relatively standard style, grammar and syntactic  % correct  structures, which removes a substantial source of  noise common to other types of text, and finally,  they have extensive coverage spanning the entire  (WN Relations)  lexicon of a natural language.  The STEP algorithm starts with a small set of  seed words of known sentiment value (positive  (POS clean-up)  or negative). This list is augmented during the  first pass by adding synonyms, antonyms and  hyTable 2: Performance statistics on STEP runs.  ponyms of the seed words supplied in WordNet.  This step brings on average a 5-fold increase in  The significant variability in accuracy of the  the size of the original list with the accuracy of the runs (Standard Deviation over 10%) is attributable  resulting list comparable to manual annotations  to the variability in the properties of the seed list (78%, similar to HM vs. GI-H4 accuracy). At the  words in these runs. The HM list includes some  second pass, the system goes through all WordNet  sentiment-marked words where not all meanings  glosses and identifies the entries that contain in  are laden with sentiment, but also the words where  their definitions the sentiment-bearing words from  some meanings are neutral and even the words  the extended seed list and adds these head words  where such neutral meanings are much more  fre(or rather, lexemes) to the corresponding category  quent than the sentiment-laden ones. The runs  positive, negative or neutral (the remainder). A  where seed lists included such ambiguous  adjecthird, clean-up pass is then performed to partially tives were labeling a lot of neutral words as sen-disambiguate the identified WordNet glosses with  timent marked since such seed words were more  Brill's part-of-speech tagger (Brill, 1995), which  likely to be found in the WordNet glosses in their  performs with up to 95% accuracy, and eliminates  errors introduced into the list by part-of-speech  # 53 had in its seed list two ambiguous adjectives  dim and plush, which are neutral in most of the sentiment extraction system errors 4. Moreover,  contexts. This resulted in only 52.6% accuracy  the boundary between sentiment-bearing (positive  (18.6% below the average).  Run # 48, on the  or negative) and neutral words in GI-H4 accounts  other hand, by a sheer chance, had only  unamfor 93% of disagreements between the labels  asbiguous sentiment-bearing words in its seed list,  signed to adjectives in GI-H4 and HM by two  inand, thus, performed with a fairly high accuracy  dependent teams of human annotators. The view  (87.5%, 16.3% above the average).  taken here is that the vast majority of such  interIn order to generate a comprehensive list  covannotator disagreements are not really errors but  ering the entire set of WordNet adjectives, the 58  a reflection of the natural ambiguity of the words  runs were then collapsed into a single set of unique that are located on the periphery of the sentiment  words. Since many of the clearly sentiment-laden  category.  adjectives that form the core of the category of  Establishing the degree of word's  sentiment were identified by STEP in multiple  centrality to the semantic category  runs and had, therefore, multiple duplicates in the list that were counted as one entry in the com-The approach to sentiment category as a fuzzy  bined list, the collapsing procedure resulted in  set ascribes the category of sentiment some  spea lower-accuracy (66.5% when GI-H4 neutrals  cific structural properties. First, as opposed to the were included) but much larger list of English ad-words located on the periphery, more central  elejectives marked as positive (n = 3, 908) or  negments of the set usually have stronger and more  ative (n = 3, 905). The remainder of WordNet's  numerous semantic relations with other category  22, 141 adjectives was not found in any STEP run  members 5. Second, the membership of these  cenand hence was deemed neutral (n = 14, 328).  tral words in the category is less ambiguous than  Overall, the system's 66.5% accuracy on the  the membership of more peripheral words. Thus,  collapsed runs is comparable to the accuracy  rewe can estimate the centrality of a word in a given ported in the literature for other systems run on  category in two ways:  large corpora (Turney and Littman, 2002;  Hatzi1. Through the density of the word's  relationvassiloglou and McKeown, 1997).  In order to  ships with other words by enumerating its  make a meaningful comparison with the results  semantic ties to other words within the field,  reported in (Turney and Littman, 2002), we also  and calculating membership scores based on  did an evaluation of STEP results on positives and  the number of these ties; and  negatives only (i.e., the neutral adjectives from GI-H4 list were excluded) and compared our labels to  2. Through the degree of word membership  amthe remaining 1266 GI-H4 adjectives. The  accubiguity by assessing the inter-annotator  racy on this subset was 73.4%, which is  comparaagreement on the word membership in this  ble to the numbers reported by Turney and Littman  Lexicographical entries in the dictionaries, such  marked GI words from different parts of speech  as WordNet, seek to establish semantic  equivausing a 2x109 corpus to compute point-wise  mulence between the word and its definition and  protual information between the GI words and 14  vide a rich source of human-annotated  relationmanually selected positive and negative paradigm  ships between the words. By using a  bootstrapping system, such as STEP, that follows the links  The analysis of STEP system performance  between the words in WordNet to find similar  words, we can identify the paths connecting  memually annotated HM and GI-H4 showed that  bers of a given semantic category in the dictionary.  the greatest challenge with sentiment tagging of  With multiple bootstrapping runs on different seed  words lies at the boundary between  sentiment4It is consistent with the observation by Kim and Hovy marked (positive or negative) and  sentiment(2004) who noticed that, when positives and neutrals were neutral words. The 7% performance gain (from  collapsed into the same category opposed to negatives, the 66.5% to 73.4%) associated with the removal of  agreement between human annotators rose by 12%.  5The operationalizations of centrality derived from the neutrals from the evaluation set emphasizes the  number of connections between elements can be found in so-importance of neutral words as a major source of  cial network theory (Burt, 1980)  lists, we can then produce a measure of the  density of such ties.  The ambiguity measure  derived from inter-annotator disagreement can then  be used to validate the results obtained from the  density-based method of determining centrality.  In order to produce a centrality measure, we  conducted multiple runs with non-intersecting  seed lists drawn from HM. The lists of words  fetched by STEP on different runs partially  overlapped, suggesting that the words identified by the system many times as bearing positive or negative  sentiment are more central to the respective  categories. The number of times the word has been  fetched by STEP runs is reflected in the Gross  Overlap Measure produced by the system.  some cases, there was a disagreement between  difFigure 1: Accuracy of word sentiment tagging.  ferent runs on the sentiment assigned to the word.  Such disagreements were addressed by  computing the Net Overlap Scores for each of the found 884 were also found in GI-H4 and/or HM lists,  words: the total number of runs assigning the word  which allowed us to evaluate STEP performance  a negative sentiment was subtracted from the  toand HM-GI agreement on the subset of neutrals as  tal of the runs that consider it positive. Thus, the well. The graph in Figure 1 shows the distribution  greater the number of runs fetching the word (i.e., of adjectives by Net Overlap scores and the aver-Gross Overlap) and the greater the agreement  beage accuracy/agreement rate for each group.  tween these runs on the assigned sentiment, the  Figure 1 shows that the greater the Net  Overhigher the Net Overlap Score of this word.  lap Score, and hence, the greater the distance of  The Net Overlap scores obtained for each  identhe word from the neutral subcategory (i.e., from  tified word were then used to stratify these words  zero), the more accurate are STEP results and the  into groups that reflect positive or negative  disgreater is the agreement between two teams of  hutance of these words from the zero score. The zero  man annotators (HM and GI-H4). On average,  score was assigned to (a) the WordNet adjectives  for all categories, including neutrals, the accuracy that were not identified by STEP as bearing posi-of STEP vs. GI-H4 was 66.5%, human-annotated  tive or negative sentiment 6 and to (b) the words  HM had 78.7% accuracy vs. GI-H4. For the words  with equal number of positive and negative hits  with Net Overlap of 7 and greater, both STEP  on several STEP runs. The performance measures  and HM had accuracy around 90%. The  accufor each of the groups were then computed to  alracy declined dramatically as Net Overlap scores  low the comparison of STEP and human annotator  approached zero (= Neutrals). In this category,  performance on the words from the core and from  human-annotated HM showed only 20%  agreethe periphery of the sentiment category. Thus, for  ment with GI-H4, while STEP, which deemed  each of the Net Overlap Score groups, both  autothese words neutral, rather than positive or  negmatic (STEP) and manual (HM) sentiment  annotaative, performed with 57% accuracy.  tions were compared to human-annotated GI-H4,  These results suggest that the two measures of  which was used as a gold standard in this  experiword centrality, Net Overlap Score based on  multiple STEP runs and the inter-annotator agreement  On 58 runs, the system has identified 3, 908  (HM vs. GI-H4), are directly related 7. Thus, the  English adjectives as positive, 3, 905 as  negaNet Overlap Score can serve as a useful tool in  tive, while the remainder (14, 428) of WordNet's  the identification of core and peripheral members  22, 141 adjectives was deemed neutral. Of these  of a fuzzy lexical category, as well as in  predic14, 328 adjectives that STEP runs deemed neutral,  7In our sample, the coefficient of correlation between the 6The seed lists fed into STEP contained positive or neg-two was 0 68. The Absolute Net Overlap Score on the  subative, but no neutral words, since HM, which was used as a groups 0 to 10 was used in calculation of the coefficient of source for these seed lists, does not include any neutrals.  tion of inter-annotator agreement and system  perannotator agreement and the system  perforformance on a subgroup of words characterized by  mance vs. human-annotated gold standard.  In order to make the Net Overlap Score measure  The list of sentiment-bearing adjectives. The usable in sentiment tagging of texts and phrases,  list produced and cross-validated by multiple  the absolute values of this score should be  norSTEP runs contains 7, 814 positive and  negmalized and mapped onto a standard [0, 1]  interative English adjectives, with an average  acval. Since the values of the Net Overlap Score  curacy of 66.5%, while the human-annotated  may vary depending on the number of runs used in  list HM performed at 78.7% accuracy vs.  the experiment, such mapping eliminates the  varithe gold standard (GI-H4) 8. The remaining  ability in the score values introduced with changes 14, 328 adjectives were not identified as sen-in the number of runs performed. In order to  actiment marked and therefore were considered  complish this normalization, we used the value of  the Net Overlap Score as a parameter in the  stanThe stratification of adjectives by their Net  dard fuzzy membership S-function (Zadeh, 1975;  Overlap Score can serve as an indicator  Zadeh, 1987). This function maps the absolute  of their degree of membership in the  catevalues of the Net Overlap Score onto the interval  gory of (positive/negative) sentiment. Since  from 0 to 1, where 0 corresponds to the absence of  low degrees of membership are associated  membership in the category of sentiment (in our  with greater ambiguity and inter-annotator  case, these will be the neutral words) and 1 reflects disagreement, the Net Overlap Score value  the highest degree of membership in this category.  can provide researchers with a set of  volThe function can be defined as follows:  ume/accuracy trade-offs.  For example, by  including only the adjectives with the Net  Overlap Score of 4 and more, the researcher  can obtain a list of 1, 828 positive and  negative adjectives with accuracy of 81% vs.  GIH4, or 3, 124 adjectives with 75% accuracy  if the threshold is set at 3. The normalization  where u is the Net Overlap Score for the word  of the Net Overlap Score values for the use in  and , , are the three adjustable parameters:  phrase and text-level sentiment tagging  sysis set to 1, is set to 15 and , which represents a tems was achieved using the fuzzy member-crossover point, is defined as = ( + )/2 = 8.  ship function that we proposed here for the  Defined this way, the S-function assigns highest  category of sentiment of English adjectives.  degree of membership (=1) to words that have the  the Net Overlap Score u 15. The accuracy vs.  Future work in the direction laid out by this  GI-H4 on this subset is 100%. The accuracy goes  study will concentrate on two aspects of  sysdown as the degree of membership decreases and  tem development. First further incremental  reaches 59% for values with the lowest degrees of  improvements to the precision of the STEP  membership.  algorithm will be made to increase the  accuracy of sentiment annotation through the  Discussion and conclusions  use of adjective-noun combinatorial patterns  This paper contributes to the development of NLP  within glosses. Second, the resulting list of  and semantic tagging systems in several respects.  adjectives annotated with sentiment and with  the degree of word membership in the  cate The structure of the semantic category of  gory (as measured by the Net Overlap Score)  The analysis of the category  will be used in sentiment tagging of phrases  of sentiment of English adjectives presented  and texts. This will enable us to compute the  here suggests that this category is structured  degree of importance of sentiment markers  as a fuzzy set: the distance from the core  found in phrases and texts. The availability  of the category, as measured by Net  Over8GI-H4 contains 1268 and HM list has 1336 positive and lap scores derived from multiple STEP runs,  negative adjectives. The accuracy figures reported here in-is shown to affect both the level of  interclude the errors produced at the boundary with neutrals.  of the information on the degree of central-with semantic tags. It has been shown here  ity of words to the category of sentiment may  that the inter-annotator agreement tends to  improve the performance of sentiment  deterfall as we proceed from the core of a fuzzy  mination systems built to identify the  sentisemantic category to its periphery.   Therement of entire phrases or texts.  fore, the disagreement between the  annotators does not necessarily reflect a quality  System evaluation considerations. The con-problem in human annotation, but rather a  tribution of this paper to the development  structural property of the semantic category.  of methodology of system evaluation is  twoThis suggests that inter-annotator  disagreefold. First, this research emphasizes the  imment rates can serve as an important source  portance of multiple runs on different seed  of empirical information about the structural  lists for a more accurate evaluation of  sentiproperties of the semantic category and can  ment tag extraction system performance. We  help define and validate fuzzy sets of  semanhave shown how significantly the system  retic category members for a number of NLP  sults vary, depending on the composition of  tasks and applications.  the seed list.  Second, due to the high cost of manual  annotation and other practical considerations,  most bootstrapping and other NLP systems  are evaluated on relatively small manually  Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case  annotated gold standards developed for a  study in part-of-speech tagging. Computational Lin-given semantic category.  The implied  assumption is that such a gold standard  represents a random sample drawn from the  popR.S. Burt. 1980. Models of network structure. Annual Review of Sociology, 6:79 141.  ulation of all category members and hence,  system performance observed on this gold  Philip Edmonds. 1999. Semantic representations of standard can be projected to the whole se-near-synonyms for automatic lexical choice. Ph.D.  mantic category. Such extrapolation is not  thesis, University of Toronto.  justified if the category is structured as a  lexChristiane Fellbaum, editor. 1998. WordNet: An Elec-ical field with fuzzy boundaries: in this case  tronic Lexical Database. MIT Press, Cambridge, the precision of both machine and human an-MA.  notation is expected to fall when more  peripheral members of the category are  proJames G. Shanahan. 2004. Validating the  Covercessed. In this paper, the sentiment-bearing  age of Lexical Resources for Affect Analysis and  words identified by the system were stratified  Automatically Classifying New Words along  Sebased on their Net Overlap Score and  evalJanyce Wiebe, editors, Exploring Attitude and Af-uated in terms of accuracy of sentiment  anfect in Text: Theories and Applications, AAAI-2004  notation within each stratum. These strata,  Spring Symposium Series, pages 71 78.  derived from Net Overlap scores, reflect the  degree of centrality of a given word to the  Vasileios Hatzivassiloglou and Kathleen B. McKeown.  1997. Predicting the Semantic Orientation of  Adjecsemantic category, and, thus, provide greater  tives. In 35th ACL, pages 174 181.  assurance that system performance on other  words with the same Net Overlap Score will  be similar to the performance observed on the  marizing customer reviews. In KDD-04, pages 168  intersection of system results with the gold  The role of the inter-annotator  disagreesemantic orientation of adjectives. In LREC 2004,  ment. The results of the study presented in volume IV, pages 1115 1118.  this paper call for reconsideration of the role  Soo-Min Kim and Edward Hovy. 2004. Determining  of inter-annotator disagreement in the  develthe sentiment of opinions. In COLING-2004, pages opment of lists of words manually annotated  Adrienne Lehrer. 1974. Semantic Fields and Lexical Structure. North Holland, Amsterdam and New York.  Eleanor Rosch. 1978. Principles of Categorization. In Eleanor Rosch and Barbara B. Lloyd, editors, Cognition and Categorization, pages 28 49. Lawrence Erlbaum Associates, Hillsdale, New Jersey.  P.J. Stone, D.C. Dumphy, M.S. Smith, and D.M.  Ogilvie. 1966. The General Inquirer: a computer approach to content analysis. M.I.T. studies in com-parative politics. M.I.T. Press, Cambridge, MA.  Pero Subasic and Alison Huettner. 2001. Affect Analysis of Text Using Fuzzy Typing. IEEE-FS, 9:483  Peter Turney and Michael Littman.  supervised learning of semantic orientation from  a hundred-billion-word corpus.  Technical Report  ERC-1094 (NRC 44929), National Research  CounDeveloping Affective Lexical  Resources. PsychNology Journal, 2(1):61 83.  wards Answering Opinion Questions: Separating  Facts from Opinions and Identifying the Polarity of Opinion Sentences.  In Conference on Empirical  Methods in Natural Language Processing  (EMNLPLotfy A. Zadeh. 1975. Calculus of Fuzzy  Restrictions.  M. Shimura, editors, Fuzzy Sets and their Applications to cognitive and decision processes, pages 1  40. Academic Press Inc., New-York.  Lotfy A. Zadeh. 1987. PRUF a Meaning  Representation Language for Natural Languages.  R.R. Yager, S. Ovchinnikov, R.M. Tong, and H.T.  Nguyen, editors, Fuzzy Sets and Applications: Selected Papers by L.A. Zadeh, pages 499 568. John Wiley & Sons. 