 A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts  Department of Computer Science  Cornell University  Ithaca, NY 14853-7501  ter; and then (2) apply a standard machine-learning Sentiment analysis seeks to identify the view-classifier to the resulting extract. This can prevent point(s) underlying a text span; an example appli-the polarity classifier from considering irrelevant or cation is classifying a movie review as thumbs up  even potentially misleading text: for example, al-or thumbs down . To determine this sentiment po-though the sentence The protagonist tries to pro-larity, we propose a novel machine-learning method tect her good name contains the word good , it that applies text-categorization techniques to just tells us nothing about the author's opinion and in the subjective portions of the document. Extracting fact could well be embedded in a negative movie these portions can be implemented using efficient review. Also, as mentioned above, subjectivity ex-techniques for finding minimum cuts in graphs; this tracts can be provided to users as a summary of the greatly facilitates incorporation of cross-sentence sentiment-oriented content of the document.  Our results show that the subjectivity extracts we create accurately represent the sentiment in-Publication info: Proceedings of the ACL, 2004.  formation of the originating documents in a much 1 Introduction  more compact form: depending on choice of  downstream polarity classifier, we can achieve highly sta-The computational treatment of opinion, sentiment, tistically significant improvement (from 82.8% to and subjectivity has recently attracted a great deal 86.4%) or maintain the same level of performance of attention (see references), in part because of its for the polarity classification task while retaining potential applications. For instance, information-only 60% of the reviews' words. Also, we  exextraction and question-answering systems could plore extraction methods based on a minimum cut flag statements and queries regarding opinions  formulation, which provides an efficient, intuitive, rather than facts (Cardie et al., 2003). Also, it and effective means for integrating inter-sentence-has proven useful for companies, recommender sys-level contextual information with traditional bag-of-tems, and editorial sites to create summaries of peo-words features.  ple's experiences and opinions that consist of subjective expressions extracted from reviews (as is 2 Method  commonly done in movie ads) or even just a  review's polarity positive ( thumbs up ) or neg-2.1 Architecture  ative ( thumbs down ).  One can consider document-level polarity classi-Document polarity classification poses a signifi-fication to be just a special (more difficult) case cant challenge to data-driven methods, resisting tra-of text categorization with rather than ditional text-categorization techniques (Pang, Lee, topic-based categories. Hence, standard machine-and Vaithyanathan, 2002). Previous approaches fo-learning classification techniques, such as support cused on selecting indicative lexical features (e.g., vector machines (SVMs), can be applied to the en-the word good ), classifying a document accord-tire documents themselves, as was done by Pang, ing to the number of such features that occur any-Lee, and Vaithyanathan (2002). We refer to such where within it. In contrast, we propose the follow-classification techniques as default polarity classi-ing process: (1) label the sentences in the document fiers.  as either subjective or objective, discarding the lat-However, as noted above, we may be able to  improve polarity classification by removing objective tors to attempt to overcome this obstacle. However, sentences (such as plot summaries in a movie re-we propose an alternative that avoids the need for view). We therefore propose, as depicted in Figure such feature engineering: we use an efficient and 1, to first employ a subjectivity detector that deter-intuitive graph-based formulation relying on find-mines whether each sentence is subjective or not: ing minimum cuts. Our approach is inspired by discarding the objective ones creates an extract that Blum and Chawla (2001), although they focused on should better represent a review's subjective content similarity between items (the motivation being to to a default polarity classifier.  combine labeled and unlabeled data), whereas we are concerned with physical proximity between the subjective  n-sentence review  items to be classified; indeed, in computer vision, review?  s1  modeling proximity information via graph cuts has s2  led to very effective classification (Boykov, Veksler, s1  s3  s4  s4  default polarity classifier  2.3 Cut-based classification  Figure 2 shows a worked example of the concepts s_n  subjectivity  in this section.  subjectivity extraction  Suppose we have  items  into two classes  and  , and we have access to  Figure 1: Polarity classification via subjectivity detec-two types of information:  tion.  mates of each 's preference for being in  based  To our knowledge, previous work has not  inon just the features of  alone; and  tegrated sentence-level subjectivity detection with Association scores  estimates of how important it is that  and  the same class.1  level analysis and for determining whether a document is subjective or not, but do not combine these We would like to maximize each item's net hap-two types of algorithms or consider document polar-piness : its individual score for the class it is as-ity classification. The motivation behind the single-signed to, minus its individual score for the other sentence selection method of Beineke et al. (2004) class. But, we also want to penalize putting tightly-is to reveal a document's sentiment polarity, but they associated items into different classes. Thus, after do not evaluate the polarity-classification accuracy some algebra, we arrive at the following optimiza-that results.  tion problem: assign the  s to  and  so as to  minimize the partition cost  2.2 Context and Subjectivity Detection  As with document-level polarity classification, we  could perform subjectivity detection on individual  sentences by applying a standard classification algorithm on each sentence in isolation. However, modeling proximity relationships between sentences The problem appears intractable, since there are  would enable us to leverage coherence: text spans possible binary partitions of the  's.  Howoccurring near each other (within discourse bound-ever, suppose we represent the situation in the fol-aries) may share the same subjectivity status, other lowing manner. Build an undirected graph  with  things being equal (Wiebe, 1994).  ; the last two are,  respecWe would therefore like to supply our algorithms tively, the source and sink. Add edges  , each  with pair-wise interaction information, e.g., to spec-with weight  , and edges  , each with  ify that two particular sentences should ideally re-weight  edges  ceive the same subjectivity label but not state which each with weight  . Then, cuts in  label this should be. Incorporating such informa-are defined as follows:  tion is somewhat unnatural for classifiers whose in-Definition 1 A cut  of  is a partition of its  put consists simply of individual feature vectors, KILM  nodes into sets  and  such as Naive Bayes or SVMs, precisely because  where Z  . Its cost  is the sum  such classifiers label each test item in isolation.  %H$&!9AKILM  One could define synthetic features or feature vec-1Asymmetry is allowed, but we used symmetric scores.  ind (Y) [.2]  Association Cost  assoc(Y,M) [1.0]  assoc(Y,N) [.1]  s ind (M)[.5]  ind (M) [.5]  assoc(M,N) [.2]  oTeRbdc  ofeRb  ofeRbni  Figure 2: Graph for classifying three items. Brackets enclose example values; here, the individual scores happen to be probabilities. Based on individual scores alone, we would put ( yes ) in _  ( no ) in ^r , and be undecided  about  ( maybe ). But the association scores favor cuts that put and  in the same class, as shown in the table.  s  s  Thus, the minimum cut, indicated by the dashed line, places together with  s  of the weights of all edges crossing from  loglou and McKeown, 1997; Agrawal et al., 2003; K  is one of minimum cost.  Joachims, 2003).  Observe that every cut corresponds to a partition of 3 Evaluation Framework  the items and has cost equal to the partition cost.  Thus, our optimization problem reduces to finding Our experiments involve classifying movie reviews minimum cuts.  as either positive or negative, an appealing task for several reasons. First, as mentioned in the intro-Practical advantages As we have noted, formulat-duction, providing polarity information about re-ing our subjectivity-detection problem in terms of views is a useful service: witness the popularity of graphs allows us to model item-specific and pair-www.rottentomatoes.com. Second, movie reviews  wise information independently. Note that this is are apparently harder to classify than reviews of a very flexible paradigm. For instance, it is per-other products (Turney, 2002; Dave, Lawrence, and fectly legitimate to use knowledge-rich algorithms Pennock, 2003). Third, the correct label can be ex-employing deep linguistic knowledge about  sentracted automatically from rating information (e.g., timent indicators to derive the individual scores.  number of stars). Our data4 contains 1000 positive And we could also simultaneously use knowledge-and 1000 negative reviews all written before 2002, lean methods to assign the association scores. In-with a cap of 20 reviews per author (312 authors terestingly, Yu and Hatzivassiloglou (2003) com-total) per category. We refer to this corpus as the pared an individual-preference classifier against a polarity dataset.  relationship-based method, but didn't combine the two; the ability to coordinate such algorithms is Default polarity classifiers We tested support vec-precisely one of the strengths of our approach.  tor machines (SVMs) and Naive Bayes (NB).  FolBut a crucial advantage specific to the utilization lowing Pang et al. (2002), we use unigram-presence of a minimum-cut-based approach is that we can use features: the th coordinate of a feature vector is maximum-flow algorithms with polynomial asymp-1 if the corresponding unigram occurs in the input totic running times and near-linear running times text, 0 otherwise. (For SVMs, the feature vectors in practice to exactly compute the minimum-are length-normalized). Each default  documentcost cut(s), despite the apparent intractability of level polarity classifier is trained and tested on the the optimization problem (Cormen, Leiserson, and extracts formed by applying one of the sentence-Rivest, 1990; Ahuja, Magnanti, and Orlin, 1993).2  level subjectivity detectors to reviews in the polarity In contrast, other graph-partitioning problems that dataset.  have been previously used to formulate NLP classification problems3 are NP-complete (Hatzivassi-Subjectivity dataset To train our detectors, we need a collection of labeled sentences. Riloff and 2Code available at http://www.avglab.com/andrew/soft.html.  3Graph-based approaches to general clustering problems 4Available   www.cs.cornell.edu/people/pabo/movieare too numerous to mention here.  review-data/ (review corpus version 2.0).  Wiebe (2003) state that It is [very hard] to ob-gree of proximity between pairs of sentences, contain collections of individual sentences that can be trolled by three parameters. The threshold  easily identified as subjective or objective ; the ifies the maximum distance two sentences can be polarity-dataset sentences, for example, have not separated by and still be considered proximal. The been so annotated.5 Fortunately, we were able  non-increasing function  specifies how the  into mine the Web to create a large, automatically-fluence of proximal sentences decays with respect to labeled sentence corpus6.  To gather subjective  distance ; in our experiments, we tried  sentences (or phrases), we collected 5000  movie, and  . The constant controls the relative  review snippets (e.g., bold, imaginative, and im-influence of the association scores: a larger makes  possible to resist ) from www.rottentomatoes.com.  the minimum-cut algorithm more loath to put prox-To obtain (mostly) objective data, we took 5000 sen-imal sentences in different classes. With these in tences from plot summaries available from the In-hand8, we set (for  ternet Movie Database (www.imdb.com). We only  selected sentences or snippets at least ten words  long and drawn from reviews or plot summaries of  otherwise.  movies released post-2001, which prevents overlap 4 Experimental Results  with the polarity dataset.  Below, we report average accuracies computed by Subjectivity detectors As noted above, we can use ten-fold cross-validation over the polarity dataset.  our default polarity classifiers as basic sentence-Section 4.1 examines our basic subjectivity extrac-level subjectivity detectors (after retraining on the tion algorithms, which are based on individual-subjectivity dataset) to produce extracts of the orig-sentence predictions alone. Section 4.2 evaluates inal reviews. We also create a family of cut-based the more sophisticated form of subjectivity extrac-subjectivity detectors; these take as input the set of tion that incorporates context information via the sentences appearing in a single document and de-minimum-cut paradigm.  termine the subjectivity status of all the sentences As we will see, the use of subjectivity extracts simultaneously using per-item and pairwise rela-can in the best case provide satisfying improve-tionship information. Specifically, for a given doc-ment in polarity classification, and otherwise can ument, we use the construction in Section 2.2 to at least yield polarity-classification accuracies indis-build a graph wherein the source and sink  cortinguishable from employing the full review. At the  respond to the class of subjective and objective sen-same time, the extracts we create are both smaller tences, respectively, and each internal node  on average than the original document and more  responds to the document's  sentence . We can  effective as input to a default polarity classifier  set the individual scores  to  and  than the same-length counterparts produced by  stanto  , as shown in Figure 3,  dard summarization tactics (e.g., or last-N  senwhere  denotes Naive Bayes' estimate of  tences). We therefore conclude that subjectivity  exthe probability that sentence is subjective; or, we traction produces effective summaries of document  can use the weights produced by the SVM  classifier instead.7 If we set all the association scores to zero, then the minimum-cut classification of the 4.1 Basic subjectivity extraction  sentences is the same as that of the basic subjectiv-As noted in Section 3, both Naive Bayes and SVMs ity detector. Alternatively, we incorporate the de-can be trained on our subjectivity dataset and then used as a basic subjectivity detector. The former has 5We therefore could not directly evaluate sentence-somewhat better average ten-fold cross-validation classification accuracy on the polarity dataset.  performance on the subjectivity dataset (92% vs.   www.cs.cornell.edu/people/pabo/movie90%), and so for space reasons, our initial discus-review-data/ , sentence corpus version 1.0.  sions will focus on the results attained via NB sub-We converted SVM output 8 , which is a signed distance  (negative=objective) from the separating hyperplane, to non-jectivity detection.  negative numbers by  Employing Naive Bayes as a subjectivity  detector ( ExtractNB) in conjunction with a Naive Bayes 8 Y  document-level polarity classifier achieves 86.4%  8Parameter training is driven by optimizing the performance and v  . Note that scaling is employed  of the downstream polarity classifier rather than the detector  only for consistency; the algorithm itself does not require prob-itself because the subjectivity dataset's sentences come from abilities for individual scores.  different reviews, and so are never proximal.  n sentence review  s1  Pr (s1)  1 Pr (s1)  s2  s  s  s3  s1  s4  Pr individual subjectivity probability link edge crossing the cut  proximity link  Figure 3: Graph-cut-based creation of subjective extracts.  accuracy.9 This is a clear improvement over the lengths by taking just the  most subjective  sen82.8% that results when no extraction is applied tences11 from the originating review. As one  base( Full review); indeed, the difference is highly sta-line to compare against, we take the canonical sum-tistically significant (  , paired t-test). With  marization standard of extracting the first  senSVMs as the polarity classifier instead, the Full re-tences in general settings, authors often  beview performance rises to 87.15%, but comparison gin documents with an overview. We also con-via the paired t-test reveals that this is statistically sider the last  indistinguishable from the 86.4% that is achieved by concluding material may be a good summary, and  running the SVM polarity classifier on ExtractNB  www.rottentomatoes.com tends to select snippets  input. (More improvements to extraction  perforfrom the end of movie reviews (Beineke et al.,  mance are reported later in this section.)  2004). Finally, as a sanity check, we include results These findings indicate10 that the extracts pre-from the  least subjective sentences according to  serve (and, in the NB polarity-classifier case, appar-Naive Bayes.  ently clarify) the sentiment information in the orig-Figure 4 shows the polarity classifier results as inating documents, and thus are good summaries  ranges between 1 and 40. Our first observation  from the polarity-classification point of view. Fur-is that the NB detector provides very good bang ther support comes from a flipping experiment: for the buck : with subjectivity extracts containing if we give as input to the default polarity classifier as few as 15 sentences, accuracy is quite close to an extract consisting of the sentences labeled ob-what one gets if the entire review is used. In fact, jective, accuracy drops dramatically to 71% for NB  for the NB polarity classifier, just using the 5 most and 67% for SVMs. This confirms our hypothesis  subjective sentences is almost as informative as the that sentences discarded by the subjectivity extrac-Full review while containing on average only about tion process are indeed much less indicative of sen-22% of the source reviews' words.  timent polarity.  Also, it so happens that at  , performance  Moreover, the subjectivity extracts are much  is actually slightly better than (but statistically in-more compact than the original documents (an im-distinguishable from) Full review even when the portant feature for a summary to have): they contain SVM default polarity classifier is used (87.2% vs.  on average only about 60% of the source reviews'  87.15%).12 This suggests potentially effective ex-words. (This word preservation rate is plotted along traction alternatives other than using a fixed proba-the x-axis in the graphs in Figure 5.) This prompts bility threshold (which resulted in the lower accu-us to study how much reduction of the original doc-racy of 86.4% reported above).  uments subjectivity detectors can perform and still Furthermore, we see in Figure 4 that the  accurately represent the texts' sentiment information.  11These are the  sentences assigned the highest probability  We can create subjectivity extracts of varying  by the basic NB detector, regardless of whether their probabilities exceed 50% and so would actually be classified as subjective by Naive Bayes. For reviews with fewer than sentences,  9This result and others are depicted in Figure 5; for now, the entire review will be returned.  consider only the y-axis in those plots.  12Note that roughly half of the documents in the polarity 10Recall that direct evidence is not available because the po-dataset contain more than 30 sentences (average=32.3, standard larity dataset's sentences lack subjectivity labels.  Accuracy for N-sentence abstracts (def = NB)  Accuracy for N-sentence abstracts (def = SVM)  Average accuracy  Average accuracy  first N sentences  first N sentences  N least subjective sentences  N least subjective sentences  Full review  Full review  Figure 4: Accuracies using N-sentence extracts for NB (left) and SVM (right) default polarity classifiers.  Accuracy for subjective abstracts (def = NB)  Accuracy for subjective abstracts (def = SVM)  Full Review  difference in accuracy  difference in accuracy  not statistically significant  not statistically significant  SVM+Prox  Average accuracy  Average accuracy  indicates statistically significant  indicates statistically significant  improvement in accuracy  improvement in accuracy  Full Review  % of words extracted  % of words extracted  Figure 5: Word preservation rate vs. accuracy, NB (left) and SVMs (right) as default polarity classifiers.  Also indicated are results for some statistical significance tests.  subjective-sentences method generally outperforms textual constraints are easily incorporated via the the other baseline summarization methods (which minimum-cut formalism but are not natural inputs perhaps suggests that sentiment summarization can-for standard Naive Bayes and SVMs.  not be treated the same as topic-based summariza-Figure 5 shows the effect of adding in  tion, although this conjecture would need to be veri-proximity information.  ExtractNB+Prox and  fied on other domains and data). It's also interesting ExtractSVM+Prox are the graph-based subjectivity to observe how much better the last  detectors using Naive Bayes and SVMs,  respecthan the first  sentences; this may reflect a (hardly  tively, for the individual scores; we depict the  surprising) tendency for movie-review authors to best performance achieved by a single setting of place plot descriptions at the beginning rather than the three proximity-related edge-weight parameters the end of the text and conclude with overtly opin-over all ten data folds13 (parameter selection was ionated statements.  not a focus of the current work). The two comparisons we are most interested in are ExtractNB+Prox 4.2 Incorporating context information  versus ExtractNB and ExtractSVM+Prox versus Extract  The previous section demonstrated the value of  subjectivity detection. We now examine whether  We see that the context-aware graph-based  subcontext information, particularly regarding sentence jectivity detectors tend to create extracts that are proximity, can further improve subjectivity extrac-13Parameters  are chosen from  tion. As discussed in Section 2.2 and 3,  con, and  at intervals of 0.1.  more informative (statistically significant so (paired fact, for the Naive Bayes polarity classifier, the sub-t-test) for SVM subjectivity detectors only), al-jectivity extracts are shown to be more effective in-though these extracts are longer than their context-put than the originating document, which suggests blind counterparts. We note that the performance that they are not only shorter, but also cleaner rep-enhancements cannot be attributed entirely to the resentations of the intended polarity.  mere inclusion of more sentences regardless of  We have also shown that employing the  whether they are subjective or not one counter-minimum-cut framework results in the  developargument is that Full review yielded substantially ment of efficient algorithms for sentiment analy-worse results for the NB default polarity classifier  sis. Utilizing contextual information via this frame-and at any rate, the graph-derived extracts are still work can lead to statistically significant improve-substantially more concise than the full texts.  ment in polarity-classification accuracy. Directions Now, while incorporating a bias for assigning  for future research include developing parameter-nearby sentences to the same category into NB and selection techniques, incorporating other sources of SVM subjectivity detectors seems to require some contextual cues besides sentence proximity, and in-non-obvious feature engineering, we also wish  vestigating other means for modeling such informa-to investigate whether our graph-based paradigm tion.  makes better use of contextual constraints that can be (more or less) easily encoded into the input of Acknowledgments  standard classifiers. For illustrative purposes, we We thank Eric Breck, Claire Cardie, Rich Caruana, consider paragraph-boundary information, looking Yejin Choi, Shimon Edelman, Thorsten Joachims,  only at SVM subjectivity detection for simplicity's Jon Kleinberg, Oren Kurland, Art Munson, Vincent sake.  Ng, Fernando Pereira, Ves Stoyanov, Ramin Zabih, It seems intuitively plausible that paragraph  and the anonymous reviewers for helpful comments.  boundaries (an approximation to discourse bound-This paper is based upon work supported in part aries) loosen coherence constraints between nearby by the National Science Foundation under grants sentences. To capture this notion for minimum-cut-ITR/IM IIS-0081334 and IIS-0329064, a Cornell  based classification, we can simply reduce the as-Graduate Fellowship in Cognitive Studies, and by sociation scores for all pairs of sentences that oc-an Alfred P. Sloan Research Fellowship. Any opin-cur in different paragraphs by multiplying them by ions, findings, and conclusions or recommendations a cross-paragraph-boundary weight  expressed above are those of the authors and do not  standard classifiers, we can employ the trick of hav-necessarily reflect the views of the National Science ing the detector treat paragraphs, rather than sen-Foundation or Sloan Foundation.  tences, as the basic unit to be labeled. This enables the standard classifier to utilize coherence be-References  tween sentences in the same paragraph; on the other Agrawal, Rakesh, Sridhar Rajagopalan, Ramakrish-hand, it also (probably unavoidably) poses a hard nan Srikant, and Yirong Xu. 2003. Mining news-constraint that all of a paragraph's sentences get the groups using networks arising from social behav-same label, which increases noise sensitivity.14 Our ior. In WWW, pages 529 535.  experiments reveal the graph-cut formulation to be Ahuja, Ravindra, Thomas L. Magnanti, and  the better approach: for both default polarity clas-James B. Orlin. 1993. Network Flows: Theory, sifiers (NB and SVM), some choice of parameters Algorithms, and Applications. Prentice Hall.  (including ) for ExtractSVM+Prox yields  statistically significant improvement over its paragraph-Beineke, Philip, Trevor Hastie, Christopher Man-unit non-graph counterpart (NB: 86.4% vs. 85.2%; ning, and Shivakumar Vaithyanathan. 2004.  Exploring sentiment summarization. In AAAI  Spring Symposium on Exploring Attitude and Af-5 Conclusions  fect in Text: Theories and Applications (AAAI tech report SS-04-07).  We examined the relation between subjectivity de-Blum, Avrim and Shuchi Chawla. 2001. Learning  tection and polarity classification, showing that sub-from labeled and unlabeled data using graph min-jectivity detection can compress reviews into much cuts. In Intl. Conf. on Machine Learning (ICML), shorter extracts that still retain polarity information pages 19 26.  at a level comparable to that of the full review. In Boykov, Yuri, Olga Veksler, and Ramin Zabih.  14For example, in the data we used, boundaries may have 1999. Fast approximate energy minimization via  been missed due to malformed html.  graph cuts. In Intl. Conf. on Computer Vision  (ICCV), pages 377 384. Journal version in IEEE  ing Attitude and Affect in Text: Theories and Ap-Trans. Pattern Analysis and Machine Intelligence plications. AAAI technical report SS-04-07.  Riloff, Ellen and Janyce Wiebe. 2003. Learning  Cardie, Claire, Janyce Wiebe, Theresa Wilson, and extraction patterns for subjective expressions. In Diane Litman. 2003. Combining low-level and  summary representations of opinions for  multiRiloff, Ellen, Janyce Wiebe, and Theresa Wilson.  perspective question answering. In AAAI Spring 2003. Learning subjective nouns using extraction Symposium on New Directions in Question An-pattern bootstrapping. In Conf. on Natural Lan-swering, pages 20 27.  guage Learning (CoNLL), pages 25 32.  Cormen, Thomas H., Charles E. Leiserson, and  Subasic, Pero and Alison Huettner. 2001.  AfRonald L. Rivest. 1990. Introduction to Algo-fect analysis of text using fuzzy semantic typing.  rithms. MIT Press.  Das, Sanjiv and Mike Chen. 2001. Yahoo! for  Tong, Richard M. 2001. An operational system for Amazon: Extracting market sentiment from stock  detecting and tracking opinions in on-line discus-message boards. In Asia Pacific Finance Associ-sion. SIGIR Wksp. on Operational Text  Classifiation Annual Conf. (APFA).  cation.  Dave, Kushal, Steve Lawrence, and David M.  PenTurney, Peter. 2002. Thumbs up or thumbs down?  nock. 2003. Mining the peanut gallery: Opinion  Semantic orientation applied to unsupervised  extraction and semantic classification of product classification of reviews. In ACL, pages 417 424.  reviews. In WWW, pages 519 528.  Wiebe, Janyce M. 1994. Tracking point of view in Dini, Luca and Giampaolo Mazzini. 2002. Opin-narrative. Computational Linguistics, 20(2):233  ion classification through information extraction.  In Intl. Conf. on Data Mining Methods and  Yi, Jeonghee, Tetsuya Nasukawa, Razvan Bunescu, Databases for Engineering, Finance and Other and Wayne Niblack. 2003. Sentiment analyzer:  Extracting sentiments about a given topic using Durbin, Stephen D., J. Neal Richter, and Doug  Warner. 2003. A system for affective rating of  Intl. Conf. on Data Mining (ICDM).  texts. In KDD Wksp. on Operational Text Classi-Yu, Hong and Vasileios Hatzivassiloglou. 2003.  fication Systems (OTC-3).  Towards answering opinion questions:  Separating facts from opinions and identifying the polar-Keown. 1997. Predicting the semantic  orientaity of opinion sentences. In EMNLP.  tion of adjectives. In 35th ACL/8th EACL, pages 174 181.  Joachims, Thorsten. 2003. Transductive learning via spectral graph partitioning. In Intl. Conf. on Machine Learning (ICML).  Liu, Hugo, Henry Lieberman, and Ted Selker.  2003. A model of textual affect sensing using  real-world knowledge. In Intelligent User Inter-faces (IUI), pages 125 132.  and Alexander Gelbukh. 1999. Text mining as a  social thermometer. In IJCAI Wksp. on Text Mining, pages 103 107.  Morinaga, Satoshi, Kenji Yamanishi, Kenji Tateishi, and Toshikazu Fukushima. 2002. Mining product reputations on the web. In KDD, pages 341  349. Industry track.  Vaithyanathan. 2002. Thumbs up?  ment classification using machine learning  tors. 2004. AAAI Spring Symposium on 