 Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification  Department of Computer and Information Science  University of Pennsylvania  deployed industrially in systems that gauge market reaction and summarize opinion from Web pages,  Automatic sentiment classification has been  discussion boards, and blogs.  extensively studied and applied in recent  With such widely-varying domains, researchers  years. However, sentiment is expressed  difand engineers who build sentiment classification  ferently in different domains, and annotating  systems need to collect and curate data for each new corpora for every possible domain of interest  domain they encounter. Even in the case of market  analysis, if automatic sentiment classification were tation for sentiment classifiers, focusing on  to be used across a wide range of domains, the  efonline reviews for different types of  prodfort to annotate corpora for each domain may  beucts. First, we extend to sentiment  classificome prohibitive, especially since product features cation the recently-proposed structural cor-change over time. We envision a scenario in which  respondence learning (SCL) algorithm,  redevelopers annotate corpora for a small number of  ducing the relative error due to adaptation  domains, train classifiers on those corpora, and then between domains by an average of 30% over  apply them to other similar corpora. However, this the original SCL algorithm and 46% over  approach raises two important questions. First, it a supervised baseline. Second, we identify  is well known that trained classifiers lose accuracy a measure of domain similarity that corre-when the test data distribution is significantly differ-lates well with the potential for adaptation  ent from the training data distribution 1. Second, it is of a classifier from one domain to another.  not clear which notion of domain similarity should This measure could for instance be used to  be used to select domains to annotate that would be select a small set of domains to annotate  good proxies for many other domains.  whose trained classifiers would transfer well  We propose solutions to these two questions and  to many other domains.  evaluate them on a corpus of reviews for four different types of products from Amazon: books, DVDs,  Introduction  electronics, and kitchen appliances2. First, we show Sentiment detection and classification has received how to extend the recently proposed structural cor-considerable attention recently (Pang et al., 2002; 1  Turney, 2002; Goldberg and Zhu, 2004).  While  For surveys of recent research on domain adaptation, see the ICML 2006 Workshop on Structural Knowledge Transfer movie reviews have been the most studied domain,  for Machine Learning (http://gameairesearch.uta.  sentiment analysis has extended to a number of  edu/) and the NIPS 2006 Workshop on Learning when test new domains, ranging from stock message boards  and training inputs have different distribution (http://ida.  to congressional floor debates (Das and Chen, 2001; 2The dataset will be made available by the authors at publi-Thomas et al., 2006). Research results have been  respondence learning (SCL) domain adaptation al-them. After learning a classifier for computer  regorithm (Blitzer et al., 2006) for use in sentiment views, when we see a cell-phone feature like good-classification. A key step in SCL is the selection of quality reception , we know it should behave in a  pivot features that are used to link the source and tar-roughly similar manner to fast dual-core .  get domains. We suggest selecting pivots based not only on their common frequency but also according  Algorithm Overview  to their mutual information with the source labels.  Given labeled data from a source domain and  unFor data as diverse as product reviews, SCL can  labeled data from both source and target domains,  SCL first chooses a set of m pivot features which oc-tion when we adapt between domains. In our second  cur frequently in both domains. Then, it models the extension we show how to correct misalignments us-correlations between the pivot features and all other ing a very small number of labeled instances.  features by training linear pivot predictors to predict Second, we evaluate the A-distance (Ben-David  occurrences of each pivot in the unlabeled data from et al., 2006) between domains as measure of the loss both domains (Ando and Zhang, 2005; Blitzer et al., due to adaptation from one to the other. The A-2006). Th th pivot predictor is characterized by  distance can be measured from unlabeled data, and it its weight vector w`; positive entries in that weight was designed to take into account only divergences vector mean that a non-pivot feature (like fast dual-which affect classification accuracy. We show that it core ) is highly correlated with the corresponding correlates well with adaptation loss, indicating that pivot (like excellent ).  we can use the A-distance to select a subset of doThe pivot predictor column weight vectors can be  mains to label as sources.  arranged into a matrix W = [w  In the next section we briefly review SCL and  inbe the top k left singular vectors of W (here d indi-troduce our new pivot selection method. Section 3  cates the total number of features). These vectors are describes datasets and experimental method. Sec-the principal predictors for our weight space. If we tion 4 gives results for SCL and the mutual informa-chose our pivot features well, then we expect these tion method for selecting pivot features. Section 5  principal predictors to discriminate among positive shows how to correct feature misalignments using a and negative words in both domains.  small amount of labeled target domain data.  SecAt training and test time, suppose we observe a  tion 6 motivates the A-distance and shows that it  feature vector x. We apply the projection x to ob-correlates well with adaptability. We discuss related tain k new real-valued features. Now we learn a  work in Section 7 and conclude in Section 8.  predictor for the augmented instance hx, xi. If  contains meaningful correspondences, then the  preStructural Correspondence Learning  dictor which uses will perform well in both source and target domains.  Before reviewing SCL, we give a brief illustrative example. Suppose that we are adapting from re-2.2  Selecting Pivots with Mutual Information  views of computers to reviews of cell phones. While The efficacy of SCL depends on the choice of pivot many of the features of a good cell phone review are features. For the part of speech tagging problem  the same as a computer review the words  excelstudied by Blitzer et al. (2006), frequently-occurring lent and awful for example many words are to-words in both domains were good choices, since  tally new, like reception . At the same time, many they often correspond to function words such as  features which were useful for computers, such as  prepositions and determiners, which are good  indi dual-core are no longer useful for cell phones.  cators of parts of speech. This is not the case for Our key intuition is that even when good-quality  sentiment classification, however. Therefore, we re-reception and fast dual-core are completely dis-quire that pivot features also be good predictors of tinct for each domain, if they both have high correla-the source label. Among those features, we then  tion with excellent and low correlation with aw-choose the ones with highest mutual information to ful on unlabeled data, then we can tentatively align the source label. Table 1 shows the set-symmetric  SCL, not SCL-MI  SCL-MI, not SCL  2004). On the polarity dataset, this model matches book one < num> so all  a must a wonderful loved it  the results reported by Pang et al. (2002). When we very about they like  report results with SCL and SCL-MI, we require that good when  highly recommended and easy  pivots occur in more than five documents in each domain. We set k, the number of singular vectors of the Table 1: Top pivots selected by SCL, but not SCL-weight matrix, to 50.  MI (left) and vice-versa (right)  Experiments with SCL and SCL-MI  differences between the two methods for pivot selec-Each labeled dataset was split into a training set of tion when adapting a classifier from books to kitchen 1600 instances and a test set of 400 instances. All appliances. We refer throughout the rest of this work the experiments use a classifier trained on the train-to our method for selecting pivots as SCL-MI.  ing set of one domain and tested on the test set of 3  a possibly different domain. The baseline is a linear classifier trained without adaptation, while the We constructed a new dataset for sentiment domain  gold standard is an in-domain classifier trained on adaptation by selecting Amazon product reviews for the same domain as it is tested.  four different product types: books, DVDs, electron-Figure 1 gives accuracies for all pairs of domain  ics and kitchen appliances. Each review consists of adaptation.  The domains are ordered clockwise  a rating (0-5 stars), a reviewer name and location, from the top left: books, DVDs, electronics, and  a product name, a review title and date, and the re-kitchen. For each set of bars, the first letter is the view text. Reviews with rating > 3 were labeled source domain and the second letter is the target  positive, those with rating < 3 were labeled neg-domain. The thick horizontal bars are the  accuraative, and the rest discarded because their polarity cies of the in-domain classifiers for these domains.  was ambiguous. After this conversion, we had 1000  Thus the first set of bars shows that the baseline positive and 1000 negative examples for each do-achieves 72.8% accuracy adapting from DVDs to  main, the same balanced composition as the polarity books. SCL-MI achieves 79.7% and the in-domain  dataset (Pang et al., 2002). In addition to the labeled gold standard is 80.4%. We say that the adaptation data, we included between 3685 (DVDs) and 5945  loss for the baseline model is 7.6% and the  adapta(kitchen) instances of unlabeled data. The size of the tion loss for the SCL-MI model is 0.7%. The relative unlabeled data was limited primarily by the number reduction in error due to adaptation of SCL-MI for of reviews we could crawl and download from the  this test is 90.8%.  Amazon website. Since we were able to obtain  laWe can observe from these results that there is a  bels for all of the reviews, we also ensured that they rough grouping of our domains. Books and DVDs  were balanced between positive and negative  examare similar, as are kitchen appliances and electron-ples, as well.  ics, but the two groups are different from one  anWhile the polarity dataset is a popular choice in  other. Adapting classifiers from books to DVDs, for the literature, we were unable to use it for our task.  instance, is easier than adapting them from books  Our method requires many unlabeled reviews and  to kitchen appliances. We note that when  transferdespite a large number of IMDB reviews available  ring from kitchen to electronics, SCL-MI actually  online, the extensive curation requirements made  outperforms the in-domain classifier. This is possi-preparing a large amount of data difficult 3.  ble since the unlabeled data may contain information For classification, we use linear predictors on un-that the in-domain classifier does not have access to.  igram and bigram features, trained to minimize the At the beginning of Section 2 we gave exam-Huber loss with stochastic gradient descent (Zhang, ples of how features can change behavior across domains. The first type of behavior is when predictive 3For a description of the construction of the polarity features from the source domain are not predictive dataset, see http://www.cs.cornell.edu/people/  or do not appear in the target domain. The second is  Figure 1: Accuracy results for domain adaptation between all pairs using SCL and SCL-MI. Thick black lines are the accuracies of in-domain classifiers.  domain\polarity  plot < num> pages predictable reader grisham engaging  reading this page < num>  must read fascinating  the plastic poorly designed  leaking awkward to defective  are perfect years now a breeze  Table 2: Correspondences discovered by SCL for books and kitchen appliances. The top row shows features that only appear in books and the bottom features that only appear in kitchen appliances. The left and right columns show negative and positive features in correspondence, respectively.  when predictive features from the target domain do Table 2 illustrates one row of the projection ma-not appear in the source domain. To show how SCL  trix for adapting from books to kitchen appliances; deals with those domain mismatches, we look at the the features on each row appear only in the corre-adaptation from book reviews to reviews of kitchen sponding domain. A supervised classifier trained on appliances. We selected the top 1000 most infor-book reviews cannot assign weight to the kitchen  mative features in both domains. In both cases, be-features in the second row of table 2.  In  contween 85 and 90% of the informative features from  trast, SCL assigns weight to these features indirectly one domain were not among the most informative  through the projection matrix. When we observe  of the other domain4. SCL addresses both of these  the feature predictable with a negative book  reissues simultaneously by aligning features from the view, we update parameters corresponding to the  two domains.  entire projection, including the kitchen-specific features poorly designed and awkward to .  4There is a third type, features which are positive in one domain but negative in another, but they appear very infrequently in our datasets.  While some rows of the projection matrix are  base  base  tures. This causes problems when a projection is  discriminative in the source domain but not in the dvd  target. This is the case for adapting from kitchen electron  appliances to books.  Since the book domain is  quite broad, many projections in books model topic distinctions such as between religious and political Table 3: For each domain, we show the loss due to transfer for each method, averaged over all domains. The bottom row books.  These projections, which are  uninformashows the average loss over all runs.  tive as to the target label, are put into correspondence with the fewer discriminating projections in the much narrower kitchen domain. When we adapt  we show adaptation from only the two domains on  from kitchen to books, we assign weight to these un-which SCL-MI performed the worst relative to the  informative projections, degrading target classifica-supervised baseline. For example, the book domain  tion accuracy.  shows only results from electronics and kitchen, but not DVDs. As a baseline, we used the label of the  Correcting Misalignments  source domain classifier as a feature in the target, but did not use any SCL features. We note that the base-We now show how to use a small amount of target  line is very close to just using the source domain domain labeled data to learn to ignore misaligned  classifier, because with only 50 target domain  inprojections from SCL-MI. Using the notation of  stances we do not have enough data to relearn all of Ando and Zhang (2005), we can write the supervised the parameters in w. As we can see, though, relearn-training objective of SCL on the source domain as  ing the 50 parameters in v is quite helpful. The cor-X  rected model always improves over the baseline for i + v0 xi, yi  every possible transfer, including those not shown in the figure.  where y is the label. The weight vector w  The idea of using the regularizer of a linear model weighs the original features, while v  to encourage the target parameters to be close to the the projected features. Ando and Zhang (2005) and  source parameters has been used previously in  doBlitzer et al. (2006) suggest = 10 4, = 0, which main adaptation. In particular, Chelba and Acero  we have used in our results so far.  (2004) showed how this technique can be effective  Suppose now that we have trained source model  for capitalization adaptation. The major difference weight vectors ws and vs. A small amount of tar-between our approach and theirs is that we only pe-get domain data is probably insufficient to signif-nalize deviation from the source parameters for the icantly change w, but we can correct v, which is  weights v of projected features, while they work  much smaller. We augment each labeled target  inwith the weights of the original features only. For stance xj with the label assigned by the source do-our small amount of labeled target data, attempting main classifier (Florian et al., 2004; Blitzer et al., to penalize w using ws performed no better than  2006). Then we solve  our baseline. Because we only need to learn to  ignore projections that misalign features, we can make w,v  much better use of our labeled data by adapting only  50 parameters, rather than 200,000.  Since we don't want to deviate significantly from the Table 3 summarizes the results of sections 4 and  source parameters, we set = = 10 1.  5. Structural correspondence learning reduces the  Figure 2 shows the corrected SCL-MI model  userror due to transfer by 21%. Choosing pivots by  ing 50 target domain labeled instances. We chose  mutual information allows us to further reduce the this number since we believe it to be a reasonable error to 36%. Finally, by adding 50 instances of tar-amount for a single engineer to label with minimal get domain data and using this to correct the mis-effort. For reasons of space, for each target domain aligned projections, we achieve an average relative  Figure 2: Accuracy results for domain adaptation with 50 labeled target domain instances.  reduction in error of 46%.  (sets on which a linear classifier returns positive value). Then the A distance between two probability 6  distributions is  Sections 2-5 focused on how to adapt to a target do-dA(D, D0) = 2 sup |PrD [A] PrD0 [A]| .  main when you had a labeled source dataset. We  now take a step back to look at the problem of  seThat is, we find the subset in A on which the distri-lecting source domain data to label. We study a set-butions differ the most in the L1 sense. Ben-David ting where an engineer knows roughly her domains  et al. (2006) show that computing the A-distance for of interest but does not have any labeled data yet. In a finite sample is exactly the problem of minimiz-that case, she can ask the question Which sources ing the empirical risk of a classifier that discrimi-should I label to obtain the best performance over nates between instances drawn from D and instances all my domains? On our product domains, for ex-drawn from D0. This is convenient for us, since it al-ample, if we are interested in classifying reviews lows us to use classification machinery to compute of kitchen appliances, we know from sections 4-5  the A-distance.  that it would be foolish to label reviews of books or DVDs rather than electronics. Here we show how to  Unlabeled Adaptability Measurements  select source domains using only unlabeled data and We follow Ben-David et al. (2006) and use the Hu-the SCL representation.  ber loss as a proxy for the A-distance. Our  procedure is as follows: Given two domains, we compute  The A-distance  the SCL representation. Then we create a data set  We propose to measure domain adaptability by  uswhere each instance x is labeled with the identity ing the divergence of two domains after the SCL  of the domain from which it came and train a linear projection. We can characterize domains by their  classifier. For each pair of domains we compute the induced distributions on instance space: the more  empirical average per-instance Huber loss, subtract different the domains, the more divergent the distri-it from 1, and multiply the result by 100. We refer butions. Here we make use of the A-distance (Ben-to this quantity as the proxy A-distance. When it is David et al., 2006). The key intuition behind the  100, the two domains are completely distinct. When A-distance is that while two domains can differ in it is 0, the two domains are indistinguishable using a arbitrary ways, we are only interested in the differ-linear classifier.  ences that affect classification accuracy.  Figure 3 is a correlation plot between the proxy  Let A be the family of subsets of  A-distance and the adaptation error. Suppose we  ing to characteristic functions of linear classifiers wanted to label two domains out of the four in such a  a completely unsupervised manner. Then he  classified documents according to various functions of 12  BK  these mutual information scores. We stress that our 10  method improves a supervised baseline. While we  do not have a direct comparison, we note that  Turney (2002) performs worse on movie reviews than  on his other datasets, the same type of data as the Adaptation Loss  We also note the work of Aue and Gamon (2005),  who performed a number of empirical tests on  domain adaptation of sentiment classifiers. Most of  these tests were unsuccessful. We briefly note their Figure 3: The proxy A-distance between each do-results on combining a number of source domains.  main pair plotted against the average adaptation loss They observed that source domains closer to the tar-of as measured by our baseline system. Each pair of get helped more. In preliminary experiments we  domains is labeled by their first letters: EK indicates confirmed these results. Adding more labeled data  the pair electronics and kitchen.  always helps, but diversifying training data does not.  When classifying kitchen appliances, for any fixed way as to minimize our error on all the domains. Us-amount of labeled data, it is always better to draw ing the proxy A-distance as a criterion, we observe from electronics as a source than use some combi-that we would choose one domain from either books  nation of all three other domains.  or DVDs, but not both, since then we would not be  Domain adaptation alone is a generally  wellable to adequately cover electronics or kitchen appli-studied area, and we cannot possibly hope to cover ances. Similarly we would also choose one domain  all of it here. As we noted in Section 5, we are  from either electronics or kitchen appliances, but not able to significantly outperform basic structural cor-both.  respondence learning (Blitzer et al., 2006). We also note that while Florian et al. (2004) and Blitzer et al.  Related Work  (2006) observe that including the label of a source Sentiment classification has advanced considerably classifier as a feature on small amounts of target data since the work of Pang et al. (2002), which we use tends to improve over using either the source alone as our baseline. Thomas et al. (2006) use discourse or the target alone, we did not observe that for our structure present in congressional records to perform data. We believe the most important reason for this more accurate sentiment classification. Pang and  is that they explore structured prediction problems, Lee (2005) treat sentiment analysis as an ordinal  where labels of surrounding words from the source  ranking problem. In our work we only show  imclassifier may be very informative, even if the cur-provement for the basic model, but all of these new rent label is not. In contrast our simple binary pre-techniques also make use of lexical features. Thus diction problem does not exhibit such behavior. This we believe that our adaptation methods could be also may also be the reason that the model of Chelba and applied to those more refined models.  Acero (2004) did not aid in adaptation.  While work on domain adaptation for  sentiFinally we note that while Blitzer et al. (2006) did ment classifiers is sparse, it is worth noting that combine SCL with labeled target domain data, they  other researchers have investigated unsupervised  only compared using the label of SCL or non-SCL  and semisupervised methods for domain adaptation.  source classifiers as features, following the work of The work most similar in spirit to ours that of Tur-Florian et al. (2004). By only adapting the  SCLney (2002). He used the difference in mutual  inrelated part of the weight vector v, we are able to formation with two human-selected features (the  make better use of our small amount of unlabeled  words excellent and poor ) to score features in data than these previous techniques.  Conclusion  Anthony Aue and Michael Gamon. 2005.  Customizing sentiment classifiers to new domains: a case study.  Sentiment classification has seen a great deal of at-http://research.microsoft.com/ anthaue/.  tention. Its application to many different domains Shai Ben-David, John Blitzer, Koby Crammer, and Fer-of discourse makes it an ideal candidate for domain nando Pereira. 2006. Analysis of representations for adaptation.  This work addressed two important  domain adaptation. In Neural Information Processing questions of domain adaptation. First, we showed  that for a given source and target domain, we can  significantly improve for sentiment classification the 2006. Domain adaptation with structural correspon-structural correspondence learning model of Blitzer dence learning. In Empirical Methods in Natural Language Processing (EMNLP)  et al. (2006). We chose pivot features using not only  common frequency among domains but also mutual  Ciprian Chelba and Alex Acero. 2004. Adaptation of information with the source labels. We also showed maximum entropy capitalizer: Little data can help a how to correct structural correspondence misalign-lot. In EMNLP.  ments by using a small amount of labeled target do-Sanjiv Das and Mike Chen. 2001. Yahoo! for  amazon: Extracting market sentiment from stock message Second, we provided a method for selecting those  boards. In Proceedings of Athe Asia Pacific Finance Association Annual Conference.  source domains most likely to adapt well to given  target domains. The unsupervised A-distance  meaR. Florian, H. Hassan, A.Ittycheriah, H. Jing, N. Kamb-sure of divergence between domains correlates well hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A  statistical model for multilingual entity detection and with loss due to adaptation. Thus we can use the A-tracking. In of HLT-NAACL.  distance to select source domains to label which will give low target domain error.  stars when there aren't many stars: Graph-based semi-In the future, we wish to include some of the more supervised learning for sentiment categorization. In recent advances in sentiment classification, as well HLT-NAACL 2006 Workshop on Textgraphs: Graph-as addressing the more realistic problem of  rankbased Algorithms for Natural Language Processing.  ing. We are also actively searching for a larger and Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting more varied set of domains on which to test our tech-class relationships for sentiment categorization with niques.  respect to rating scales. In Proceedings of Association for Computational Linguistics.  Acknowledgements  2002. Thumbs up? sentiment classification using ma-We thank Nikhil Dinesh for helpful advice  throughchine learning techniques. In Proceedings of Empiri-out the course of this work. This material is based cal Methods in Natural Language Processing.  upon work partially supported by the Defense  AdMatt Thomas, Bo Pang, and Lillian Lee. 2006. Get out vanced Research Projects Agency (DARPA) un-the vote: Determining support or opposition from conder Contract No. NBCHD03001.  gressional floor-debate transcripts. In Empirical Meth-findings, and conclusions or recommendations  exods in Natural Language Processing (EMNLP).  pressed in this material are those of the authors and Peter Turney. 2002. Thumbs up or thumbs down? se-do not necessarily reflect the views of DARPA or  mantic orientation applied to unsupervised classifica-the Department of Interior-National BusinessCenter tion of reviews.  In Proceedings of Association for  Computational Linguistics.  Tong Zhang. 2004. Solving large scale linear prediction problems using stochastic gradient descent al-References  gorithms.  In International Conference on Machine  Learning (ICML).  A framework for  learning predictive structures from multiple tasks and unlabeled data. JMLR, 6:1817 1853. 