 Maximum Entropy Models for FrameNet Classification Michael Fleischman, Namhee Kwon and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way  NP, PP), and their grammatical function (e.g.,  external argument, object argument). Figure 1 shows  an example of an annotated sentence and its  approThe development of FrameNet, a large  database of semantically annotated  sentences, has primed research into statistical  methods for semantic tagging. We  advance previous work by adopting a  Maximum Entropy approach and by using  Agent Body Part  previous tag information to find the  highest probability tag sequence for a given  She clapped her hands in inspiration.  sentence. Further we examine the use of  sentence level syntactic pattern features to  -Comp  increase performance. We analyze our  Figure 1. Frame for lemma 'clap' shown with three  strategy on both human annotated and  core frame elements and a sentence annotated with  eleautomatically identified frame elements,  ment type, phrase type, and grammatical function.  and compare performance to previous  As of its first release in June 2002, FrameNet  work on identical test data. Experiments  has made available 49,000 annotated sentences.  The release contains 99,000 annotated frame  eleprovement (p<0.01) of over 6%.  verbs, 339 nouns, and 175 adjectives).  While considerable in scale, the FrameNet  da1 Introduction  tabase does not yet approach the magnitude of  reRecent work in the development of FrameNet, a sources available for other NLP tasks. Each target large database of semantically annotated sentences, predicate, for example, has on average only 30 sen-has laid the foundation for statistical approaches to tences tagged. This data sparsity makes the task of the task of automatic semantic classification.  learning a semantic classifier formidable, and  inThe FrameNet project seeks to annotate a large creases the importance of the modeling framework subset of the British National Corpus with seman-that is employed.  tic information. Annotations are based on Frame  Semantics (Fillmore, 1976), in which frames are 2 Related Work defined as schematic representations of situations  To our knowledge, Gildea and Jurafsky (2002)  involving various frame elements such as  particiis the only work to use FrameNet to build a  statispants, props, and other conceptual roles.  tically based semantic classifier. They split the  In each FrameNet sentence, a single target problem into two distinct sub-tasks: frame element predicate is identified and all of its relevant frame identification and frame element classification. In elements are tagged with their semantic role (e.g., the identification phase, syntactic information is Agent, Judge), their syntactic phrase type (e.g., extracted from a parse tree to learn the boundaries  of the frame elements in a sentence. In the classi-ment into one of 120 semantic role categories. The  fication phase, similar syntactic information is boundaries of each frame element are given based used to classify those elements into their semantic on the human annotations in FrameNet. In Section roles.  4, experiments are performed using automatically  In both phases Gildea and Jurafsky (2002) identified frame elements.  build a model of the conditional probabilities of the  classification given a vector of syntactic features. 3.1 Features The full conditional probability is decomposed into For each frame element, features are extracted simpler conditional probabilities that are then in-from the surface text of the sentence and from an  terpolated to make the classification. Their best automatically generated syntactic parse tree performance on held out test data is achieved using (Collins, 1997). The features used are described a linear interpolation model:  below:  Target predicate (tar): Although there may  be many predicates in a sentence with  associwhere r is the class to be predicted, x is the vector ated frame elements, classification operates on  of syntactic features, x  only one target predicate at a time. The target  i is a subset of those  feapredicate is the only feature that is not  exi is the weight given to that subset  conditional probability (as determined using the EM  tracted from the sentence itself and must be  algorithm), and m is the total number of subsets  given by the user. Note that the frame which  used. Using this method, they report a test set  acthe target predicate instantiates is not given,  curacy of 78.5% on classifying semantic roles and  leaving any word sense ambiguities to be  hanprecision/recall scores of .726/.631 on frame  eledled implicitly by the classifier. 2  Phrase type (pt): The syntactic phrase type of  We extend Gildea and Jurafsky (2002)'s initial  the frame element (e.g. NP, PP) is extracted  effort in three ways. First, we adopt a maximum  from the parse tree of the sentence by finding  entropy (ME) framework in order to learn a more  the constituent in the tree whose boundaries  accurate classification model. Second, we include  match the human annotated boundaries of the  features that look at previous tags and use previous  element. In cases where there exists no  contag information to find the highest probability  sestituent that perfectly matches the element, the  mantic role sequence for a given sentence. Finally,  constituent is chosen which matches the largest  text span of the element and has the same  leftmore global information in order to classify frame  most boundary.  elements. We compare the results of our classifier Syntactic head (head): The syntactic heads of to that of Gildea and Jurafsky (2002) on matched  the frame elements are extracted from the  test sets of both human annotated and  automatically identified frame elements.  scribed above) using a heuristic method  described by Michael Collins. 3 This method 3 Semantic Role Classification  extracts the syntactic heads of constituents;  thus, for example, the second frame element in  Training (36,993 sentences / 75,548 frame  eleFigure 1 has head 'hands,' while the third  frame element has head 'in.'  frame elements), and held out test sets (3,865  sen Logical Function (lf): A simplification of the  tences / 7,899 frame elements) were obtained in  grammatical function annotation (see section  order to exactly match those used in Gildea and  1) is extracted from the parse tree. Unlike the  Jurafsky (2002)1. In the experiments presented below, features are extracted for each frame ele-2  ment in a sentence and used to classify that  eleBecause of the interaction of head word features with the  target predicate, we suspect that ambiguous lexical items do not account for much error. This question, however, will be 1 Data sets (including parse trees) were obtained from Dan addressed explicitly in future work.  Table 1. Feature sets used in ME frame element classifier. Shows individual feature sets, example feature function from that set, and total number of feature functions in the set. Examples taken from frame element  'in inspiration,' shown in Figure 1.  Feature Set  Number of Functions  in Feature Set  f(CAUSE, 'clap')=1  f(CAUSE, 'clap', PP)=1  f(r, tar, pt, lf)  f(CAUSE, 'clap', PP, other)=1  f(CAUSE, NP, 'clap', active)=1  f(r, pt, pos, voice ,tar) f(CAUSE, PP, after, active, 'clap')=1  f(CAUSE, 'in')=1  f(CAUSE, 'in', 'clap')=1  f(r, head, tar, pt)  f(CAUSE, 'in', 'clap', PP)=1  f(CAUSE, 2,  [NP-Ext,Target,NP-Obj,PP-other])=1  f(CAUSE, 'clap', 2,  [NP-Ext,Target,NP-Obj,PP-other])=1  Total Number of Features:  full grammatical function, the lf can have only  These syntactic patterns can be highly  inone of three values: e xternal argument, object  formative for classification. For example, in  argument, other. A node is considered an e  xthe training data, a syntactic pattern of  [NPternal argument if it is an ancestor of an S  ext, target, NP-obj] given the predicate bend  node, an o bject argument if it is an ancestor of  was associated 100% of the time with the  a VP node, and other for all other cases. This  feature is only applied to frame elements  whose phrase type is NP.  Position (pos): The position of the frame  eleoccur in isolation, but rather, depend very  ment relative to the target ( before, after) is  exmuch on the other elements in a sentence.  tracted based on the surface text of the  This dependency can be exploited in  classification by using the semantic roles of previously  Voice (voice): The voice of the sentence (  acclassified frame elements as features in the  tive, passive) is determined using a simple  classification of a current element. This  stratregular expression passed over the surface text  egy takes advantage of the fact that, for  examof the sentence.  ple, if a frame element is tagged as an AGENT  Order (order): The position of the frame  eleit is highly unlikely that the next element will  ment relative to the other frame elements in the  also be an AGENT.  sentence. For example, in the sentence from  The previous role feature indicates the  Figure 1, the element 'She' has order=0, while  classification that the previous frame  ele'in inspiration' has order=2.  ment received. During training, this  informa Syntactic pattern (pat): The sentence level  tion is provided by simply looking at the true  syntactic pattern of the sentence is generated  classes of the frame element occurring n  posiby looking at the phrase types and logical  tions before the target element. During testing,  functions of each frame element in the  senhypothesized classes of the n elements are used  tence. For example, in the sentence:  'Alexanand Viterbi search is performed to find the  dra bent her head;' 'Alexandra' is an external  argument Noun Phrase, 'bent' is a target  predicate, and 'her head' is an object  argument Noun Phrase. Thus, the syntactic pattern ME models implement the intuition that the best associated with the sentence is [NP-ext, target, model will be the one that is consistent with the set NP-obj].  of constrains imposed by the evidence, but  otherwise is as uniform as possible (Berger et al., 1996). 3.3 Experiments We model the probability of a semantic role r  given a vector of features x according to the ME We present three experiments in which different formulation below:  feature sets are used to train the ME classifier. The  first experiment uses only those feature  combinations described in Gildea and Jurafsky (2002)  (feaZ exp[ f ( r, x)]  ture sets 0-7 from Table 1). The second  experiment uses a super set of the first and  incorx is a normalization constant, fi(r,x) is a  feature function which maps each role and vector porates the syntactic pattern features described element (or combination of elements) to a binary above (feature sets 0-9). The final experiment uses value, n is the total number of feature functions, the previous tags and implements Viterbi search to find the best tag sequence (feature sets 0-11).  and i is the weight for a given feature function.  The final classification is just the role with highest  We further investigate the effect of varying two  probability given its feature vector and the model.  aspects of classifier training: the standard deviation  of the Gaussian priors used for smoothing, and the  The feature functions that we employ can be number of sentences used for training. To examine divided into feature sets based upon the types and the effect of optimizing the standard deviation, a combinations of features on which they operate. range of values was chosen and a classifier was Table 1 lists the feature sets that we use, as well as trained using each value until performance on a the number of individual feature functions they development set ceased to improve.  contain. The feature combinations were chosen  based both on previous work and trial and error. In  To examine the effect of training set size on  future work we will examine more principled  feaperformance, five data sets were generated from  the original set with 36, 367, 3674, 7349, and  24496 sentences, respectively. These data sets  It is important to note that the feature functions were created by going through the original set and described here are not equivalent to the subset selecting every thousandth, hundredth, tenth, fifth, conditional distributions that are used in the Gildea and every second and third sentence, respectively.  models in which feature functions map specific  instances of syntactic features and classes to binary  Classifier Performance on Test Set  values (e.g., if a training element has head='in'  and role=C  AUSE, then, for that element, the feature  function f(  CAUSE, 'in') will equal 1). Thus, ME is  not here being used as another way to find weights  ct 82  for an interpolated model. Rather, the ME  aporre  proach provides an overarching framework in  which the full distribution of semantic roles given  syntactic features can be modeled.  We train the ME models using the GIS  algorithm (Darroch and Ratcliff, 1972) as implemented  Figure 2. Performance of models on test data using  in the YASMET ME package (Och, 2002). We hand annotated frame element boundaries. G&J refers use the YASMET MEtagger (Bender et al., 2003) to the results of Gildea and Jurafsky (2002). Exp 1 in-to perform the Viterbi search. The classifier was corporates feature sets 0-7 from Table 1; Exp 2 feature trained until performance on the development set sets 0-9; Exp 3 features 0-11.  ceased to improve. Feature weights were  smoothed using Gaussian priors with mean 0 3.4 Results  (Chen and Rosenfeld, 1999). The standard  deviaFigure 2 shows the results of our experiments  tion of this distribution was optimized on the  dealongside those of (Gildea and Jurafsky, 2002) on  velopment set for each experiment.  identical held out test sets. The difference in  performance between each classifier is statistically  significant at (p<0.01) (Mitchell, 1997), with the  exception of Exp 2 and Exp 3, whose difference is classification benefits from the ME model's bias statistically significant at (p<0.05).  for more uniform probability distributions that  satisfy the constraints placed on the model by the  Table 2. Effect of different smoothing parameter (std. training data.  dev.) values on classification performance.  Another reason for improved performance comes  % Correct  from ME's simpler design. Instead of having to  worry about finding proper backoff strategies  amongst distributions of features subsets, ME  allows one to include many features in a single  Table 2 shows the effect of varying the  stanmodel and automatically adjusts the weights of  dard deviation of the Gaussian priors used for these features appropriately.  smoothing in Experiment 1. The difference in  performance between the classifiers trained using Table 3. Confusion matrix for five roles which contrib-ute most to overall system error. Columns refer to  acstandard deviation 1 and 2 is statistically  signifitual role. Rows refer to the model's hypothesis. Other  refers to combination of all other roles.  Area Spkr Goal Msg Path Other Prec.  Path  Other 15 21 26  Also, because the ME models find weights for  # Sentences in Training  many thousands of features, they have many more  Figure 3. Effect of training set size on semantic role degrees of freedom than the linear interpolated classification.  models of Gildea and Jurafsky. Although many  degrees of freedom can lead to overfitting of the  Figure 3 shows the change in performance as a training data, the smoothing procedure employed function of training set size. Classifiers were in our experiments helps to counteract this prob-trained using the full set of features described for lem. As evidenced in Table 2, by optimizing the Experiment 3.  standard deviation used in smoothing the ME  Table 3 shows the confusion matrix for a subset models are able to show significant increases in of semantic roles. Five roles were chosen for pres-performance on held out test data.  entation based upon their high contribution to  clasFinally, by including in our model  sentencesifier error. Confusion between these five account level pattern features and information about previ-for 27% of all errors made amongst the 120  possious classes, global information can be exploited for  ble roles. The tenth role, other, represents the sum improved classification. The accuracy gained by of the remaining 115 roles. Table 4 presents ex-including such global information confirms the  ample errors for five of the most confused roles.  intuition that the semantic role of an element is  much related to the entire sentence of which it is a  It is clear that the ME models improve  performHaving discussed the advantages of the models  ance on frame element classification. There are a presented here, it is interesting to look at the errors number of reasons for this improvement.  that the system makes. It is clear from the  confuFirst, for this task the log-linear model employed sion matrix in Table 3 that a great deal of the sys-in the ME framework is better than the linear tem error comes from relatively few semantic interpolation model used by Gildea and Jurafsky.  One possible reason for this is that semantic role  roles.4 Table 4 offers some insight into why these more data. The slope of the curve indicates that errors occur. For example, the confusions exem-we are far from a plateau, and that even constant  plified in 1 and 2 are both due to the fact that the increases in the amount of available training data particular phrases employed can be used in multi-may push classifier performance above 90%  accuple roles (including the roles hypothesized by the racy.  system). Thus, while 'across the counter' may be  Having demonstrated the effectiveness of the  considered a goal when one is talking about a  perME approach on frame element classification  son and their head, the same phrase would be  congiven hand annotated frame element boundaries,  sidered a path if one were talking about a mouse we next examine the value of the approach given who is running.  automatically identified boundaries.  Table 4. Example errors for five of the most often  confused semantic roles  Actual Proposed Example  Gildea and Jurafsky equate the task of locating  Path  The  craned his head  frame element boundaries to one of identifying  across the counter.  Path  frame elements amongst the parse tree constituents  throwing books around the  of a given sentence. Because not all frame element  boundaries exactly match constituent boundaries,  this approach can perform no better than 86.9%  ber, opposition being voiced  by a number of Italian and  (i.e. the number of elements that match  constituSpanish prelates.  ents (6864) divided by the total number of  ele4 Addressee Speaker Furious staff claim they were  ments (7899)) on the test set.  even called in from holiday to  be grilled by a specialist  secu4.1 Features  rity firm  5 Reason  Evaluee We cannot but admire the  Frame element identification is a binary  classificaefficiency with which she  tion problem in which each constituent in a parse  took control of her own life.  tree is described by a feature vector and, based on  Examples 3 and 4, while showing phrases with that vector, tagged as either a frame element or not.  similar confusions, stand out as being errors caused In generating feature vectors we use a subset of the by an inability to deal with passive sentences. features described for role tagging as well as an Such errors are not unexpected; for, even though additional path feature.  the voice of the sentence is an explicit feature, the  system suffers from the paucity of passive  sentences in the data (approximately 5%).  Finally, example 5 shows an error that is based  on the difficult nature of the decision itself (i.e., it  is unclear whether 'the efficiency' is the reason for  admiration, or what is being admired). Often  times, phrases are assigned semantic roles that are  not obvious even to human evaluators. In such  cases it is difficult to determine what information  might be useful for the system.  Having looked at the types of errors that are  common for the system, it becomes interesting to  Figure 4. Generation of path features used in frame examine what strategy may be best to overcome element tagging. The path from the constituent 'in in-such errors. Aside from new features, one solution spiration' to the target predicate 'clapped' is repre-is obvious: more data. The curve in Figure 2 sented as the string PP VP VBD.  shows that there is still a great deal of performance  to be gained by training the current ME models on  Gildea and Jurafsky introduce the path feature  in order to capture the structural relationship  between a constituent and the target predicate. The  44% of all error is due to confusion between only nine roles.  Table 5. Results of frame element identification. G&J represents results reported in (Gildea and Jurafsky, 2002), ME results for the experiments reported here. The second column shows precision, recall, and F-scores for the task of frame element identification, the third column for the combined task of identification and classification.  FE ID only  FE ID + FE Classification  G&J Boundary id + baseline role labeler .726  ME Boundary id + ME role labeler  path of a constituent is represented by the nodes 100%. Precision is calculated as the number of through which one passes while traveling up the correct positive classifications divided by the num-tree from the constituent and then down through ber of total positive classifications.  the governing category to the target. Figure 4  The difference in the F-scores on the  identificashows an example of this feature for a frame  eletion task alone and on the combined task are  statisment from the sentence presented in Figure 1.  tically significant at the (p<0.01) level 5 . The  accuracy of the ME semantic classifier on the  automatically identified frame elements is 81.5%,  We use the ME formulation described in Section not a statistically significant difference from its 3.2 to build a binary classifier. The classifier fea-performance on hand labeled elements, but a  statistures follow closely those used in Gildea and  Juraftically significant difference from the classifier of  sky. We model the data using the feature sets: f(fe, Gildea and Jurafsky (2002) (p<0.01).  path), f(fe, path, tar), and f(fe, head, tar), where fe represents the binary classification of the constitu-0.8  ent. While this experiment only uses three feature  sets, the heterogeneity of the path feature is so  great that the classifier itself uses 1,119,331 unique  binary features.  With the constituents having been labeled, we  apply the ME frame element classifier described  above. Results are presented using the classifier of  Experiment 1, described in section 3.3. We then  investigate the effect of varying the number of  constituents used for training on identification  per# Constituents in Training  formance. Five data sets of approximately 100,000 Figure 5. Effect of training set size on frame element 10,000, 1,000, and 100 constituents were generated boundary identification.  from the original set by random selection and used  to train ME models as described above.  Figure 5 shows the results of varying the  training set size on identification performance. For  each data set, thresholds were chosen to maximize  Table 5 compares the results of Gildea and  Jurafsky (2002) and the ME frame element identifier on 4.4 Discussion both the task of frame element identification alone,  and the combined task of frame element  identificaIt is clear from the results above that the  performtion and classification. In order to be counted  corance of the ME model for frame element  classificarect on the combined task, the constituent must tion is robust to the use of automatically identified have been correctly identified as a frame element, frame element boundaries. Further, the ME  and then must have been correctly classified into  one of the 120 semantic categories.  5 G&J's results for the combined task were generated with a Recall is calculated based on the total number threshold applied to the FE classifier (Dan Gildea, personal of frame elements in the test set, not on the total communication). This is why their precision/recall scores are number of elements that have matching parse con-dissimilar to their accuracy scores, as reported in section 3.  Because the ME classifier does not employ a threshold,  comstituents. Thus, the upper limit is 86.9%, not parisons must be based on F-score.  framework yields better results on the frame ele-Acknowledgments  ment identification task than the simple linear  interpolation model of Gildea and Jurafsky. This The authors would like to thank Dan Gildea who result is not surprising given the discussion in Sec-generously allowed us access to his data files and  tion 3.  What is striking, however, is the drastic overall available. Finally, we thank Franz Och whose help reduction in performance on the combined and expertise was invaluable.  identification and classification task. The References  bottleneck here is the identification of frame  element boundaries. Unlike with classification O. Bender, K. Macherey, F. J. Och, and H. Ney.  though, Figure 5 indicates that a plateau in the  2003. Comparison of Alignment Templates and  learning curve has been reached, and thus, more  data will not yield as dramatic an improvement for  guage Processing. Proc. of EACL-2003.  Budathe given feature set and model.  5 Conclusion  1996. A Maximum Entropy Approach to  Natural Language Processing. Computational  LinThe results reported here show that ME models  provide higher performance on frame element  classification tasks, given both human and  automatiS. F. Chen and R. Rosenfeld. 1999. A Gaussian  cally identified frame element boundaries, than the  prior for smoothing maximum entropy models.  linear interpolation models examined in previous  work. We attribute this increase to the benefits of  Mellon University  the ME framework itself, the incorporation of  senM. Collins. 1997. Three generative, lexicalized  tence-level syntactic patterns into our feature set,  models for statistical parsing. Proc. of the 35th  and the use of previous tag information to find the  Annual Meeting of the ACL. pages 16-23,  Mamost probable sequence of roles for a sentence.  But perhaps most striking in our results are the  effects of varying training set size on the  performJ. N. Darroch and D. Ratcliff. 1972. Generalized  ance of the classification and identification models.  iterative scaling for log-linear models. Annals  While for classification, the learning curve appears  of Mathematical Statistics, 43:1470-1480.  to be still increasing with training set size, the C. Fillmore 1976. Frame semantics and the nature learning curve for identification appears to have  of language. Annals of the New York Academy  already begun to plateau. This suggests that while  of Sciences: Conference on the Origin and  Declassification will continue to improve as the  Fravelopment of Language and Speech, Volume  meNet database gets larger, increased performance  on identification will rely on the development of  more sophisticated models.  D. Gildea and D. Jurafsky. 2002. Automatic  LaIn future work, we intend to apply the lessons  beling of Semantic Roles, Computational  Linlearned here to the problem of frame element  identification. Gildea and Jurafsky have shown that T. Mitchell. 1997. Machine Learning. McGraw-improvements in identification can be had by more  Hill International Editions, New York, NY.  closely integrating the task with classification (they  report an F-Score of .719 using an integrated  model). We are currently exploring a ME  apF.J. Och. 2002. Yet another maxent toolkit:  proach which integrates these two tasks under a  YASMET.  www-i6.informatik.rwthtagging framework. Initial results show that  sigaachen.de/Colleagues/och/.  nificant improvements can be had using techniques  similar to those described above.  Introduction  Related Work  Semantic Role Classification Features  Frame Element Identification Features FE ID only  FE ID + FE Classification  Conclusion 