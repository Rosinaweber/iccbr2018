 Crystal: Analyzing Predictive Opinions on the Web Soo-Min Kim and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way, Marina del Rey, CA 90292  first category Judgment Opinions and the second Abstract  (those discussing the future) Predictive Opinions.  Judgment opinions express positive or negative  In this paper, we present an election  predicsentiment about a topic such as, for example,  retion system ( Crystal) based on web users'  views about cameras, movies, books, or hotels, and  opinions posted on an election prediction  discussions about topics like abortion and war. In  website. Given a prediction message,  Cryscontrast, predictive opinions express a person's  tal first identifies which party the message  opinion about the future of a topic or event such as  predicts to win and then aggregates  predicthe housing market, a popular sports match, and  tion analysis results of a large amount of  national election, based on his or her belief and  opinions to project the election results. We  knowledge.  collect past election prediction messages  Due to the different nature of these two  categofrom the Web and automatically build a  ries of opinion, each has different valences.  Judggold standard. We focus on capturing  leximent opinions have core valences of positive and  cal patterns that people frequently use  negative. For example, liking a product and  when they express their predictive opinions  supporting abortion have the valence positive  about a coming election. To predict  electoward each topic (namely a product and  abortion results, we apply SVM-based  supertion ). Predictive opinions have the core valence of  vised learning. To improve performance,  likely or unlikely predicated on the event. For ex-we propose a novel technique which  generample, a sentence Housing prices will go down  soon carries the valence of likely for the event  tal results show that Crystal significantly  of housing prices go down .  outperforms several baselines as well as a  The two types of opinions can co-appear. The  non-generalized n-gram approach. Crystal  sentence I like Democrats but I think they are not  predicts future elections with 81.68 %  acculikely to win considering the war issue contains  both types of opinion: positive valence towards  Democrats and unlikely valence towards the  1 Introduction  event of Democrats wins . In order to accurately  identify and analyze each type of opinion, different  As a growing number of people use the Web as a  medium for expressing their opinions, the Web is  Note that our work is different from predictive  becoming a rich source of various opinions in the  data mining which models a data mining system  form of product reviews, travel advice, social issue  using statistical approaches in order to forecast the  discussions, consumer complaints, stock market future or trace a pattern of interest (Rickel and Por-predictions, real estate market predictions, etc.  ter, 1997; Rodionov and Martin, 1996). Example  At least two categories of opinions can be  idendomains of predictive data mining include  earthtified. One consists of opinions such as I quake prediction, air temperature prediction, for-like/dislike it , and the other consists of opinions  eign exchange prediction, and energy price  prediclike It is likely/unlikely to happen. We call the  Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 1056 1064, Prague, June 2007. c 2007 Association for Computational Linguistics  tion. However, predictive data mining is only fea-reports empirical evidence that Crystal  outpersible when a large amount of structured numerical  forms several baseline systems. Finally, Section 6  data (e.g., in a database) is available. Unlike this  concludes with a description of the impact of this  research area which analyzes numeric values, our  study mines unstructured text using NLP  techniques and it can potentially extend the reach of  2 Related Work  This work is closely related to opinion analysis and  Despite the vast amount of predictive opinions  text classification. Most research on opinion  analyand their potential applications such as  identificasis in computational linguistics has focused on  sention and analysis of people's opinions about the timent analysis, subjectivity detection, and review real estate market or a specific country's economic  mining. Pang et al. (2002) and Turney (2002)  clasfuture, studies on predictive opinions have been sified sentiment polarity of reviews at the docu-neglected in Computational Linguistics, where ment level. Wiebe et al. (1999) classified sentence most previous work focuses on judgment opinions  level subjectivity using syntactic classes such as  (see Section 2). In this paper, we concentrate on  adjectives, pronouns and modal verbs as features.  identifying predictive opinion with its valence.  Riloff and Wiebe (2003) extracted subjective  exAmong many prediction domains on the Web,  pressions from sentences using a bootstrapping  we focus on election prediction and introduce pattern learning process. Wiebe et. al (2004) and Crystal, a system to predict election results using Riloff et. al (2005) adopted pattern learning with  the public's written viewpoints. To build our  syslexical feature generalization for subjective  exprestem, we collect opinions about past elections sion detection. Dave et. al (2003) and Jindal and posted on an election prediction project website Liu (2006) also learned patterns of opinion expres-before the election day, and build a corpus1. We  sion in product reviews. Yu and Hatzivassiloglou  then use this corpus to train our system for  analyz(2003) identified the polarity of opinion sentences  ing predictive opinion messages and, using this, to  using semantically oriented words. These  techpredict the election outcome. Due to the  availabilniques were applied and examined in different  doity of actual results of the past elections, we can  mains, such as customer reviews (Hu and Liu  not only evaluate how accurately Crystal analyzes 2004; Popescu et al., 2005) and news articles (Kim  prediction messages (by checking agreement with  and Hovy, 2004; Wilson et al., 2005).  the gold standard), but also objectively measure the  In text classification, systems typically use  bagprediction accuracy of our system.  of-words models, mostly with supervised learning  The main contributions of this work are as  folalgorithms using Naive Bayes or Support Vector  Machines (Joachims, 1998) to classify documents  an NLP technique for analyzing predictive  into several categories such as sports, art, politics,  opinions in the electoral domain;  a method of automatically building a corpus  (2005) address the difficulty of obtaining training  of predictive opinions for a supervised corpora for supervised learning and propose unsu-learning approach; and  pervised learning approaches. Another recent  re a feature generalization technique that  outlated classification task focuses on academic and  performs all the baselines on the task of commercial efforts to detect email spam messages.  identifying a predicted winning party given  For an SVM-based approach, see (Drucker et al.,  a predictive opinion.  1999). In our study, we explore the use of  generalThe rest of this paper is structured as follows.  ized lexical features for predictive opinion analysis  Section 2 surveys previous work. Section 3  forand compare it with the bag-of-words approach.  Crystal with  proModeling Prediction  posed feature generalization algorithm. Section 5 In this section, we define the task of analyzing predictive opinions in the electoral domain.  1 The resulting corpus is available at  http://www.isi.edu/ ~skim/Download/Data/predictive.htm  Predicted  Riding Year  winning party  Party_3 Riding_206 2004  Party_2 Riding_206 2004  Party_2 Riding_189 2006  Party_1 Riding_189 2006  Party_2 Riding_189 2006  Party_1 Riding_46  Table 1. A snapshot of the processed data  Party  Table 2. An example of our Party-Candidate  listing for a riding (PC: Progressive  ConservaFigure 1 illustrates an overview of our election  Figure 1. Our election prediction system. Public  prediction system Crystal in action. Given each  opinions are collected from message boards (a)  and our system determines for each the election  www.election prediction.org) as seen in Figure 1.a,  prediction Party' and Valence' (b). The output  a system can determine a Party that the author of a of the system is a prediction of the election out-document thinks to win or lose ( Valence), Figure  1.b. For the example document starting with the  sentence I think this riding will stay NDP as it has 3.1  for the past 11 years. in Figure 1.a, our predictive  opinion analysis system aims to recognize NDP as  We model predictive opinions in an election as Party and WIN as Valence. After aggregating the follows:  predictive opinion analysis results of all  documents, we project the election results in Figure 1.c.  The following section describes how we obtain our  where Party is a political party running for an elec-data set and the subsequent sections describe  Crystion (e.g., Democrats and Republicans) and  Valence is the valence of a predictive opinion which can be either likely to win ( WIN) or unlikely to 3.2 Automatically Labeled Data  win ( LOSE). Values for Party vary depending on We collected messages posted on an election pre-in which year (e.g., 1996 and 2006) and where an  diction project page, www.electionprediction. org.  election takes place (e.g., United States, France, or  The website contains various election prediction  Japan). The unit of a predictive opinion is an  unprojects (e.g., provincial election, federal election,  structured textual document such as an article in a  and general election) of different countries (e.g.,  personal blog or a message posted on a news group  Canada and United Kingdom) from 1999 to 2006.  discussion board about the topic of Which party For our data set, we downloaded Canadian federal  do you think will win/lose in this election? .  election prediction data for 2004 and 2006. The  Canadian federal electoral system is based on 308  ridings (electoral districts). The website contains 308 separate html files of messages corresponding  1 for each message M with a party that M  predicts to win, P  to the 308 ridings for different years. In total, we  collected 4858 and 4680 messages for the 2004  2 for each sentence Si in a message M  and 2006 federal elections respectively. On  aver3 for each party Pj in Si  age, a message consists of 98.8 words.  To train and evaluate our system, we require a  gold standard for each message (i.e., which party  does an author of a message predict to win?). One  6 Generate S'ij by substituting Pj with  option is to hire human annotators to build the gold  standard. Instead, we used an online party logo  7 and all other parties in Si with OTHER  image file that the author of each message already  labeled for the message. Note that authors only  select parties they think will win, which means our  Table 3. Feature generalization algorithm  gold standard only contains a party with WIN  valence of each message. However, we leverage this  information to build a system which is able to  deSVM result integration3. Crystal generates general-termine a party even with LOSE valence. We  deized sentences in the feature generalization step.  scribe this idea in detail in Section 4.  Then it classifies each sentence using generalized  Finally, we pre-processed the data by converting  lexical features in order to determine Valence of the downloaded html source files into a structured  Party in a sentence. Finally, it combines results of format with the following fields: message, party,  sentences to determine Valence and Party of a riding, and year, where message is a text, party is a message. Note that the classification using SVM is  winning party predicted in the text, riding is one of an intermediate step conducting a binary classifica-the 308 ridings, and year is either 2004 or 2006.  tion (i.e., WIN or LOSE) for the final multi-class  Table 1 shows a snapshot of the processed data set  classification in result integration. The following  that we used for our system training and  evaluasections describe each step.  tion. An additional piece of information consisting  of a candidate's name for each party for each riding  was also stored in our data set. With this  informaIn the feature generalization step, we generalize  tion, the system can infer opinions about a party  patterns of words used in predictive opinions. For  based on opinions about candidates who run for the  example, instead of using three different trigrams  party. Table 2 shows an example of a riding.  like Liberals will win , NDP will win , and 4 Analyzing Predictions  Conservatives will win , we generalize these to  PARTY will win . The assumption is that the In this section we describe Crystal. One simple generalized patterns can represent better the rela-approach could be a system (see NGR system in  tionship among Party, Valence, and words  surSection 5) trained by a machine learning technique  rounding Party (e.g., will win) than pure lexical  using n-gram features and classifying a message patterns. For this algorithm, we first substitute a into multiple classes (e.g., NDP, Liberal, or Pro-candidate's name (both the first name and the last  gressive). However, we develop a more  sophistiname) with the political party name that the  candicated algorithm and compare its result with several  date belongs to (see Table 2). We then break each  baselines, including the simple n-gram method2.  Experimental results in Section 5 show that Crystal  Table 3 outlines the feature generalization  algooutperforms all the baselines.  rithm. Here, our approach is that if a message  preOur approach consists of three steps: feature  generalization, classification using SVMs, and  3 feature indicates n-grams in our corpus that we use in the SVM classification step.  4 The sentence breaker that we used is available at  2 N-gram approach is often unbeatable (and therefore great) in http://search.cpan.org/ ~shlomoy/Lingua-EN-sentence  many text classification tasks.  ble 3). Then it generates a generalized sentence by  substituting NDP with PARTY and Liberal with  OTHER (Lines 6 7). It returns (NDP, -1, PARTY  will barely take this riding from OTHER ). For  Liberal, on the other hand, the algorithm  determines its Valence as +1 since Liberal is the same  as the predicted winning party of the message.  After similar generalization, it returns (Liberal, +1,  OTHER will barely take this riding from  Note that the final result of the feature  generalization algorithm is a set of triplets: (Party,  Valence, Generalized Sentence). Among a triplet, we  use (Valence, Generalized Sentence) to produce  feature vectors for a machine learning algorithm  (see Section 4.2) and (Party, Valence) to integrate  system results of each sentence for the final  decision of Party and Valence of a message (see  Section 4.3). Figure 2 shows an example of the  algoFigure 2. An example of feature generalization  4.2 Classification Using SVMs  of a message  In this step, we use Support Vector Machines  dicts a particular party to win, sentences which (SVMs) to train our system using the generalized mention that party in the message also imply that it  features described in Section 4.1. After we  obwill win. Conversely all other parties are assumed  tained examples of (Valence, Generalized  Sento be in sentences that imply they will lose. As tence) in the feature generalization step, we mod-shown in Section 3.2, a message ( M) in our corpus eled a subtask of classifying a Generalized Sen-has a label of a party ( P ) that the author of  tence into Valence towards our final goal of  deterdicts to win. After breaking sentences in M, we mining (Valence, Party) of a message. This subtask duplicate a sentence by the number of unique paris a binary classification since Valence has only 2  ties in the sentence and modify the duplicated  senclasses: +1 and -16. Given a generalized sentence  tences by substituting the party names with OTHER will barely take this riding from PARTY and OTHER in order to generalize fea-PARTY in Figure 2, for example, the goal of our tures.  system is to learn WIN valence for PARTY. Fea-Consider the following sentence:  tures for SVMs are extracted from generalized  sen Dockrill will barely take this riding from tences. We implemented our SVM learning model  using the SVM light package7.  which gets re-written as:  NDP will barely take this riding from Liberal  4.3 SVM Result Integration  because Dockrill is an NDP candidate and Rodger  In this step, we combine the valence of each  senCuzner is a Liberal candidate. Since the sentence  tence predicted by SVMs to determine the final  contains two parties (i.e., NDP and Liberal), the valence and predicted party of a message. For each algorithm duplicates the sentence twice, once for  party mentioned in a message, we calculate the  each party (see Lines 4 8 in Table 3)5. For NDP,  sum of the party's valences of each sentence and  the algorithm determines its Valence as -1 because  NDP is not equal to the predicted winning party  (i.e., Liberal) of the message (see Lines 4 5 in  Ta6 However, the final evaluation of the system and all the  baselines is equally performed on the multi-classification results of messages.  5 In the feature generalization algorithm, we represent  7 SVM light is available from http://svmlight.joachims.  WIN and LOSE valence as +1 and -1.  pick a party that has the maximum value. This in-step of the feature generalization algorithm).  Integration algorithm can be represented as follows:  stead, it blindly picks the party that appeared most  in a message.  MJR: This system marks all messages with the  where p is one of parties mentioned in a message, most dominant predicted party in the entire data  m is the number of sentences that contains party p set. In our corpus, Conservatives was the majority  in a message, and Valence  party (3480 messages) followed closely by Liberal  k(p) is the valence of p in  the kth sentence that contains p. Given the example (3473 messages).  in Figure 2, the Liberal party appears twice in  sen INC: This system chooses the incumbent party  tence S0 and S1 and its total valence score is +2,  as the predicted winning party of a message. (This  whereas the NDP party appears once in sentence  is a strong baseline since incumbents often win in  S1 and its valence sum is -1. As a result, our  algoCanadian politics). For example, since the  incumrithm picks liberal as the winning party that the bent party of the riding Blackstrap in 2004 was message predicts.  Conservative, all the messages about Blackstrap in  2004 were marked Conservative as their predicted  winning party by this system.  This section reports our experimental results  show JDG: This system uses judgment opinion words  ing empirical evidence that Crystal outperforms as its features for SVM. For our list of judgment several baseline systems.  opinion words, we use General Inquirer which is a publicly available list of 1635 positive and nega-5.1 Experimental Setup  tive sentiment words (e.g., love, hate, wise, dumb,  Our corpus consists of 4858 and 4680 messages etc.)10.  from 2004 and 2006 Canadian federal election  prediction data respectively described in detail in  Section 3.2. We split our pre-processed corpus into 10  We measure the system performance with its  accufolds for cross-validation. We implemented the racy in two different ways: accuracy per message following five systems to compare with Crystal 8.  ) and accuracy per riding (  ). Both  Accriding  accuracies are represented as follows:  NGR: In this algorithm, we train the system  using SVM with n-gram features without the  generof  the system  correctly  labled  alization step described in Section 4.19. The  reTotal of  placement of each candidate's first and last name  of  ridings the system  correctly  predicted  by his or her party name was still applied.  Total # of  FRQ: This system picks the most frequently We first report the results with Accmessage in mentioned party in a message as the predicted Evaluation1 and then report with Accriding in winning party. Party name substitution is also ap-Evaluation2.  plied. For example, given a message This riding Evaluation1: Table 4 shows accuracies of base-will go liberal. Dockrill will barely take this riding  lines and Crystal. We calculated accuracy for each from Rodger Cuzner. , all candidates' names are test set in 10-fold data sets and averaged it. Among replaced by party names (i.e., This riding will go the baselines, MJR performed worst (36.48 %).  Liberal. NDP will barely take this riding from  LibBoth FRQ and INC performed around 50 %  eral. ). After name replacement, the system picks (54.82 % and 53.29 % respectively). NGR achieved Liberal as an answer because Liberal appears twice  its best score (62.02 %) when using unigram,  biwhereas NDP appears only once. Note that, unlike  gram, and trigram features together (uni+bi+tri).  Crystal, this system does not consider the valence We also experimented with other feature combina-of each party (as done in our sentence duplication  tions (see Table 5). Our system achieved 73.07 %  which is 11 % higher than NGR and around 20 %  8 In our experiments using SVM, we used the linear kernel for all Crystal, NGR, and JDG.  9 This system is exactly like Crystal without the feature gener-10 Available at http://www.wjh.harvard.edu/~inquirer  alization and result integration steps.  /homecat.htm  Accmessage (%) Accriding (%)  Patterns in WIN class  Patterns in LOSE class  PARTY_will_win want_OTHER  PARTY_hold PARTY_don't_have  INC 53.29  PARTY_will_win_this OTHER_and  PARTY_win the_PARTY  PARTY_will_take OTHER_is  PARTY_will_take_this to_the_OTHER  Table 4. System performance with accuracy  PARTY_is and_OTHER  ) and accuracy per  riding ( Acc  ): FRQ, MJR, INC, NGR,  PARTY_has OTHER_has  JDG, and Crystal.  Table 6. Examples of frequent features in  Accmessage (%)  Features  WIN and LOSE classes.  NGR Crystal  lower than both s  ystems. This indicates that the  features are not as good as the  same number of sentiment words carefully chosen  four 47.25  from a dictionary but the generalized features of  Crystal represent the predictive opinions better  than JDG features.  Table 5 illustrates the comparison of NGR  (without feature generalization) and Crystal (with bi + four  posed technique Crystal performed always better  than the pure n-gram system (NGR). Both systems  performed best (62.02 % and 73.07 %) with the uni + bi + tri + four  combination of unigram, bigram, and trigram  Table 5. System performance with different  (uni+bi+tri). The second best scores (61.96 % and  features: Pure n-gram (NGR) and  General73.01 %) are achieved with the combinations of all ized n-gram Crystal.  grams (uni+bi+tri+four) in both systems. Using  fourgrams alone performed worst since the system  higher than FRQ and INC. The best accuracy of  overfitted to the training examples.  our system was also obtained with the combination  Table 6 presents several examples of frequent  nof unigram, bigram, and trigram features.  gram features in both WIN and LOSE classes. As  The JDG system, which uses positive and  negashown in Table 6, lexical patterns in the WIN class  tive sentiment word features, had 66.23 %  accuexpress optimistic sentiments about PARTY (e.g.,  racy. This is about 7% lower than Crystal. Since  the lower performance of JDG might be related to  whereas patterns in the LOSE class express  pessithe number of features it uses, we also  experimistic sentiments (e.g., PARTY_don't_have) and  mented with the reduced number of features of optimistic ones about OTHER (e.g., Crystal based on the tfidf scores11. With the same want_OTHER).  number of features (i.e., 1635), Crystal performed Evaluation2: In this evaluation, we use Acc  70.62% which is 4.4% higher than JDG. An  intercomputed as the number of ridings that a system  esting finding was that NGR with 1635 features correctly predicted, divided by the total number of performed only 54.60% which is significantly ridings. For each riding R, systems pick a party  that obtains the majority prediction votes from  11 The total number of all features of Crystal is 689,642.  messages in R as the winning party of R. For ex-1062  ample, if Crystal identified 9 messages predicting (e.g., judgment opinion) can help identify the other  for Conservative Party, 3 messages for NDP, and 1  (e.g., predictive opinion). Combining two types of  message for Liberal among 13 messages in the  ridopinion features and testing on each domain can  ing Blackstrap , the system will predict that the examine this issue.  Conservative Party would win in Blackstrap .  In our experiment, we used General Inquirer  Table 4 shows the system performance with  Acwords as judgment opinion indicators for JDG  . Note that people who write messages on a  baseline system. It might be interesting to employ  particular web site are not a random sample for different resources for judgment words such as the prediction. So we introduce a measure of confi-polarity lexicon by Wilson et al. (2005) and the  dence ( ConfidenceScore) of each system and use  recently released SentiWordNet12.  the prediction results when the ConfidenceScore is Our work is an initial step towards analyzing a  higher than a threshold. Otherwise, we use a  denew type of opinion. In the future, we plan to  infault party (i.e., the incumbent party) as the  wincorporate more features such as priors like  incumning party. ConfidenceScore of a riding R is calcu-bent party in addition to the lexical features to  imlated as follows:  prove the system performance.  ConfidenceScore = countmessage(Pfirst) countmes-7 Conclusions  sage(Psecond)  where  In this paper, we proposed a framework for  workcountmessage(Px) is the number of messages  that predict a party  ing with predictive opinion. Previously, research-Px to win, Pfirs t is the party that  the most number of messages predict to win, and  ers in opinion analysis mostly focused on judgment  opinions which express positive or negative  sentiis the party that the second most number of  second  messages predict to win.  ment about a topic, as in product reviews and  polWe used 62 ridings to tune the  icy discussions. Unlike judgment opinions,  predicparameter arriving at the value of 4. As shown in  tive opinions express a person's opinion about the  Table 4, the system which just considers the  infuture of a topic or event such as the housing  marcumbent party (INC) performed fairly well ket, a popular sports match, and election results, (78.03  based on his or her belief and knowledge. Among  % accuracy) because incumbents are often  re-elected in Canadian elections. The upper bound  these many kinds of predictive opinions, we  foof this prediction task is 88.85  cused on election prediction.  % accuracy which is  the prediction result using numerical values of a  We collected past election prediction data from  prediction survey. FRQ and MJR performed an election prediction project site and automati-63.14  cally built a gold standard. Using this data, we  % and 36.63 % respectively. Similarly to  Evaluation1, JDG which only uses judgment word  modeled the election prediction task using a  superfeatures performed worse than both  vised learning approach, SVM. We proposed a  Crystal and  NGR. Also,  novel technique which generalized n-gram feature  Crystal with our feature generalization  algorithm performed better than NGR with  nonpatterns. Experimental results showed that this  apgeneralized n-gram features. The accuracy of  proach outperforms several baselines as well as a  non-generalized n-gram approach. This is  signifital (81.68 %) is comparable to the upper bound 88.85  cant because an n-gram model without  generalization is often extremely competitive in many text  classification tasks.  This work adopts NLP techniques for predictive  In this section, we discuss possible extensions and  opinions and it sets the foundation for exploring a  improvements of this work.  whole new subclass of the opinion analysis  probOur experiment focuses on investigating aspects  lems. Potential applications of this work are  sysof predictive opinions by learning lexical patterns  tems that analyze various kinds of election  predicand comparing them with judgment opinions. tions by monitoring texts in discussion boards and However, this work can be extended to investigat-personal blogs. In the future, we would like to  ing how those two types of opinions are related to  each other and whether lexical features of one  model predictive opinions in other domains such as Riloff, E., Wiebe, J. and Wilson, T. 2003. Learning Sub-the real estate market and the stock market which  jective Nouns Using Extraction Pattern  Bootstrapwould require further exploration of system design  ping. Proc. of CoNLL 2003. pp 25 32.  and data collection.  Rodionov, S. and Martin, J. H. 1996. A  KnowledgeBased System for the Diagnosis and Prediction of  Reference  Short-Term Climatic Changes in the North Atlantic,  Journal of Climate, 9(8)  Engelmore, R., and Morgan, A. eds. 1986. Blackboard  Systems. Reading, Mass.: Addison-Wesley.  Turney, P. 2002. Thumbs Up or Thumbs Down?  Semantic Orientation Applied to Unsupervised  ClassifiDave, K., Lawrence, S. and Pennock, D. M. 2003.  Mincation of Reviews. Proc. of ACL 2002, pp 417 424.  ing the peanut gallery: Opinion extraction and  semantic classification of product reviews. Proc. of  Wiebe, J., Bruce, R. and O'Hara, T. 1999. Development  World Wide Web Conference 2003  and use of a gold standard data set for subjectivity  classifications. Proc. of ACL 1999, pp 246 253.  tor machines for spam categorization. IEEE Trans.  Wiebe, J., Wilson, T. , Bruce, R. , Bell , M. and Martin, Neural Netw., 10, pp 1048 1054.  M. Learning Subjective Language. 2004.  Computational Linguistics  gating Unsupervised Learning for Text  CategorizaWilson, T., Wiebe, J. and Hoffmann, P. 2005.  Recogtion Bootstrapping, Proc. of EMNLP 2005. Vancou-nizing Contextual Polarity in Phrase-Level Sentiment  Analysis. Proc. of HLT/EMNLP 2005.  Hu, M. and Liu, B. 2004. Mining and summarizing  cusYu, H. and Hatzivassiloglou, V. 2003. Towards  Antomer reviews. Proc. Of KDD-2004, Seattle,  Washswering Opinion Questions: Separating Facts from  ington, USA.  Opinions and Identifying the Polarity of Opinion  Sentences. Proc. of EMNLP 2003.  Jindal, N. and Liu, B. 2006. Mining Comprative  Sentences and Relations. Proc. of 21st National  ConferJoachims, T. 1998. Text categorization with support  vector machines: Learning with many relevant  features, Proc. of ECML, p. 137 142.  Kim, S-M. and Hovy, E. 2004. Determining the  Sentiment of Opinions. Proc. of COLING 2004.  Liu, B., Li, X., Lee, W. S. and Yu, P. S. Text Classification by Labeling Words Proc. of AAAI-2004, San  Pang, B, Lee, L. and Vaithyanathan, S. 2002. Thumbs  up? Sentiment Classification using Machine Learning  Techniques. Proc. of EMNLP 2002.  Features and Opinions from Reviews, Proc. of  HLTRickel, J. and Porter, B. 1997. Automated Modeling of  Complex Systems to Answer Prediction Questions,  Artificial Intelligence Journal, volume 93, numbers 1-2, pp. 201 260  Riloff, E., Wiebe, J., and Phillips, W. 2005. Exploiting  Subjectivity Classification to Improve Information  Extraction, Proc. of the 20th National Conference on 