 Topic Identification for Fine-Grained Opinion Analysis  Veselin Stoyanov and Claire Cardie  Department of Computer Science  Cornell University  the form How/what does entity X feel/think about  topic Y? , for which document-level opinion  analWithin the area of general-purpose  fineysis methods can be problematic.  grained subjectivity analysis, opinion topic  identification has, to date, received little  Fine-grained subjectivity analyses typically  attention due to both the difficulty of the  identify SUBJECTIVE EXPRESSIONS in context,  charactask and the lack of appropriately  annoterize their POLARITY (e.g. positive, neutral or  negtated resources. In this paper, we  proative) and INTENSITY (e.g. weak, medium, strong,  vide an operational definition of opinion  extreme), and identify the associated SOURCE, or  topic and present an algorithm for opinion  OPINION HOLDER, as well as the TOPIC, or TARGET, of  topic identification that, following our new  the opinion. While substantial progress has been  definition, treats the task as a problem in  made in automating some of these tasks, opinion  topic coreference resolution. We develop a  topic identification has received by far the least  atmethodology for the manual annotation of  tention due to both the difficulty of the task and the  opinion topics and use it to annotate topic  lack of appropriately annotated resources.1  information for a portion of an existing  This paper addresses the problem of topic  idengeneral-purpose opinion corpus. In  expertification for fine-grained opinion analysis of  geniments using the corpus, our topic  identieral text.2 We begin by providing a new,  operafication approach statistically significantly  tional definition of opinion topic in which the topic  outperforms several non-trivial baselines  of an opinion depends on the context in which  according to three evaluation measures.  its associated opinion expression occurs. We also  present a novel method for general-purpose  opin1 Introduction  ion topic identification that, following our new  definition, treats the problem as an exercise in topic  Subjectivity analysis is concerned with coreference resolution. We evaluate the approach ing information about attitudes, beliefs, emotions, using the existing MPQA corpus (Wiebe et al., opinions, evaluations, sentiment and other private 2005), which we extend with manual annotations states expressed in texts. In contrast to the that encode topic information (and refer to here-lem of identifying subjectivity or sentiment at the after as the  MPQATOPIC corpus).  (2002)), we are interested in fine-grained  subjecInter-annotator agreement results for the manual  tivity analysis, which is concerned with annotations are reasonably strong across a num-tivity at the phrase or clause level. We expect ber of metrics and the results of experiments that fine-grained subjectivity analysis to be useful for evaluate our topic identification method in the con-question-answering, summarization, information text of fine-grained opinion analysis are promising: extraction and search engine support for queries of  1Section 3 on related work provides additional discussion.  Licensed under the Creative Commons  2The identification of products and their components and  Attribution-Noncommercial-Share Alike 3.0 Unported  liattributes from product reviews is a related, but quite different  task from that addressed here. Section 3 briefly discusses, and  Some rights reserved.  provides references, to the most relevant research in that area.  Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 817 824  using either automatically or manually identified surface form comprising the contents of the opin-topic spans, we achieve topic coreference scores ion.  that statistically significantly outperform two topic  In Example 1, for instance, Marseille is both  segmentation baselines across three coreference the TOPIC SPAN and the TARGET SPAN associated with resolution evaluation measures (B3, and CEAF). the city of Marseille, which is the TOPIC of the opin-For the B3 metric, for example, the best ion. In Example 2, the TARGET SPAN consists of the line achieves a topic coreference score on the text that comprises the complement of the subjec-MPQATOPIC corpus of 0.55 while our topic  coreftive verb thinks . Example 2 illustrates why  opinerence algorithm scores 0.57 and 0.71 using ion topic identification is difficult: within the sin-tomatically, and manually, identified topic spans, gle target span of the opinion, there are multiple respectively.  potential topics, each identified with its own topic  In the remainder of the paper, we define span. Without more context, however, it is impos-ion topics (Section 2), present related work sible to know which phrase indicates the intended tion 3), and motivate and describe the key idea topic. If followed by sentence 3, however, of topic coreference that underlies our methodology for both the manual and automatic (3)Although he doesn't like government-imposed taxes, he tion of opinion topics (Section 4). Creation of thinks that a fuel tax is the only effective solution.  the MPQA  the topic of Al's opinion in 2 is much clearer it  TOPIC corpus is described in Section 5  and our topic identification algorithm, in Section 6. is likely to be fuel tax, denoted via the TOPIC SPAN  The evaluation methodology and results are tax gas or tax .  sented in Sections 7 and 8, respectively.  3 Related Work  2 Definitions and Examples  As previously mentioned, there has been much  recent progress in extracting fine-grained  subjectivConsider the following opinion sentences:  ity information from general text. Previous efforts  have focused on the extraction of opinion  expresOH John] adores [TARGET+TOPIC SPAN Marseille] and  visits it often.  sions in context (e.g. Bethard et al. (2004), Breck  et al. (2007)), the assignment of polarity to these  (2)[OH Al] thinks that [TARGET SPAN [TOPIC SPAN? the  expressions (e.g. Wilson et al. (2005), Kim and  government] should [TOPIC SPAN? tax gas] more in order to  Hovy (2006)), source extraction (e.g. Bethard et  [TOPIC SPAN? curb [TOPIC SPAN? CO2 emissions]]].  al. (2004), Choi et al. (2005)), and identification of  A fine-grained subjectivity analysis should the source-expresses-opinion relation (e.g. Choi et tify: the OPINION EXPRESSION3 as adores in al. (2006)), i.e. linking sources to the opinions that ple 1 and thinks in Example 2; the POLARITY as they express.  positive in Example 1 and neutral in Example 2;  Not surprisingly, progress has been driven by  the INTENSITY as medium and low, respectively; and the creation of language resources. In this regard, the OPINION HOLDER (OH) as John and Al , Wiebe et al.'s (2005) opinion annotation scheme spectively. To be able to discuss the opinion TOPIC  for subjective expressions was used to create the  in each example, we begin with three definitions:  MPQA corpus, which consists of 535 documents  Topic. The TOPIC of a fine-grained opinion is manually annotated for phrase-level expressions of the real-world object, event or abstract entity that is opinions, their sources, polarities, and intensities.  the subject of the opinion as intended by the Although other opinion corpora exist (e.g. Bethard ion holder.  Topic span. The TOPIC SPAN associated with an product review corpora of Liu4), we are not aware OPINION EXPRESSION is the closest, minimal span of  of any corpus that rivals the scale and depth of the  text that mentions the topic.  Target span. In contrast, we use  In the related area of opinion extraction from  TARGET SPAN  to denote the span of text that covers the syntactic product reviews, several research efforts have focused on the extraction of the topic of the  opin3For simplicity, we will use the term opinion throughout  the paper to cover all types of private states expressed in  sub4http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html  Popescu and Etzioni (2005), Hu and Liu (2004)). assume that fragments of text (e.g. sentences or For this specialized text genre, it has been sequences of words of a fixed length) with sim-ficient to limit the notion of topic to mentions ilar lexical distribution are about the same topic; of product names and components and their the goal of these methods is to find the boundaries tributes. Thus, topic extraction has been where the lexical distribution changes (e.g. Choi tively substituted with a lexicon look-up and (2000), Malioutov and Barzilay (2006)). Opin-niques have focused on how to learn or acquire an ion topic identification differs from topic segmen-appropriate lexicon for the task. While the tation in that opinion topics are not necessarily spa-niques have been very successful for this genre tially coherent there may be two opinions in of text, they have not been applied outside the the same sentence on different topics, as well as product reviews domain. Further, there are opinions that are on the same topic separated by yses (Wiebe et al., 2005) and experiments (Wilson opinions that do not share that topic. Nevertheless, et al., 2005) that indicate that lexicon-lookup we will compare our topic identification approach proaches to subjectivity analysis will have limited to a state-of-the-art topic segmentation algorithm success on general texts.  (Choi, 2000) in the evaluation.  Outside the product review domain, there has  Other work has successfully adopted the use of  been little effort devoted to opinion topic clustering to discover entity relations by identify-tion. The MPQA corpus, for example, was ing entities that appear in the same sentence and inally intended to include topic annotations, but clustering the intervening context (e.g. Hasegawa the task was abandoned after confirming that it et al. (2004), Rosenfeld and Feldman (2007)). This was very difficult (Wiebe, 2005; Wilson, 2005), work, however, considers named entities and heads although target span annotation is currently of proper noun phrases rather than topic spans, way. While useful, target spans alone will be and the relations learned are those commonly held ficient for many applications: they neither contain between NPs (e.g. senator-of-state, city-of-state, information indicating which opinions are about chairman-of-organization) rather than a more gen-the same topic, nor provide a concise textual eral coreference relation.  resentation of the topics.  4 A Coreference Approach to Topic  Due to the lack of appropriately annotated  corpora, the problem of opinion topic extraction has  been largely unexplored in NLP. A notable Given our initial definition of opinion topics (Section is the work of Kim and Hovy (2006). They tion 2), the next task is to determine which com-propose a model that extracts opinion topics for putational approaches might be employed for au-subjective expressions signaled by verbs and tomatic opinion topic identification. We begin this jectives. Their model relies on semantic frames exercise by considering some of the problematic and extracts as the topic the syntactic constituent characteristics of opinion topics.  at a specific argument position for the given verb Multiple potential topics. As noted earlier via or adjective. In other words, Kim and Hovy extract Example 2, a serious problem in opinion topic what we refer to as the target spans, and do so for identification is the mention of multiple potential a subset of the opinion-bearing words in the text. topics within the target span of the opinion. Al-Although on many occasions target spans coincide though an issue for all opinions, this problem is with opinion topics (as in Example 1), we have typically more pronounced in opinions that do not served that on many other occasions this is not the carry sentiment (as in Example 2). Our current case (as in Example 2). Furthermore, hampered by definition of opinion topic requires the NLP sys-the lack of resources with manually annotated tem (or a human annotator) to decide which of the gets, Kim and Hovy could provide only a limited entities described in the target span, if any, refers evaluation.  to the intended topic. This decision can be aided  As we have defined it, opinion topic by the following change to our definition of opin-tion bears some resemblance to topic ion topic, which introduces the idea of a context-tion, the goal of which is to partition a text into dependent information focus: the TOPIC of an opin-a linear sequence of topically coherent segments. ion is the real-world entity that is the subject of the Existing methods for topic segmentation typically opinion as intended by the opinion holder based 819  on the discourse context.  scribes the opinion topic that covers all opinions in the cluster.  With this modified definition in hand, and given 5. The annotator marks the TOPIC SPAN of each opinion.  Example 3 as the succeeding context for Example (This can be done at any point in the process.) 2, we argue that the intended subject, and hence  the TOPIC, of Al's opinion in 2 can be quickly The manual annotation procedure is de-tified as the FUEL TAX, which is denoted by the TOPIC  scribed in a set of instructions available at  SPANS tax gas in 2 and fuel tax in 3.  http://www.cs.cornell.edu/ ves. In addition, we  created a GUI that facilitates the annotation  proceOpinion topics not always explicitly mentioned. dure. With the help of these resources, one person In stark contrast to the above, on many occasions annotated opinion topics for a randomly selected the topic is not mentioned explicitly at all within set of 150 of the 535 documents in the MPQA the target span, as in the following example:  corpus. In addition, 20 of the 150 documents were  selected at random and annotated by a second  (5)[OH John] identified the violation of Palestinian human  annotator for the purposes of an inter-annotator  rights as one of the main factors.  TOPIC:  ISRAELIagreement study, the results of which are presented  in Section 8.1. The MPQATOPIC and the procedure  We have further observed that the opinion topic by which it was created are described in more is often not mentioned within the same paragraph detail in (Stoyanov and Cardie, 2008).  and, on a few occasions, not even within the same  document as the opinion expression.  6 The Topic Coreference Algorithm  As mentioned in Section 4, our computational  ap4.1 Our Solution: Topic Coreference  proach to opinion topic identification is based on  With the above examples and problems in mind, topic coreference: For each document (1) find the we hypothesize that the notion of topic clusters of coreferent opinions, and (2) label the ence will facilitate both the manual and automatic clusters with the name of the topic. In this paper identification of opinion topics: We say that two we focus only on the first task, topic coreference opinions are topic-coreferent if they share the resolution the most critical step for topic identi-same opinion topic. In particular, we fication. We conjecture that the second step can be ture that judging whether or not two opinions are performed through frequency analysis of the terms topic-coreferent is easier than specifying the topic in each of the clusters and leave it for future work.  of each opinion (due to the problems described  Topic coreference resolution resembles another  well-known problem in NLP noun phrase (NP)  coreference resolution. Therefore, we adapt a  5 Constructing the MPQA  Corpus  standard machine learning-based approach to NP  TOPIC  coreference resolution (Soon et al., 2001; Ng and  Relying on the notion of topic coreference, we next Cardie, 2002) for our purposes. Our adaptation has introduce a new methodology for the manual three steps: (i) identify the topic spans; (ii) perform tation of opinion topics in text:  pairwise classification of the associated opinions  1. The annotator begins with a corpus of documents that  as to whether or not they are topic-coreferent; and,  has been annotated w.r.t. OPINION EXPRESSIONS. With  (iii) cluster the opinions according to the results of  each opinion expression, the corpus provides POLARITY and  (ii). Each step is discussed in more detail below.  OPINION HOLDER information. (We use the aforementioned  6.1 Identifying Topic Spans  2. The annotator maintains a list of the opinion expressions  that remain to be annotated (initially, all opinion expressions  Decisions about topic coreference should depend  in the document) as well as a list of the current groupings (i.e.  on the text spans that express the topic. Ideally,  clusters) of opinion expressions that have been identified as  topic-coreferent (initially this list is empty).  we would be able to recover the topic span of each  opinion and use its content for the topic  corefer3. For each opinion expression, in turn, the annotator decides  whether the opinion is on the same topic as the opinions in  ence decision. However, the topic span depends on  one of the existing clusters or should start a new cluster, and  the topic itself, so it is unrealistic that topic spans  inserts the opinion in the appropriate cluster.  can be recovered with simple methods.  Neverthe4. The annotator labels each cluster with a string that  deless, in this initial work, we investigate two  simple methods for automatic topic span identification Positional features These features are intended and compare them to two manual approaches:  to exploit the fact that opinions that are close to  each other are more likely to be on the same topic.  Sentence. Assume that the topic span is the We use six positional features: whole sentence containing the opinion.  Same Sentence/Paragraph5 True if the two  Automatic. A rule-based method for  identiopinions are in the same sentence/paragraph.  fying the topic span (developed using MPQA  documents that are not part of MPQA  TOPIC).  Rules depend on the syntactic constituent  the two opinions are in consecutive  sentype of the opinion expression and rely on  syntactic parsing and grammatical role  label Number of Sentences/Paragraphs The  number of sentences/paragraphs that separate  Manual. Use the topic span marked by the  the two opinions.  human annotator. We included this method  to provide an upper bound on performance of  TOPIC SPAN-based lexico-semantic features  The  the topic span extractor.  features in this group rely on the topic spans and  are recomputed w.r.t. each of the four topic span  Modified Manual. Meant to be a more methods. The intuition behind this group of fea-istic use of the manual topic span annotations, tures is that topic-coreferent opinions are likely to this method returns the manually identified exhibit lexical and semantic similarity within the topic span only when it is within the sentence topic span.  of the opinion expression. When this span  is outside the sentence boundary, this method  tf.idf The cosine similarity of the tf.idf  returns the opinion sentence.  weighted vectors of the terms contained in the  two spans.  Of the 4976 opinions annotated across the 150  documents of MPQA  Word overlap True if the two topic spans  TOPIC, the topic spans  associated with 4293 were within the same sentence as  contain any contain words in common.  the opinion; 3653 were within the span extracted  by our topic span extractor. Additionally, the topic  NP coref True if the two spans contain NPs  spans of 173 opinions were outside of the  parathat are determined to be coreferent by a  simgraph containing the opinion.  ple rule-based coreference system.  6.2 Pairwise Topic Coreference Classification  NE overlap True if the two topic spans  contain named entities that can be considered  The heart of our method is a pairwise topic  corefaliases of each other.  erence classifier. Given a pair of opinions (and  their associated polarity and opinion holder Opinion features The features in this group de-mation), the goal of the classifier is to determine pend on the attributes of the opinion. In the cur-whether the opinions are topic-coreferent. We use rent work, we obtain these features directly from the manually annotated data to automatically learn the manual annotations of the MPQATOPIC corpus, the pairwise classifier. Given a training document, but they might also be obtained from automatically we construct a training example for every pair of identified opinion information using the methods opinions in the document (each pair is represented referenced in Section 3.  as a feature vector). The pair is labeled as a  positive example if the two opinions belong to the same  Source Match True if the two opinions have  topic cluster, and a negative example otherwise.  the same opinion holder.  Pairwise coreference classification relies  critically on the expressiveness of the features used  Polarity Match True if the two opinions have  to describe the opinion pair. We use three  catethe same polarity.  gories of features: positional, lexico-semantic and  5We use sentence/paragraph to describe two features one  opinion-based features.  based on the sentence and one on the paragraph.  Source-Polarity Match False if the two  opin one opinion per cluster assigns each  opinions have the same opinion holder but  conion to its own cluster.  flicting polarities (since it is unlikely that a  source will have two opinions with  conflictThe other two baselines attempt to perform topic  ing polarities on the same topic).  segmentation (discussed in Section 3) and assign  all opinions within the same segment to the same  opinion topic:  We employ three classifiers for pairwise  coreference classification an averaged perceptron  (Fretion by splitting documents into segments at  paragraph boundaries.  1998) and a rule-learner RIPPER (Cohen, 1995).  However, we report results only for the averaged  Choi 2000 Choi's (2000) state-of-the-art  perceptron, which exhibited the best performance.  approach to finding segment boundaries. We  use the freely available C99 software  de6.3 Clustering  scribed in Choi (2000), varying a parameter  Pairwise classification provides an estimate of the  that allows us to control the average number  likelihood that two opinions are topic-coreferent.  of sentences per segment and reporting the  To form the topic clusters, we follow the pairwise  best result on the test data.  classification with a clustering step. We selected 7.2 Evaluation Metrics a simple clustering algorithm single-link clustering, which has shown good performance for NP Because there is disagreement among researchers coreference. Given a threshold, single-link w.r.t. the proper evaluation measure for NP coref-ing proceeds by assigning pairs of opinions with a erence resolution, we use three generally accepted topic-coreference score above the threshold to the metrics7 to evaluate our topic coreference system.  same topic cluster and then performs transitive B-CUBED. B-CUBED (B3) is a commonly sure of the clusters.6  used NP coreference metric (Bagga and Baldwin,  7 Evaluation Methodology  1998). It calculates precision and recall for each  item (in our case, each opinion) based on the  numFor training and evaluation we use the ber of correctly identified coreference links, and document MPQATOPIC corpus. All machine then computes the average of the item scores in ing methods were tested via 10-fold cross each document. Precision/recall for an item i is tion. In each round of cross validation, we use computed as the proportion of items in the inter-eight of the data partitions for training and one for section of the response (system-generated) and key parameter estimation (we varied the threshold for (gold standard) clusters containing i divided by the the clustering algorithm), and test on the remaining number of items in the response/key cluster.  partition. We report results for the three evaluation  measures of Section 7 using the four topic span CEAF. As a representative of another group of extraction methods introduced in Section 6. The coreference measures that rely on mapping re-threshold is tuned separately for each evaluation sponse clusters to key clusters, we selected Luo's measure. As noted earlier, all runs obtain opinion (2005) CEAF score (short for Constrained Entity-information from the MPQA  Alignment F-Measure). Similar to the ACE (2005)  TOPIC corpus (i.e. this  work does not incorporate automatic opinion score, CEAF operates by computing an optimal traction).  mapping of response clusters to key clusters and  assessing the goodness of the match of each of the  7.1 Topic Coreference Baselines  mapped clusters.  We compare our topic coreference system to four Krippendorff's . Finally, we use Passonneau's baselines. The first two are the default baselines: (2004) generalization of Krippendorff's (1980)  one topic assigns all opinions to the same a standard metric employed for inter-annotator cluster.  7The MUC scoring algorithm (Vilain et al., 1995) was  6Experiments using best-first and last-first clustering  apomitted because it led to an unjustifiably high MUC F-score  proaches provided similar or worse results.  (.920) for the ONE TOPIC baseline.  All opinions  One topic  Sentiment opinions .7180 .7285 .7967  One opinion per cluster  Strong opinions  Table 1: Inter-annotator agreement results.  Rule-based  reliability studies.  Krippendorff's is based  Modified manual  on a probabilistic interpretation of the agreement  of coders as compared to agreement by chance.  While Passonneau's innovation makes it possible Table 2: Results for the topic coreference algo-to apply Krippendorff's to coreference clusters, rithms.  the probabilistic interpretation of the statistic is  unfortunately lost.  but other sensible choices for the distance lead to  much higher scores. Furthermore, we observed  that the behavior of the score can be rather  erratic small changes in one of the clusterings can  8.1 Inter-annotator Agreement  lead to big differences in the score.  As mentioned previously, out of the 150  annoPerhaps a better indicator of the reliability of  tated documents, 20 were annotated by two the coreference annotation is a comparison with tators for the purpose of studying the agreement the baselines, shown in the top half of Table 2.  between coders. Inter-annotator agreement results All baselines score significantly lower than the are shown in Table 1. We compute agreement for inter-annotator agreement scores. With one excep-three subsets of opinions: all available opinions, tion, the inter-annotator agreement scores are also only the sentiment-bearing opinions and the higher than those for the learning-based approach set of sentiment-bearing opinions judged to have (results shown in the lower half of Table 2), as polarity of medium or higher.  would typically be expected. The exception is the  The results support our conjecture that topics classifier that uses the manual topic spans, but as of sentiment-bearing opinions are much easier to we argued earlier these spans carry significant in-identify: inter-annotator agreement for opinions formation about the decision of the annotator.  with non-neutral polarity (SENTIMENT OPINIONS)  improves by a large margin for all measures. As in 8.2 Baselines  other work in subjectivity annotation, we find that Results for the four baselines are shown in the first strong sentiment-bearing opinions are easier to four rows of Table 2. As expected, the two base-notate than sentiment-bearing opinions in general.  lines performing topic segmentation show  substanGenerally, the score aims to probabilistically tially better scores than the two default base-capture the agreement of annotation data and lines.  arate it from chance agreement. It is generally  accepted that an score of .667 indicates reliable 8.3 Learning methods  agreement. The score that we observed for the Results for the learning-based approaches are overall agreement was an of .547, which is below shown in the bottom half of Table 2. First, we the generally accepted level, while for the two see that each of the learning-based methods out-subsets of sentiment-bearing opinions is above .72. performs the baselines. This is the case even when However, as discussed above, due to the way that sentences are employed as a coarse substitute for it is adapted to the problem of coreference the true topic span. A Wilcoxon Signed-Rank test tion, the score loses its probabilistic shows that differences from the baselines for the tion. For example, the score requires that a learning-based runs are statistically significant for wise distance function between clusters is the B3 and measures (p < 0.01); for CEAF, fied. We used one sensible choice for such a using sentences as topic spans for the learning al-tion (we measured the distance between clusters A gorithm outperforms the SAME PARAGRAPH baseline and B as dist(A, B) = (2 |A B|)/(|A|+|B|)), (p < 0.05), but the results are inconclusive when 823  compared with the system of CHOI.  Hasegawa, T., S. Sekine, and R. Grishman. 2004.  DiscoverIn addition, relying on manual topic span  inforing relations among named entities from large corpora. In  mation (  Proceedings of ACL.  MANUAL and MODIFIED MANUAL) allows the  Hu, M. and B. Liu. 2004. Mining opinion features in  cuslearning-based approach to perform significantly  tomer reviews. In AAAI.  better than the two runs that use automatically Joachims, T. 1998. Making large-scale support vector ma-identified spans (p < 0.01, for all three measures).  chine learning practical. In B. Sch lkopf, C. Burges,  A. Smola, editor, Advances in Kernel Methods: Support  The improvement in the scores hints at the  imporVector Machines. MIT Press, Cambridge, MA.  tance of improving automatic topic span Kim, S. and E. Hovy. 2006. Extracting opinions, opinion tion, which will be a focus of our future work.  holders, and topics expressed in online news media text.  In Proceedings of ACL/COLING Workshop on Sentiment  and Subjectivity in Text.  We presented a new, operational definition of  opinT. Fukushima. 2004. Collecting evaluative expressions  for opinion extraction. In Proceedings of IJCNLP.  ion topics in the context of fine-grained Krippendorff, K. 1980. Content Analysis: An Introduction to tivity analysis. Based on this definition, we inIts Methodology. Sage Publications, Beverly Hills, CA.  troduced an approach to opinion topic Luo, X. 2005. On coreference resolution performance met-cation that relies on the identification of  topicrics. In Proceedings of EMNLP.  coreferent opinions. We further employed the  for spoken lecture segmentation.  In Proceedings of  opinion topic definition for the manual annotation  of opinion topics to create the MPQA  Ng, V. and C. Cardie. 2002. Improving machine learning  TOPIC corpus.  Inter-annotator agreement results show that  opinapproaches to coreference resolution. In In Proceedings of  ion topic annotation can be performed reliably. Pang, B., L. Lee, and S. Vaithyanathan. 2002. Thumbs Finally, we proposed an automatic approach for  up? Sentiment classification using machine learning  techidentifying topic-coreferent opinions, which  signiques. In Proceedings of EMNLP.  Passonneau, R. 2004. Computing reliability for coreference  nificantly outperforms all baselines across three  annotation. In Proceedings of LREC.  coreference evaluation metrics.  features and opinions from reviews. In Proceedings of  Acknowledgments The authors of this paper Rosenfeld, B. and R. Feldman. 2007. Clustering for unsuper-would like to thank Janyce Wiebe and Theresa  vised relation identification. In Proceedings of CIKM.  Wilson for many insightful discussions. This work Soon, W., H. Ng, and D. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Com-was supported in part by National Science  Founputational Linguistics, 27(4).  dation Grants 0624277 and IIS-0535099 and Stoyanov, V. and C. Cardie. 2008. Annotating topics of opin-by DHS Grant N0014-07-1-0152.  ions. In Proceedings of LREC.  Turney, P. 2002. Thumbs up or thumbs down? Semantic  orientation applied to unsupervised classification of reviews.  In Proceedings of ACL.  The NIST ACE evaluation website.  Vilain, M., J. Burger, J. Aberdeen, D. Connolly, and  L. Hirschman. 1995. A model-theoretic coreference  scorBagga, A. and B. Baldwin. 1998. Algorithms for scoring  ing scheme. In Proceedings of the MUC6.  coreference chains. In In Proceedings of MUC7.  Voorhees, E. and L. Buckland. 2003. Overview of the  Bethard, S., H. Yu, A. Thornton, V. Hativassiloglou, and  TREC 2003 Question Answering Track. In Proceedings  D. Jurafsky. 2004. Automatic extraction of opinion  propoof TREC 12.  sitions and their holders. In 2004 AAAI Spring Symposium  Wiebe, J., T. Wilson, and C. Cardie. 2005. Annotating  exon Exploring Attitude and Affect in Text.  pressions of opinions and emotions in language. Language  Breck, E., Y. Choi, and C. Cardie. 2007. Identifying  expresResources and Evaluation, 1(2).  sions of opinion in context. In Proceedings of IJCAI.  Wiebe, J. 2005. Personal communication.  Choi, Y., C. Cardie, E. Riloff, and S. Patwardhan. 2005.  Identifying sources of opinions with conditional random fields  contextual polarity in phrase-level sentiment analysis. In  and extraction patterns. In Proceedings of EMNLP.  Proceedings of HLT/EMNLP.  Choi, Y., E. Breck, and C. Cardie. 2006. Joint extraction of  Wilson, T. 2005. Personal communication.  entities and relations for opinion recognition. In  Proceedings of EMNLP.  timent analyzer: Extracting sentiments about a given topic  Choi, F. 2000. Advances in domain independent linear text  using natural language processing techniques. In  Proceedsegmentation. Proceedings of NAACL.  ings of ICDM.  Cohen, W. 1995. Fast effective rule induction. In  Proceedings of ICML.  Freund, Y. and R. Schapire. 1998. Large margin  classification using the perceptron algorithm. In Proceedings of  Computational Learing Theory. 