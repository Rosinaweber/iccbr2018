 Forest Reranking: Discriminative Parsing with Non-Local Features  University of Pennsylvania  conventional reranking  only at the root  Conventional n-best reranking techniques  ofDP-based discrim. parsing  ten suffer from the limited scope of the  nthis work: forest-reranking  on-the-fly  best list, which rules out many potentially  good alternatives. We instead propose forest  Table 1: Comparison of various approaches for  inreranking, a method that reranks a packed for-corporating local and non-local features.  est of exponentially many parses. Since  exact inference is intractable with non-local  features, we present an approximate algorithm  insentence length. As a result, we often see very few  spired by forest rescoring that makes  discrimvariations among the n-best trees, for example,  50inative training practical over the whole  Treebest trees typically just represent a combination of 5  bank. Our final result, an F-score of 91.7,  outto 6 binary ambiguities (since 25 < 50 < 26).  performs both 50-best and 100-best reranking  Alternatively, discriminative parsing is tractable  baselines, and is better than any previously  rewith exact and efficient search based on dynamic  ported systems trained on the Treebank.  programming (DP) if all features are restricted to be local, that is, only looking at a local window within 1  Introduction  the factored search space (Taskar et al., 2004;  McDiscriminative reranking has become a popular  Donald et al., 2005). However, we miss the benefits  of non-local features that are not representable here.  parsing (Collins, 2000) and machine translation  Ideally, we would wish to combine the merits of  (Shen et al., 2005). Typically, this method first gen-both approaches, where an efficient inference  algoerates a list of top-n candidates from a baseline sys-rithm could integrate both local and non-local  featem, and then reranks this n-best list with arbitrary tures. Unfortunately, exact search is intractable (at features that are not computable or intractable to  least in theory) for features with unbounded scope.  compute within the baseline system. But despite its  So we propose forest reranking, a technique inspired apparent success, there remains a major drawback:  by forest rescoring (Huang and Chiang, 2007) that  this method suffers from the limited scope of the  napproximately reranks the packed forest of  expobest list, which rules out many potentially good  alnentially many parses. The key idea is to compute  ternatives. For example 41% of the correct parses  non-local features incrementally from bottom up, so  were not in the candidates of 30-best parses in  that we can rerank the n-best subtrees at all internal (Collins, 2000). This situation becomes worse with  nodes, instead of only at the root node as in  convenlonger sentences because the number of possible  intional reranking (see Table 1). This method can thus  terpretations usually grows exponentially with the  be viewed as a step towards the integration of  discriminative reranking with traditional chart parsing.  Part of this work was done while I was visiting Institute of Computing Technology, Beijing, and I thank Prof. Qun Liu Although previous work on discriminative pars-and his lab for hosting me. I am also grateful to Dan Gildea and ing has mainly focused on short sentences ( 15  Mark Johnson for inspirations, Eugene Charniak for help with his parser, and Wenbin Jiang for guidance on perceptron aver-words) (Taskar et al., 2004; Turian and Melamed,  aging. This project was supported by NSF ITR EIA-0205456.  2007), our work scales to the whole Treebank, where  Proceedings of ACL-08: HLT, pages 586 594,  Columbus, Ohio, USA, June 2008. c  2008 Association for Computational Linguistics  head (e) V is the consequent node in the  deductive step, and tails(e) V is the list of antecedent nodes. For example, the hyperedge for deduction (*)  is notated:  We also denote IN (v) to be the set of  incoming hyperedges of node v, which represent the dif-Figure 1: A partial forest of the example sentence.  ferent ways of deriving v. For example, in the  forest in Figure 1, IN (VP1,6) is {e1, e2}, with e2 =  we achieved an F-score of 91.7, which is a 19%  erh(VBD1,2, NP2,6), VP1,6i. We call |e| the arity of ror reduction from the 1-best baseline, and outper-hyperedge e, which counts the number of tail nodes  forms both 50-best and 100-best reranking. This  rein e. The arity of a hypergraph is the maximum  arsult is also better than any previously reported  sysity over all hyperedges. A CKY forest has an arity  tems trained on the Treebank.  of 2, since the input grammar is required to be  binary branching (cf. Chomsky Normal Form) to  enPacked Forests as Hypergraphs  sure cubic time parsing complexity. However, in this  Informally, a packed parse forest, or forest in short, work, we use forests from a Treebank parser (Char-is a compact representation of all the derivations  niak, 2000) whose grammar is often flat in many  productions. For example, the arity of the forest in  context-free grammar (Billot and Lang, 1989). For  Figure 1 is 3. Such a Treebank-style forest is  easexample, consider the following sentence  ier to work with for reranking, since many features  can be directly expressed in it. There is also a distin-I  with  guished root node TOP in each forest, denoting the goal item in parsing, which is simply S  where the numbers between words denote string  po0,l where S is  the start symbol and l is the sentence length.  sitions. Shown in Figure 1, this sentence has (at  least) two derivations depending on the attachment  of the prep. phrase PP3,6 with a mirror : it can  either be attached to the verb saw ,  Generic Reranking with the Perceptron  We first establish a unified framework for parse  reranking with both n-best lists and packed forests.  For a given sentence s, a generic reranker selects  or be attached to him , which will be further  comthe best parse  y among the set of candidates cand (s)  bined with the verb to form the same VP as above.  according to some scoring function:  These two derivations can be represented as a  single forest by sharing common sub-derivations. Such  y cand (s)  a forest has a structure of a hypergraph (Klein and  Manning, 2001; Huang and Chiang, 2005), where  In n-best reranking, cand (s) is simply a set of  items like PP3,6 are called nodes, and deductive n-best parses from the baseline parser, that is,  steps like (*) correspond to hyperedges.  cand (s) = {y1, y2, . . . , yn}. Whereas in forest  More formally, a forest is a pair hV, Ei, where V  reranking, cand (s) is a forest implicitly  representis the set of nodes, and E the set of hyperedges. For ing the set of exponentially many parses.  a given sentence w1:l = w1 . . . wl, each node v V  As usual, we define the score of a parse y to be  is in the form of Xi,j, which denotes the  recognithe dot product between a high dimensional feature  tion of nonterminal X spanning the substring from  representation and a weight vector w:  positions i through j (that is, wi+1 . . . wj). Each hyperedge e E is a pair htails(e), head (e)i, where  where the feature extractor f is a vector of d func-Pseudocode 1 Perceptron for Generic Reranking  tions f = (f1, . . . , fd), and each feature fj maps  1: Input: Training examples {cand (si), y+}N  is the  a parse y to a real number f  oracle tree for s  j (y). Following  (Char initial weights  niak and Johnson, 2005), the first feature f1(y) =  T iterations  log Pr(y) is the log probability of a parse from the  baseline generative parser, while the remaining  features are all integer valued, and each of them counts 6:  the number of times that a particular configuration  8: return w  occurs in parse y. For example, one such feature  f2000 might be a question  In n-best reranking, since all parses are explicitly  how many times is a VP of length 5 surrounded enumerated, it is trivial to compute the oracle tree.2  by the word has' and the period?  However, it remains widely open how to identify the  which is an instance of the WordEdges feature (see forest oracle. We will present a dynamic program-Figure 2(c) and Section 3.2 for details).  ming algorithm for this problem in Sec. 4.1.  Using a machine learning algorithm, the weight  We also use a refinement called averaged  paramvector w can be estimated from the training data  eters where the final weight vector is the average of where each sentence si is labelled with its cor-weight vectors after each sentence in each iteration  rect ( gold-standard ) parse y . As for the learner, i  over the training data. This averaging effect has been Collins (2000) uses the boosting algorithm and  shown to reduce overfitting and produce much more  Charniak and Johnson (2005) use the maximum  entropy estimator. In this work we use the averaged  perceptron algorithm (Collins, 2002) since it is an  Factorizing Local and Non-Local Features  online algorithm much simpler and orders of  magniA key difference between n-best and forest  reranktude faster than Boosting and MaxEnt methods.  ing is the handling of features. In n-best reranking, Shown in Pseudocode 1, the perceptron algo-all features are treated equivalently by the decoder, rithm makes several passes over the whole train-which simply computes the value of each one on  ing data, and in each iteration, for each sentence si, each candidate parse. However, for forest reranking,  it tries to predict a best parse  yi among the  candisince the trees are not explicitly enumerated, many  dates cand (si) using the current weight setting.  Infeatures can not be directly computed. So we first  tuitively, we want the gold parse y to be picked, but i  classify features into local and non-local, which the in general it is not guaranteed to be within cand (si), decoder will process in very different fashions.  because the grammar may fail to cover the gold  We define a feature f to be local if and only if parse, and because the gold parse may be pruned  it can be factored among the local productions in a  away due to the limited scope of cand (si). So we  tree, and non-local if otherwise. For example, the define an oracle parse y+ to be the candidate that i  Rule feature in Fig. 2(a) is local, while the Paren-has the highest Parseval F-score with respect to the  tRule feature in Fig. 2(b) is non-local. It is worth gold tree y :1  noting that some features which seem complicated  at the first sight are indeed local. For example, the i  WordEdges feature in Fig. 2(c), which classifies where function F returns the F-score. Now we train  a node by its label, span length, and surrounding  the reranker to pick the oracle parses as often as pos-words, is still local since all these information are sible, and in case an error is made (line 6), perform encoded either in the node itself or in the input sen-an update on the weight vector (line 7), by adding  tence. In contrast, it would become non-local if we  the difference between two feature representations.  replace the surrounding words by surrounding POS  1If one uses the gold y  i for oracle y+, the perceptron will  In case multiple candidates get the same highest F-score, i  continue to make updates towards something unreachable even we choose the parse with the highest log probability from the when the decoder has picked the best possible candidate.  baseline parser to be the oracle parse (Collins, 2000).  has  the  (c) WordEdges (local)  (d) NGramTree (non-local)  h NP 5 has . i  h VP (VBD saw) (NP (DT the)) i  Figure 2: Illustration of some example features. Shaded nodes denote information included in the feature.  tags, which are generated dynamically.  More formally, we split the feature extractor f =  (f1, . . . , fd) into f = (fL; fN ) where fL and fN are Bi,j  the local and non-local features, respectively. For the former, we extend their domains from parses to hy-wi . . . wj 1  peredges, where f (e) returns the value of a local feature f fL on hyperedge e, and its value on a parsey Figure 3: Example of the unit NGramTree feature factors across the hyperedges (local productions),  unit NGramTree instance is for the pair hwj 1, wji on the boundary between the two subtrees, whose  and we can pre-compute fL(e) for each e in a forest.  smallest common ancestor is the current node. Other  Non-local features, however, can not be  preunit NGramTree instances within this span have al-computed, but we still prefer to compute them as  ready been computed in the subtrees, except those  early as possible, which we call on-the-fly com-for the boundary words of the whole node, wi and  putation, so that our decoder can be sensitive to them wk 1, which will be computed when this node is fur-at internal nodes. For instance, the NGramTree fea-ther combined with other nodes in the future.  ture in Fig. 2 (d) returns the minimum tree fragement spanning a bigram, in this case saw and the , and  Approximate Decoding via Cube Pruning  should thus be computed at the smallest common an-Before moving on to approximate decoding with  cestor of the two, which is the VP node in this ex-non-local features, we first describe the algorithm  ample. Similarly, the ParentRule feature in Fig. 2  for exact decoding when only local features are  (b) can be computed when the S subtree is formed.  present, where many concepts and notations will be  In doing so, we essentially factor non-local features re-used later. We will use D(v) to denote the top  across subtrees, where for each subtree y in a parse y  derivations of node v, where D  f (y ) to be the part of  derivation. We also use the notation he, ji to denote that are computable within y , but not com-the derivation along hyperedge e, using the j  putable in any (proper) subtree of y . Then we have:  ith  subderivation for tail ui, so he, 1i is the best  derivation along e. The exact decoding algorithm, shown  in Pseudocode 2, is an instance of the bottom-up  Viterbi algorithm, which traverses the hypergraph in  Intuitively, we compute the unit non-local  feaa topological order, and at each node v, calculates  tures at each subtree from bottom-up. For example,  its 1-best derivation using each incoming hyperedge  for the binary-branching node Ai,k in Fig. 3, the  e IN (v). The cost of e, c(e), is the score of its  Pseudocode 2 Exact Decoding with Local Features derivations along a hyperedge e to form a new sub-1: function VITERBI(hV, Ei)  tree y = he, ji, we also compute its unit non-local  for v V in topological order do  (heap in Pseudocode 3) is used to hold the  candiif c(e) > c(D1(v)) then  dates for the next-best derivation, which is  initialized to the set of best derivations along each  hyperedge (lines 7 to 9). Then at each iteration, we pop  return D1(TOP)  the best derivation (lines 12), and push its  successors back into the priority queue (line 14).  AnaloPseudocode 3 Cube Pruning for Non-local Features gous to the language model cost in forest rescoring,  1: function CUBE(hV, Ei)  the unit feature cost here is a non-monotonic score in 2:  for v V in topological order do  the dynamic programming backbone, and the  derivareturn D  tions may thus be extracted out-of-order. So a buffer 1(TOP)  buf is used to hold extracted derivations, which is  heap ; buf  sorted at the end (line 15) to form the list of top-k 7:  c(he, 1i) E  derivations D(v) of node v. The complexity of this  append he, 1i to heap  algorithm is O(E + V k log kN ) (Huang and  Chi prioritized frontier  ang, 2005), where O(N ) is the time for on-the-fly  while |heap| > 0 and |buf | < k do  feature extraction for each subtree, which becomes  item POP-MAX(heap)  append item to buf  the bottleneck in practice.  PUSHSUCC(item, heap)  sort buf to D(v)  Supporting Forest Algorithms  16: procedure PUSHSUCC(he, ji, heap)  bi is 1 only on the ith dim.  Recall that the Parseval F-score is the harmonic  enough sub-derivations?  mean of labelled precision P and labelled recall R:  where |y| and |y | are the numbers of brackets in the test parse and gold parse, respectively, and |y y |  (pre-computed) local features w f  is the number of matched brackets. Since the  harL(e). This  algorithm has a time complexity of O(E), and is almost  monic mean is a non-linear combination, we can not  identical to traditional chart parsing, except that the optimize the F-scores on sub-forests independently  forest might be more than binary-branching.  with a greedy algorithm. In other words, the optimal  F-score tree in a forest is not guaranteed to be com-For non-local features, we adapt cube pruning  posed of two optimal F-score subtrees.  from forest rescoring (Chiang, 2007; Huang and  We instead propose a dynamic programming  alChiang, 2007), since the situation here is analogous  gorithm which optimizes the number of matched  to machine translation decoding with integrated  lanbrackets for a given number of test brackets. For  exguage models: we can view the scores of unit  nonample, our algorithm will ask questions like,  local features as the language model cost, computed  on-the-fly when combining sub-constituents.  when a test parse has 5 brackets, what is the  Shown in Pseudocode 3, cube pruning works  maximum number of matched brackets?  bottom-up on the forest, keeping a beam of at most k  derivations at each node, and uses the k-best  parsMore formally, at each node v, we compute an  oraing Algorithm 2 of Huang and Chiang (2005) to  cle function ora[v] : N 7 N, which maps an integer speed up the computation. When combining the subt to ora[v](t), the max. number of matched brackets  Pseudocode 4 Forest Oracle Algorithm Shown in Pseudocode 4, we perform these com-1: function ORACLE(hV, Ei, y )  putations in a bottom-up topological order, and  fifor v V in topological order do  nally at the root node TOP, we can compute the best  global F-score by maximizing over different  numbers of test brackets (line 7). The oracle tree y+ can 6:  be recursively restored by keeping backpointers for  return F (y+, y ) = max 2 ora[TOP](t)  each ora[v](t), which we omit in the pseudocode.  The time complexity of this algorithm for a  sentence of l words is O(|E| l2(a 1)) where a is the  for all parses yv of node v with exactly t brackets:  arity of the forest. For a CKY forest, this amounts  those in our experiments the complexities are much  higher. In practice it takes on average 0.05 seconds  When node v is combined with another node u  for forests pruned by p = 10 (see Section 4.2), but  along a hyperedge e = h(v, u), wi, we need to  comwe can pre-compute and store the oracle for each  bine the two oracle functions ora[v] and ora[u] by  forest before training starts.  distributing the test brackets of w between v and u,  and optimize the number of matched bracktes. To  do this we define a convolution operator between two functions f and g:  Our forest pruning algorithm (Jonathan Graehl, p.c.)  is very similar to the method based on marginal  (f g)(t) , max f(t1) + g(t2)  probability (Charniak and Johnson, 2005), except  that ours prunes hyperedges as well as nodes.  BaFor instance:  sically, we use an Inside-Outside algorithm to  compute the Viterbi inside cost (v) and the Viterbi out-t  f (t)  side cost (v) for each node v, and then compute the  merit (e) for each hyperedge:  The oracle function for the head node w is then  where 1 is the indicator function, returning 1 if node Intuitively, this merit is the cost of the best deriva-w is found in the gold tree y , in which case we  tion that traverses e, and the difference (e) =  increment the number of matched brackets. We can  (e) (TOP) can be seen as the distance away  also express Eq. 9 in a purely functional form  from the globally best derivation. We prune away  all hyperedges that have (e) > p for a  threshold p. Nodes with all incoming hyperedges pruned  are also pruned. The key difference from (Charniak  where is a translation operator which shifts a and Johnson, 2005) is that in this algorithm, a node  function along the axes:  can partially survive the beam, with a subset of its (f (a, b))(t) , f(t a) + b  hyperedges pruned. In practice, this method prunes  on average 15% more hyperedges than their method.  Above we discussed the case of one hyperedge. If  there is another hyperedge e deriving node w, we  also need to combine the resulting oracle functions  from both hyperedges, for which we define a point-We compare the performance of our forest reranker  wise addition operator :  against n-best reranking on the Penn English  Treebank (Marcus et al., 1993). The baseline parser is  (f g)(t) , max{f(t), g(t)}  the Charniak parser, which we modified to output a  instances  Non-Local  instances  Word  WordEdges  Heads  HeadTree  NGramTree  forest oracle  HeadMod  RightBranch  n-best oracle  Total Feature Instances: 800, 582  Table 2: Features used in this work. Those with a  average # of hyperedges or brackets per sentence  are from (Collins, 2000), and others are from  (Charniak and Johnson, 2005), with simplifications.  Figure 4: Forests (shown with various pruning  thresholds) enjoy higher oracle scores and more  compact sizes than n-best lists (on sec 23).  packed forest for each sentence.3  tures in the updated version.5 However, our initial  We use the standard split of the Treebank: sections  experiments show that, even with this much simpler  02-21 as the training data (39832 sentences),  secfeature set, our 50-best reranker performed equally  tion 22 as the development set (1700 sentences), and  well as theirs (both with an F-score of 91.4, see  Tasection 23 as the test set (2416 sentences).  Followbles 3 and 4). This result confirms that our feature  ing (Charniak and Johnson, 2005), the training set is set design is appropriate, and the averaged percep-split into 20 folds, each containing about 1992  sentron learner is a reasonable candidate for reranking.  tences, and is parsed by the Charniak parser with a  The forests dumped from the Charniak parser are  model trained on sentences from the remaining 19  huge in size, so we use the forest pruning algorithm  folds. The development set and the test set are parsed in Section 4.2 to prune them down to a reasonable  with a model trained on all 39832 training sentences.  size. In the following experiments we use a  threshWe implemented both n-best and forest reranking  old of p = 10, which results in forests with an  avsystems in Python and ran our experiments on a  64erage number of 123.1 hyperedges per forest. Then  bit Dual-Core Intel Xeon with 3.0GHz CPUs. Our  for each forest, we annotate its forest oracle, and  feature set is summarized in Table 2, which closely  on each hyperedge, pre-compute its local features.6  follows Charniak and Johnson (2005), except that  Shown in Figure 4, these forests have an forest  orwe excluded the non-local features Edges, NGram, acle of 97.8, which is 1.1% higher than the 50-best  and CoPar, and simplified Rule and NGramTree oracle (96.7), and are 8 times smaller in size.  features, since they were too complicated to  comResults and Analysis  pute.4 We also added four unlexicalized local fea-Table 3 compares the performance of forest  reranktures from Collins (2000) to cope with data-sparsity.  ing against standard n-best reranking. For both  sysFollowing Charniak and Johnson (2005), we  extems, we first use only the local features, and then  tracted the features from the 50-best parses on the  all the features. We use the development set to deter-training set (sec. 02-21), and used a cut-off of 5 to mine the optimal number of iterations for averaged  prune away low-count features. There are 0.8M  feaperceptron, and report the F  tures in our final set, considerably fewer than that  1 score on the test set.  With only local features, our forest reranker achieves of Charniak and Johnson which has about 1.3M fea-an F-score of 91.25, and with the addition of  non3This is a relatively minor change to the Charniak parser, since it implements Algorithm 3 of Huang and Chiang (2005) 5http://www.cog.brown.edu/ mj/software.htm. We follow for efficient enumeration of n-best parses, which requires stor-this version as it corrects some bugs from their 2005 paper ing the forest. The modified parser and related scripts for han-which leads to a 0.4% increase in performance (see Table 4).  dling forests (e.g. oracles) will be available on my homepage.  6A subset of local features, e.g. WordEdges, is independent 4In fact, our Rule and ParentRule features are two special of which hyperedge the node takes in a derivation, and can thus cases of the original Rule feature in (Charniak and Johnson, be annotated on nodes rather than hyperedges. We call these 2005). We also restricted NGramTree to be on bigrams only.  features node-local, which also include part of Word features.  type  Collins (2000)  features  Charniak and Johnson (2005)  updated (Johnson, 2006)  this work  features  Table 4: Comparison of our final results with other  Table 3: Forest reranking compared to n-best  rerankbest-performing systems on the whole Section 23.  ing on sec. 23. The pre-comp. column is for feature Types D, G, and S denote discriminative, generative,  extraction, and training column shows the number and semi-supervised approaches, respectively.  of perceptron iterations that achieved best results on the dev set, and average time per iteration.  McClosky et al. (2006) achieved an even higher  accuarcy (92.1) by leveraging on much larger  unlalocal features, the accuracy rises to 91.69 (with beam belled data. Moreover, their technique is orthogonal  size k = 15), which is a 0.26% absolute  improveto ours, and we suspect that replacing their n-best  This improvement might look relatively small, but  ing, most discriminative methods require repeated  it is much harder to make a similar progress with  parsing of the training set, which is generally  imn-best reranking. For example, even if we double  pratical (Petrov and Klein, 2008). Therefore,  prethe size of the n-best list to 100, the performance  vious work often resorts to extremely short  senonly goes up by 0.06% (Table 3). In fact, the  100tences ( 15 words) or only looked at local  feabest oracle is only 0.5% higher than the 50-best one  (see Fig. 4). In addition, the feature extraction step and Melamed, 2007). In comparison, thanks to the  in 100-best reranking produces huge data files and  efficient decoding, our work not only scaled to the  takes 44 hours in total, though this part can be paral-whole Treebank, but also successfully incorporated  lelized.8 On two CPUs, 100-best reranking takes 25  non-local features, which showed an absolute  imhours, while our forest-reranker can also finish in 26  provement of 0.44% over that of local features alone.  hours, with a much smaller disk space. Indeed, this  demonstrates the severe redundancies as another  disConclusion  advantage of n-best lists, where many subtrees are  We have presented a framework for reranking on  repeated across different parses, while the packed  packed forests which compactly encodes many more  forest reduces space dramatically by sharing  comcandidates than n-best lists. With efficient  approxmon sub-derivations (see Fig. 4).  imate decoding, perceptron training on the whole  To put our results in perspective, we also compare  Treebank becomes practical, which can be done in  them with other best-performing systems in Table 4.  about a day even with a Python implementation. Our  Our final result (91.7) is better than any previously final result outperforms both 50-best and 100-best  reported system trained on the Treebank, although  reranking baselines, and is better than any  previously reported systems trained on the Treebank. We  7It is surprising that 50-best reranking with local features achieves an even higher F-score of 91.28, and we suspect this is also devised a dynamic programming algorithm for  due to the aggressive updates and instability of the perceptron, forest oracles, an interesting problem by itself. We  as we do observe the learning curves to be non-monotonic. We believe this general framework could also be applied  leave the use of more stable learning algorithms to future work.  to other problems involving forests or lattices, such The n-best feature extraction already uses relative counts (Johnson, 2006), which reduced file sizes by at least a factor 4.  as sequence labeling and machine translation.  Proceedings of the HLT-NAACL, New York City,  Sylvie Billot and Bernard Lang. 1989. The  strucRyan McDonald, Koby Crammer, and Fernando  ture of shared forests in ambiguous parsing. In  Pereira. 2005. Online large-margin training of  Proceedings of ACL '89, pages 143 151.  dependency parsers. In Proceedings of the 43rd  Rens Bod. 2003. An efficient implementation of a  new DOP model. In Proceedings of EACL.  Eugene Charniak and Mark Johnson. 2005.  Coarseence for unlexicalized parsing. In Proceedings of to-fine-grained n-best parsing and discriminative  reranking. In Proceedings of the 43rd ACL.  log-linear grammars with latent variables. In Pro-inspired parser. In Proceedings of NAACL.  ceedings of NIPS 20.  David Chiang.  Hierarchical  phrasebased translation.  Computational Linguistics,  Discriminative reranking for machine  translation. In Proceedings of HLT-NAACL.  Michael Collins. 2000. Discriminative reranking  for natural language parsing. In Proceedings of  Koller, and Chris Manning. 2004. Max-margin  parsing. In Proceedings of EMNLP.  Michael Collins.  Discriminative training  Joseph Turian and I. Dan Melamed. 2007. Scalable  discriminative learning for natural language  parsexperiments with perceptron algorithms. In  Proing and translation. In Proceedings of NIPS 19.  ceedings of EMNLP.  James Henderson. 2004. Discriminative training of  a neural network statistical parser. In Proceedings of ACL.  best Parsing. In Proceedings of the Ninth International Workshop on Parsing Technologies  (IWPTrescoring: Fast decoding with integrated language  models. In Proceedings of ACL.  Features of  statistiTalk given at the Joint  Microsoft  Research  and  of   Washington  Linguistics  Parsing and Hypergraphs. In Proceedings of the  Seventh International Workshop on Parsing  Technologies (IWPT-2001), 17-19 October 2001,  Beiand  Building a  large annotated corpus of English: the Penn  Treebank. Computational Linguistics, 19:313 330.  David McClosky, Eugene Charniak, and Mark  Johnson. 2006. Effective self-training for parsing. In 