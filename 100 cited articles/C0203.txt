 Playing the Telephone Game: Determining the Hierarchical Structure of Perspective and Speech Expressions  Eric Breck and Claire Cardie  Department of Computer Science  Cornell University  Ithaca, NY 14853  to refer to these mental and emotional states that  News articles report on facts, events, and  opincannot be directly observed or verified (Quirk et al.,  ions with the intent of conveying the truth.  1985). Further, we define the source of a perspec-However, the facts, events, and opinions  appeartive expression to be the experiencer of that private  ing in the text are often known only  secondstate, that is, the person or entity whose opinion  or third-hand, and as any child who has played  or emotion is being conveyed in the text. Second,  telephone knows, this relaying of facts often  speech expressions simply convey the words of an-garbles the original message. Properly  underother individual and by the choice of words, the  standing the information filtering structures that  reporter filters the original source's intent. Consider  govern the interpretation of these facts, then, is  for example, the following sentences (in which  percritical to appropriately analyzing them. In this  work, we present a learning approach that  corspective expressions are denoted in bold, speech ex-rectly determines the hierarchical structure of  pressions are underlined, and sources are denoted in  information filtering expressions 78.30% of the  1. Charlie was angry at Alice's claim that Bob was unhappy.  Introduction  2. Philip Clapp, president of the National Environ-Newswire text has long been a primary target for  ment Trust, sums up well the general thrust of the  reaction of environmental movements: There is no as information extraction, summarization, and ques-reason at all to believe that the polluters are  sudtion answering (e.g. MUC (1998); NIS (2003);  denly going to become reasonable.  DUC (2003)). However, newswire does not offer  direct access to facts, events, and opinions; rather,  Perspective expressions in Sentence 1 describe the  journalists report what they have experienced, and  emotions or opinion of three sources: Charlie's  report on the experiences of others. That is, facts,  anger, Bob's unhappiness, and Alice's belief.  Perevents, and opinions are filtered by the point of  spective expressions in Sentence 2, on the other  view of the writer and other sources.  hand, introduce the explicit opinion of one source,  nately, this filtering of information through multiple  i.e. the reaction of the environmental movements.  sources (and multiple points of view) complicates  Speech expressions also perform filtering in these  the natural language interpretation process because  examples. The reaction of the environmental  movethe reader (human or machine) must take into  acments is filtered by Clapp's summarization, which,  count the biases introduced by this indirection. It  in turn, is filtered by the writer's choice of quotation.  is important for understanding both newswire and  In addition, the fact that Bob was unhappy is filtered  narrative text (Wiebe, 1994), therefore, to  approprithrough Alice's claim, which, in turn, is filtered by  ately recognize expressions of point of view, and to  the writer's choice of words for the sentence.  Simassociate them with their direct and indirect sources.  ilarly, it is only according to the writer that Charlie  This paper introduces two kinds of expression  that can filter information. First, we define a  perThe specific goal of the research described here  spective expression to be the minimal span of text is to accurately identify the hierarchical structure of  that denotes the presence of an explicit opinion,  perspective and speech expressions (pse's) in text.2  evaluation, emotion, speculation, belief, sentiment,  al.'s (2003) expressive subjective elements are not the subject etc.1 Private state is the general term typically used of study here.  2For the rest of this paper, then, we ignore the distinction 1Note that implicit expressions of perspective, i.e. Wiebe et between perspective and speech expressions, so in future  exGiven sentences 1 and 2 and their pse's, for  exampse class  count  ple, we will present methods that produce the  strucwriter  tures shown in Figure 1, which represent the  multistage information filtering that should be taken into  account in the interpretation of the text.  adjective  writer's implicit speech event  other  Table 1: Breakdown of classes of pse's. writer  denotes pse's with the writer as source. No parse denotes unhappy  pse's in sentences where the parse failed, and so the part Sentence 2:  of speech could not be determined.  writer's implicit speech event  number of pse's  number of sentences  reaction  Figure 1: Hierarchical structure of the perspective and  speech expressions in sentences 1 and 2  We propose a supervised machine learning  apTable 2: Breakdown of number of pse's per sentence  proach to the problem that relies on a small set  of syntactically-based features. More specifically,  system. Such a system would be able to identify  the method first trains a binary classifier to make  all pse's in a document, as well as identify their  pairwise parent-child decisions among the pse's in  structure. The system would also identify the direct  the same sentence, and then combines the  decisource of each pse. Finally, the system would  idensions to determine their global hierarchical  structify the text corresponding to the content of a private  ture. We compare the approach to two  heuristicstate or the speech expressed by a pse.3 Such a  sysbased baselines one that simply assumes that  evtem might analyze sentence 2 as follows:  ery pse is filtered only through the writer, and a  (source: writer  second that is based on syntactic dominance  relapse: (implicit speech event)  tions in the associated parse tree. In an evaluation  content: Philip ... reasonable. )  using the opinion-annotated NRRC corpus (Wiebe  et al., 2002), the learning-based approach achieves  an accuracy of 78.30%, significantly higher than  both the simple baseline approach (65.57%) and the  content: There ... reasonable. )  parse-based baseline (71.64%). We believe that this  study provides a first step towards understanding the  multi-stage filtering process that can bias and garble  pse: reaction  the information present in newswire text.  The rest of the paper is organized as follows. We  As far as we are aware, no single system  expresent related work in Section 2 and describe the  ists that simultaneously solves all these problems.  machine learning approach in Section 3. The  exThere is, however, quite a bit of work that addresses  perimental methodology and results are presented  various pieces of this larger task, which we will now  in Sections 4 and 5, respectively. Section 6  summasurvey.  rizes our conclusions and plans for future work.  The Larger Problem and Related Work  of the reader of a news article. Her model provides  for multiple levels of hierarchical beliefs, such as  This paper addresses the problem of identifying the  the nesting of a primary source's belief within that  hierarchical structure of perspective and speech  exof a reporter. However, Gerard does not provide  alWe view this as a necessary and  imgorithms for extracting this structure directly from  portant component of a larger perspective-analysis  newswire texts.  amples, both types of pse appear in boldface. Note that the Bethard et al. (2004) seek to extract propositional  acronym pse' has been used previously with a different mean-ing (Wiebe, 1994).  3In (Wiebe, 2002), this is referred to as the inside.  opinions and their holders. They define an opinion  was  as a sentence, or part of a sentence that would  answer the question How does X feel about Y?' A  propositional opinion is an opinion localized in the  propositional argument of certain verbs, such as  believe or realize . Their task then corresponds  to identifying a pse, its associated direct source, and  the content of the private state. However, they  con's  that  sider as pse's only verbs, and further restrict  attention to verbs with a propositional argument, which  is a subset of the perspective and speech expressions  that we consider here. Table 1, for example, shows  the diversity of word classes that correspond to pse's  Figure 2: Dependency parse of sentence 1 according to  in our corpus. Perhaps more importantly for the  the Collins parser.  purposes of this paper, their work does not address  information filtering issues, i.e. problems that arise  when an opinion has been filtered through multiple  measure), the problem is still clearly unsolved. As  sources. Namely, Bethard et al. (2004) do not  conexplained below, we will instead rely on manually  sider sentences that contain multiple pse's, and do  tagged pse's for the studies presented here.  not, therefore, need to identify any indirect sources  The Approach  of opinions. As shown in Table 2, however, we  find that sentences with multiple non-writer pse's  Our task is to find the hierarchical structure among  (i.e. sentences that contain 3 or more total pse's)  the pse's in individual sentences. One's first  imcomprise a significant portion (29.98%) of our  corpression might be that this structure should be  obpus. An advantage over our work, however, is that  vious from the syntax: one pse should filter  another roughly when it dominates the other in a  detions to pse identification and the identification of  pendency parse. This heuristic, for example, would  their direct sources.  succeed for claim and unhappy in sentence 1,  Automatic identification of sources has also  whose pse structure is given in Figure 1 and parse  been addressed indirectly by Gildea and Jurafsky's  structure (as produced by the Collins parser) in  Fig(2002) work on semantic role identification in that  finding sources often corresponds to finding the  Even in sentence 1, though, we can see that  filler of the agent role for verbs. Their methods then  the problem is more complex: angry dominates  might be used to identify sources and associate them  claim in the parse tree, but does not filter it.  Unwith pse's that are verbs or portions of verb phrases.  fortunately, an analysis of the parse-based heuristic  Whether their work will also apply to pse's that are  on our training data (the data set will be described  realized as other parts of speech is an open question.  in Section 4), uncovered numerous, rather than just  Wiebe (1994), studies methods to track the  a few, sources of error. Therefore, rather than trying  change of point of view in narrative text (fiction).  to handcraft a more complex collection of  heurisThat is, the writer of one sentence may not  corretics, we chose to adopt a supervised machine  learnspond to the writer of the next sentence. Although  ing approach that relies on features identified in this  this is not as frequent in newswire text as in fiction,  analysis. In particular, we will first train a binary  it will still need to be addressed in a solution to the  classifier to make pairwise decisions as to whether  a given pse is the immediate parent of another. We  then use a simple approach to combine these  deBergler (1993) examines the lexical semantics of  cisions to find the hierarchical information-filtering  speech event verbs in the context of generative  lexstructure of all pse's in a sentence.  icon theory. While not specifically addressing our  We assume that we have a training corpus of  problem, the semantic dimensions of reporting  verbs that she extracts might be very useful as  fea4For this heuristic and the features that follow, we will speak tures in our approach.  of the pse's as if they had a position in the parse tree. However, Finally, Wiebe et al. (2003) present preliminary  since pse's are often multiple words, and do not necessarily form a constituent, this is not entirely accurate. The parse node results for the automatic identification of perspec-corresponding to a pse will be the highest node in the depen-tive and speech expressions using corpus-based  dency parse corresponding to a word in the pse. We consider techniques. While the results are promising (66% F-the writer's implicit pse to correspond to the root of the parse.  sentences, annotated with pse's and their  hierdominates psetarget, but only if the first dependency  archical pse structure (Section 4 describes the  relation is an object relation.  Training instances for the binary  clasFor similar reasons, we include a feature  calculatsifier are pairs of pse's from the same sentence,  ing the domination relation based on a partial parse.  We assign a class value  Consider the following sentence:  of 1 to a training instance if pseparent is the  3. He was criticized more than recognized for his immediate parent of psetarget in the manually  annotated hierarchical structure for the sentence,  and 0 otherwise.  For sentence 1, there are nine  One of criticized or recognized will be the root  instances  hclaim, writeri,  of this dependency parse, thus dominating the other,  hangry, writeri,  (class  and suggesting (incorrectly) that it filters the other  hunhappy, writeri,  stituents, such spurious dominations are eliminated.  hunhappy, angryi (class 0).  The features used  The partial parse feature is 1 for fewer instances  to describe each training instance are explained  than pseparent-dominates-psetarget, but it is more  below.  indicative of a positive instance when it is 1.  During testing, we construct the hierarchical pse  So that the model can adjust when the parse is  structure of an entire sentence as follows. For each  not present, we include a feature that is 1 for all  pse in the sentence, ask the binary classifier to judge  instances generated from sentences on which the  each other pse as a potential parent, and choose the  parser failed.  pse with the highest confidence6. Finally, join these  Positional features (5).  Forcing the model to  deimmediate-parent links to form a tree.7  cide whether pseparent is the parent of psetarget  One might also try comparing pairs of potential  without knowledge of the other pse's in the  senparents for a given pse, or other more direct means  tence is somewhat artificial. We therefore include  of ranking potential parents. We chose what seemed  several features that encode the relative position of  to be the simplest method for this first attempt at the  pseparent and psetarget in the sentence.  Specifically, we add a feature that is 1 if pseparent is the  Features  root of the parse (and similarly for psetarget ). We  also include a feature giving the ordinal position of  Here we motivate and describe the 23 features used  in our model. Unless otherwise stated, all features  parent among the pse's in the sentence, relative  are binary (1 if the described condition is true, 0  target (-1 means pseparent is the pse that  immediately precedes pse  otherwise).  target, 1 means immediately  following, and so forth). To allow the model to vary  Parse-based features (6).  Based on the  perforwhen there are more potential parents to choose  mance of the parse-based heuristic, we include a  from, we include a feature giving the total number  pseparent-dominates-psetarget feature in our feature  of pse's in the sentence.  set. To compensate for parse errors, however, we  also include a variant of this that is 1 if the parent of particular pse's are special, so we specify indicator  features for four types of parents: the writer pse,  Many filtering expressions filter pse's that occur  and the lexical items said (the most common  nonin their complements, but not in adjuncts.  Therewriter pse) and according to . According to is  fore, we add variants of the previous two  syntaxspecial because it is generally not very high in the  based features that denote whether the parent node  parse, but semantically tends to filter everything else  5We skip sentences where there is no decision to make (sen-in the sentence.  tences with zero or one non-writer pse). Since the writer pse is In addition, we include as features the part of  the root of every structure, we do not generate instances with speech of pseparent and psetarget (reduced to noun,  the writer pse in the psetarget position.  verb, adjective, adverb, or other), since intuitively  6There is an ambiguity if the classifier assigns the same confidence to two potential parents. For evaluation purposes, we we expected distinct parts of speech to behave dif-consider the classifier's response incorrect if any of the highest-ferently in their filtering.  scoring potential parents are incorrect.  Genre-specific features (6).  Finally, journalistic  7The directed graph resulting from flawed automatic  predictions might not be a tree (i.e. it might be cyclic and discon-writing contains a few special forms that are not  alnected). Since this occurs very rarely (5 out of 9808 sentences ways parsed accurately. Examples are:  on the test data), we do not attempt to correct any non-tree graphs.  4. Alice disagrees with me, Bob argued.  5. Charlie, she noted, dislikes Chinese food.  This structure must be extracted from  an attribute of each pse annotation, which lists the  The parser may not recognize that noted and  pse's direct and indirect sources. For example, the  argued should dominate all other pse's in  sen source chain for unhappy in sentence 1, would  tences 4 and 5, so we attempt to recognize when  be ( writer, Alice, Bob). The source chains allow a sentence falls into one of these two patterns.  us to automatically recover the hierarchical  strucFor hdisagrees, arguedi generated from sentence 4,  ture of the pse's: the parent of a pse with source  features pseparent-pattern-1 and  psetarget-patternchain (s0, s1, . . . sn 1, sn) is the pse with source  1 would be 1, while for hdislikes, notedi generated  chain (s0, s1, . . . sn 1). Unfortunately, ambiguities  from sentence 5, feature pseparent-pattern-2 would  can arise. Consider the following sentence:  be 1. We also add features that denote whether the  pse in question falls between matching quote marks.  6. Bob said, you're welcome because he was glad to see that Mary was happy.  Finally, a simple feature indicates whether pseparent  is the last word in the sentence.  Both said and was glad have the source chain  ( writer, Bob),11 while was happy has the source 3.2  chain ( writer, Bob, Mary). It is therefore not clear We rely on a variety of resources to generate our fea-from the manual annotations whether was happy  tures. The corpus (see Section 4) is distributed with  should have was glad or said as its parent.  annotations for sentence breaks, tokenization, and  5.82% of the pse's have ambiguous parentage (i.e.  part of speech information automatically generated  the recovery step finds a set of parents P (pse) with  by the GATE toolkit (Cunningham et al., 2002).8  |P (pse)| > 1). For training, we assign a class value For parsing we use the Collins (1999) parser.9 For  of 1 to all instances hpse, pari, par P (pse). For  partial parses, we employ CASS (Abney, 1997).  Fitesting, if an algorithm attaches pse to any element  nally, we use a simple finite-state recognizer to  idenof P (pse), we score the link as correct (see  Sectify (possibly nested) quoted phrases.  tion 5.1). Since ultimately our goal is to find the  For classifier construction, we use the IND  packsources through which information is filtered (rather  age (Buntine, 1993) to train decision trees (we use  than the pse's), we believe this is justified.  the mml tree style, a minimum message length  criFor training and testing, we used only those  senterion with Bayesian smoothing).  tences that contain at least two non-writer pse's12  for all other sentences, there is only one way to  construct the hierarchical structure. Again, Table 2  The data for these experiments come from version  presents a breakdown (for the test set) of the  num1.1 of the NRRC corpus (Wiebe et al., 2002).10. The  ber of pse's per sentence thus we only use  approxcorpus consists of 535 newswire documents (mostly  imately one-third of all the sentences in the corpus.  from the FBIS), of which we used 66 (1375  sentences) for developing the heuristics and features,  Results and Discussion  while keeping the remaining 469 (9808 sentences)  blind (used for 10-fold cross-validation).  How do we evaluate the performance of an  auAlthough the NRRC corpus provides annotations  tomatic method of determining the hierarchical  structure of pse's? Lin (1995) proposes a method  note directly their hierarchical structure within a  for evaluating dependency parses: the score for  a sentence is the fraction of correct parent links  boundaries, which seems never to be warranted. Inaccurately identified; the score for the corpus is the aver-joining sentences has the effect of adding more noise to our age sentence score.  Formally, the score for a  problem, so we split GATE's sentences at paragraph boundaries, and introduce writer pse's for the newly created sen-11The annotators also performed coreference resolution on tences.  9We convert the parse to a dependency format that makes  12Under certain circumstances, such as paragraph-long  some of our features simpler using a method similar to the one quotes, the writer of a sentence will not be the same as the described in Xia and Palmer (2001). We also employ a method writer of a document. In such sentences, the NRRC corpus con-from Adam Lopez at the University of Maryland to find gram-tains additional pse's for any other sources besides the writer of matical relationships between words (subject, object, etc.).  the document. Since we are concerned in this work only with 10The  one sentence at a time, we discard all such implicit pse's  besides the writer of the sentence. Also, in a few cases, more than 04/approval_mpqa.htm.  Code and data used in our  one pse in a sentence was marked as having the writer as its experiments are available at http://www.cs.cornell.  source. We believe this to be an error and so discarded all but edu/ ebreck/breck04playing/.  one writer pse.  Encouragingly, our machine learning method  uniformly and significantly14 outperforms the two  heuristic methods, on all metrics and in sentences  with any number of pse's. The difference is most  striking in the perf metric, which is perhaps  Table 3: Performance on test data. Lin is Lin's  depenthe most intuitive. Also, the syntax-based  heurisdency score, perf is the fraction of sentences whose  tic (heurTwo) significantly15 outperforms heurOne,  structure was identified perfectly, and bin is the perfor-confirming our intuitions that syntax is important in  mance of the binary classifier (broken down for positive this task.  and negative instances). Size is the number of  sentences or pse pairs.  As the binary classifer sees many more negative  instances than positive, it is unsurprising that its performance is much better on negative instances. This  suggests that we might benefit from machine  learning methods for dealing with unbalanced datasets.  Examining the errors of the machine learning  system on the development set, we see that for half  of the pse's with erroneously identified parents, the  Table 4: Performance by number of pse's per sentence  parent is either the writer's pse, or a pse like said  in sentences 4 and 5 having scope over the entire  method evaluated on the entire corpus ( Lin ) is  7. Our concern is whether persons used to the role s S  |Non writer pse0s(s)|  of policy implementors can objectively assess and  where S is the set of all sentences in the corpus,  critique executive policies which impinge on  huNon writer pse0s(s) is the set of non-writer pse's  in sentence s, parent(pse) is the correct parent  of pse, and autopar(pse) is the automatically  to be said rather than concern. We also see from  identified parent of pse.  Table 4 that the model performs more poorly on  senWe also present results using two other (related)  tences with more pse's. We believe that this reflects  metrics. The perf metric measures the fraction  a weakness in our decision to combine binary  deciof sentences whose structure is determined entirely  sions, because the model has learned that in general,  correctly (i.e. perf ectly). Bin is the accuracy of  a said or writer's pse (near the root of the  structhe binary classifier (with a 0.5 threshold) on the  inture) is likely to be the parent, while it sees many  stances created from the test corpus. We also report  fewer examples of pse's such as concern that lie  the performance on positive and negative instances.  in the middle of the tree.  Although we have ignored the distinction  throughout this paper, error analysis suggests  speech event pse's behave differently than private  We compare the learning-based approach ( decTree) state pse's with respect to how closely syntax re-to the heuristic-based approaches introduced in  Secflects their hierarchical structure. It may behoove  tion 3 heurOne assumes that all pse's are  atus to add features to allow the model to take this  tached to the writer's implicit pse; heurTwo is the into account.  Other sources of error include  erparse-based heuristic that relies solely on the  domironeous sentence boundary detection, parenthetical  statements (which the parser does not treat correctly  for our purposes) and other parse errors, partial  quoWe use 10-fold cross-validation on the  evaluatations, as well as some errors in the annotation.  tion data to generate training and test data (although  Examining the learned trees is difficult because  the heuristics, of course, do not require training).  of their size, but looking at one tree to depth three  The results of the decision tree method and the two  heuristics are presented in Table 3.  14p < 0.01, using an approximate randomization test with 9,999 trials. See (Eisner, 1996, page 17) and (Chinchor et al., 1993, pages 430-433) for descriptions of this method.  13That is, heurTwo attaches a pse to the pse most immedi-15Using the same test as above, p < 0.01, except for the ately dominating it in the dependency tree. If no other pse performance on sentences with more than 5 pse's, because of dominates it, a pse is attached to the writer's pse.  the small amount of data, where p < 0.02.  reveals a fairly intuitive model. Ignoring the prob-statistics, pages 182 201. Chapman & Hall,London.  abilities, the tree decides pseparent is the parent  of pse  group/ind/IND-program.html.  target if and only if pseparent is the writer's  pse (and pse  Nancy Chinchor, Lynette Hirschman, and David Lewis.  target is not in quotation marks), or  if pse  parent is the word said.  For all the trees  An analysis of the third message understanding  learned, the root feature was either the writer pse  Computational Linguistics,  test or the partial-parse-based domination feature.  Michael John Collins. 1999. Head-driven Statistical  Conclusions and Future Work  Models for Natural Language Parsing. Ph.D. thesis, We have presented the concept of perspective and  University of Pennsylvania, Philadelphia.  speech expressions, and argued that determining  cheva, and Valentin Tablan. 2002. GATE: A  frametheir hierarchical structure is important for natural  work and graphical development environment for  rolanguage understanding of perspective. We have  bust nlp tools and applications. In Proceedings of the shown that identifying the hierarchical structure of  40th Anniversary Meeting of the Association for  Compse's is amenable to automated analysis via a  maputational Linguistics (ACL '02), Philadelphia, July.  chine learning approach, although there is room for  2003. Proceedings of the Workshop on Text  Summarizaimprovement in the results.  tion, Edmonton, Alberta, Canada, May. Presented at In the future, we plan to address the related tasks  the 2003 Human Language Technology Conference.  discussed in Section 2, especially identifying pse's  Jason Eisner. 1996. An empirical comparison of  probaand their immediate sources. We are also interested  bility models for dependency grammar. Technical  Rein ways of improving the machine learning  formuport IRCS-96-11, IRCS, University of Pennsylvania.  Christine Gerard. 2000. Modelling readers of news  arlation of the current task, such as optimizing the  ticles using nested beliefs. Master's thesis, Concordia  binary classifier on the whole-sentence evaluation,  University, Montr al, Qu bec, Canada.  or defining a different binary task that is easier to  Daniel Gildea and Daniel Jurafsky. 2002. Automatic  lalearn. Nevertheless, we believe that our results  probeling of semantic roles. Computational Linguistics, vide a step towards the development of natural lan-28(3):245 288.  guage systems that can extract and summarize the  Dekang Lin. 1995. A dependency-based method for  viewpoints and perspectives expressed in text while  evaluating broad-coverage parsers. In IJCAI, pages taking into account the multi-stage information fil-1420 1427.  tering process that can mislead more na ve systems.  1998. Proceedings of the Seventh Message  Understanding Conference (MUC-7). Morgan Kaufman, April.  Acknowledgments  NIST. 2003. Proceedings of The Twelfth Text REtrieval Conference (TREC 2003), Gaithersburg, MD, Novem-This work was supported in part by NSF Grant  IISber. NIST special publication SP 500-255.  0208028 and by an NSF Graduate Research Fellowship.  We thank Rebecca Hwa for creating the dependency  parses. We also thank the Cornell NLP group for  helpof the English Language. Longman, New York.  ful suggestions on drafts of this paper. Finally, we thank J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,  Janyce Wiebe and Theresa Wilson for draft suggestions  B. Fraser, D. Litman, D. Pierce, E. Riloff, and T.  Wiland advice regarding this problem and the NRRC corpus.  son. 2002. NRRC Summer Workshop on  MultiplePerspective Question Answering Final Report. Tech  Steven Abney. 1997. The SCOL manual. cass is  available from http://www.vinartus.net/spa/scol1h.tar.gz.  B. Fraser, D. Litman, D. Pierce, E. Riloff, T. Wilson,  Sabine Bergler. 1993. Semantic dimensions in the field  D. Day, and M. Maybury. 2003. Recognizing and  Orof reporting verbs. In Proceedings of the Ninth  Anganizing Opinions Expressed in the World Press. In  nual Conference of the University of Waterloo Centre Papers from the AAAI Spring Symposium on New Di-for the New Oxford English Dictionary and Text  Rerections in Question Answering (AAAI tech report  SSsearch, Oxford, England, September.  03-07). March 24-26, 2003. Stanford.  Steven Bethard, Hong Yu, Ashley Thornton, Vasileios  Janyce Wiebe. 1994. Tracking point of view in narrative.  Hatzivassiloglou, and Dan Jurafsky. 2004. Automatic  Computational Linguistics, 20(2):233 287.  extraction of opinion propositions and their holders.  Janyce Wiebe. 2002. Instructions for annotating  opinIn Working Notes of the AAAI Spring Symposium on  ions in newspaper articles. Technical Report  TR-02Exploring Attitude and Affect in Text: Theories and  101, Dept. of Comp. Sci., University of Pittsburgh.  Applications. March 22-24, 2004, Stanford.  Fei Xia and Martha Palmer. 2001. Converting  depenWray Buntine. 1993. Learning classification trees. In  dency structures to phrase structures. In Proc. of the D. J. Hand, editor, Artificial Intelligence frontiers in HLT Conference. 