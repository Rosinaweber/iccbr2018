 Amazon Mechanical Turk for Subjectivity Word Sense Disambiguation Cem Akkaya  Alexander Conrad  University of Pittsburgh University of Pittsburgh University of Pittsburgh University of North Texas cem@cs.pitt.edu  Recently researchers have been investigating  Amazon Mechanical Turk (MTurk) as a source of  Amazon Mechanical Turk (MTurk) is a  marketplace for so-called human intelligence  non-expert natural language annotation, which is a  tasks (HITs), or tasks that are easy for  hucheap and quick alternative to expert annotations  mans but currently difficult for automated  proProviders upload tasks to MTurk  In this paper, we utilize MTurk to obtain training  which workers then complete. Natural  landata for Subjectivity Word Sense Disambiguation  guage annotation is one such human  intelli(SWSD) as described in (Akkaya et al., 2009). The  gence task. In this paper, we investigate  usgoal of SWSD is to automatically determine which  ing MTurk to collect annotations for  Subjecword instances in a corpus are being used with  subtivity Word Sense Disambiguation (SWSD),  a coarse-grained word sense disambiguation  jective senses, and which are being used with  obWe investigate whether we can use  jective senses. SWSD is a new task which suffers  MTurk to acquire good annotations with  refrom the absence of a substantial amount of  annospect to gold-standard data, whether we can  tated data and thus can only be applied on a small  filter out low-quality workers (spammers), and  scale. SWSD has strong connections to WSD. Like  whether there is a learning effect associated  supervised WSD, it requires training data where  tarwith repeatedly completing the same kind of  get word instances words which need to be  distask. While our results with respect to  spammers are inconclusive, we are able to  obambiguated by the system are labeled as having  tain high-quality annotations for the SWSD  an objective sense or a subjective sense. (Akkaya  task. These results suggest a greater role for  et al., 2009) show that SWSD may bring substantial  MTurk with respect to constructing a large  improvement in subjectivity and sentiment analysis,  scale SWSD system in the future, promising  if it could be applied on a larger scale. The good  substantial improvement in subjectivity and  news is that training data for 80 selected keywords is  sentiment analysis.  enough to make a substantial difference (Akkaya et  al., 2009). Thus, large scale SWSD is feasible. We  Introduction  hypothesize that annotations for SWSD can be  proMany Natural Language Processing (NLP) systems  vided by non-experts reliably if the annotation task  rely on large amounts of manually annotated data  is presented in a simple way.  that is collected from domain experts. The  annoThe annotations obtained from MTurk workers  tation process to obtain this data is very laborious  are noisy by nature, because MTurk workers are  and expensive. This makes supervised NLP systems  not trained for the underlying annotation task. That  subject to a so-called knowledge acquisition  bottleis why previous work explored methods to assess  neck. For example, (Ng, 1997) estimates an effort of  annotation quality and to aggregate multiple noisy  16 person years to construct training data for a  highannotations for high reliability (Snow et al., 2008;  Callison-Burch, 2009). It is understandable that not  biguation (WSD) system.  every worker will provide high-quality annotations,  Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk, pages 195 203, Los Angeles, California, June 2010. c  2010 Association for Computational Linguistics  depending on their background and interest.  Unworkers. Providers create HITs using the  Mechanfortunately, some MTurk workers do not follow the  ical Turk API and, for a small fee, upload them to  annotation guidelines and carelessly submit  annotathe HIT database. Workers search through the HIT  tions in order to gain economic benefits with only  database, choosing which to complete in exchange  minimal effort. We define this group of workers  for monetary compensation. Anyone can sign up as  as spammers. We believe it is essential to  distina provider and/or worker. Each HIT has an  associguish between workers as well-meaning annotators  ated monetary value, and after reviewing a worker's  and workers as spammers who should be filtered out  submission, a provider may choose whether to  acas a first step when utilizing MTurk. In this work,  cept the submission and pay the worker the promised  we investigate how well the built-in qualifications in  sum or to reject it and pay the worker nothing. HITs  MTurk function as such a filter.  typically consist of tasks that are easy for humans  Another important question about MTurk workers  but difficult or impossible for computers to complete  is whether they learn to provide better annotations  quickly or effectively, such as annotating images,  over time in the absence of any interaction and  feedtranscribing speech audio, or writing a summary of  back. The presence of a learning effect may support  working with the same workers over a long time and  One challenge for requesters using MTurk is that  creating private groups of workers. In this work, we  of filtering out spammers and other workers who  also examine if there is a learning effect associated  consistently produce low-quality annotations. In  order to allow requesters to restrict the range of  workTo summarize, in this work we investigate the  folers who can complete their tasks, MTurk provides  several types of built-in statistics, known as  qualifications. One such qualification is approval rating,  Can MTurk be utilized to collect reliable  traina statistic that records a worker's ratio of accepted  HITs compared to the total number of HITs  submitted by that worker. Providers can require that a  Are the built-in methods provided by MTurk  worker's approval rating be above a certain threshold  enough to avoid spammers ?  before allowing that worker to submit one of his/her  Is there a learning effect associated with MTurk  HITs. Country of residence and lifetime approved  number of HITs completed also serve as built-in  qualifications that providers may check before  alThe remainder of the paper is organized as  following workers to access their HITs.2 Amazon also  lows. In Section 2, we give general background  inallows providers to define their own qualifications.  formation on the Amazon Mechanical Turk service.  Typically, provider-defined qualifications are used to  In Section 3, we discuss sense subjectivity. In  Secensure that HITs which require particular skills are  tion 4, we describe the subjectivity word sense  disonly completed by qualified workers. In most cases,  ambiguation task. In Section 5, we discuss the  deworkers acquire provider-defined qualifications by  sign of our experiment and our filtering mechanisms  completing an online test.  for workers. In Section 6, we evaluate MTurk  annoAmazon also provides a mechanism by which  multiple unique workers can complete the same HIT.  7, we review related work. In Section 8, we draw  The number of times a HIT is to be completed is  conclusions and discuss future work.  known as the number of assignments for the HIT.  By having multiple workers complete the same HIT,  Amazon Mechanical Turk  2According to the terms of use, workers are prohibited from Amazon Mechanical Turk (MTurk)1 is a market-having more than one account, but to the writer's knowledge place for so-called human intelligence tasks, or  there is no method in place to enforce this restriction. Thus, HITs. MTurk has two kinds of users: providers and  a worker with a poor approval rating could simply create a new account, since all accounts start with an approval rating 1http://mturk.amazon.com  of 100%.  this source of errors. (Akkaya et al., 2009) shows  His alarm grew.  that SWSD helps with various subjectivity and  senalarm, dismay, consternation (fear resulting from the aware-timent analysis systems by ignoring false hits.  => fear, fearfulness, fright (an emotion experienced in anticipation of some specific pain or danger (usually accompa-4  nied by a desire to flee or fight))  Subjectivity Word Sense Disambiguation  What's the catch?  catch (a hidden drawback; it sounds good but what's the Our target task is Subjectivity Word Sense Disam-catch? )  biguation (SWSD). SWSD aims to determine which  => drawback (the quality of being a hindrance; he  word instances in a corpus are being used with  subpointed out all the drawbacks to my plan )  jective senses and which are being used with  objective senses. It can be considered to be a  coarseThe alarm went off.  grained application-specific WSD that distinguishes  alarm, warning device, alarm system (a device that signals the between only two senses: (1) the subjective sense  occurrence of some undesirable event)  and (2) the objective sense.  => device (an instrumentality invented for a particular pur-pose; the device is small enough to wear on your wrist ; a Subjectivity word sense annotation is done in the  device intended to conserve water )  following way. We try to keep the annotation task  He sold his catch at the market.  for the worker as simple as possible. Thus, we do  catch, haul (the quantity that was caught; the catch was only not directly ask them if the instance of a target word  has a subjective or an objective sense (without any  => indefinite quantity (an estimated quantity)  sense inventory), because the concept of subjectivity  Figure 1: Subjective and objective word sense examples.  is fairly difficult to explain to someone who does not  have any linguistics background. Instead we show  MTurk workers two sets of senses one subjective  techniques such as majority voting among the  subset and one objective set for a specific target word  missions can be used to aggregate the results for  and a text passage in which the target word appears.  some types of HITs, resulting in a higher-quality  Their job is to select the set that best reflects the  final answer.  Previous work (Snow et al., 2008)  meaning of the target word in the text passage. The  demonstrates that aggregating worker submissions  specific sense set automatically gives us the  subjecoften leads to an increase in quality.  tivity label of the instance. This makes the  annotation task easier for them as (Snow et al., 2008) shows  Word Sense Subjectivity  that WSD can be done reliably by MTurk workers.  (Wiebe and Mihalcea, 2006) define subjective  exThis approach presupposes a set of word senses that  pressions as words and phrases being used to  exhave been annotated as subjective or objective. The  press mental and emotional states, such as  speculaannotation of senses in a dictionary for subjectivity  tions, evaluations, sentiments, and beliefs. Many  apis not difficult for an expert annotator. Moreover,  proaches to sentiment and subjectivity analysis rely  it needs to be done only once per target word,  alon lexicons of such words (subjectivity clues).  Howlowing us to collect hundreds of subjectivity labeled  ever, such clues often have both subjective and  obinstances for each target word through MTurk.  jective senses, as illustrated by (Wiebe and  MihalIn this annotation task, we do not inform the  cea, 2006). Figure 1 provides subjective and  objecMTurk workers about the nature of the sets. This  tive examples of senses.  means the MTurk workers have no idea that they are  (Akkaya et al., 2009) points out that most  subannotating subjectivity of senses; they are just  sejectivity lexicons are compiled as lists of keywords,  lecting the set which contains a sense matching the  rather than word meanings (senses). Thus,  subjecusage in the sentence or being as similar to it as  postivity clues used with objective senses false hits  sible. This ensures that MTurk workers are not  biare a significant source of error in subjectivity and  ased by the contextual subjectivity of the sentence  sentiment analysis. SWSD specifically deals with  while tagging the target word instance.  et al., 2009). This dataset (called subjSENSEVAL)  { look, appear, seem } give a certain impression or have a consists of target word instances in a corpus labeled  certain outward aspect; She seems to be sleeping ; This ap-as S or O, indicating whether they are used with  pears to be a very difficult problem ; This project looks fishy ;  They appeared like people who had not eaten or slept for a a subjective or objective sense. It is based on the  { appear, seem } seem to be true, probable, or apparent; It riff and Palmer, 2000), SENSEVAL2 (Preiss and  seems that he is very gifted ; It appears that the weather in Yarowsky, 2001), and SENSEVAL3 (Mihalcea and  California is very bad  Edmonds, 2004). SubjSENSEVAL consists of  instances for 39 ambiguous (having both subjective  { appear } come into sight or view; He suddenly appeared and objective meanings) target words.  at the wedding ; A new star appeared on the horizon  (Akkaya et al., 2009) also provided us with  sub{ appear, come out } be issued or published, as of news in a jectivity labels for word senses which are used in the  paper, a book, or a movie; Did your latest book appear yet? ; creation of subjSENSEVAL. Sense labels of the  tar The new Woody Allen film hasn't come out yet  get word senses are defined on the sense inventory  { appear, come along } come into being or existence, or ap-of the underlying corpus (Hector for SENSEVAL1;  pear on the scene; Then the computer came along and changed WordNet1.7 for SENSEVAL2; and WordNet1.7.1  our lives ; Homo sapiens appeared millions of years ago  This means the target words  { appear } appear as a character on stage or appear in a play, from SENSEVAL1 have their senses annotated in  etc.; Gielgud appears briefly in this movie ; She appeared in  Hamlet' on the London  the Hector dictionary, while the target words from  { appear } present oneself formally, as before a (judicial) au-SENSEVAL2 and SENSEVAL3 have their senses  thority; He had to appear in court last month ; She appeared annotated in WordNet1.7. We make use of these la-on several charges of theft  beled sense inventories to build our subjective and  Figure 2: Sense sets for target word appear .  objective sets of senses, which we present to the  spectively. We want to have a uniform sense  repBelow, we describe a sample annotation problem.  resentation for the words we ask subjectivity sense  An MTurk worker has access to the following two  labels for. Thus, we consider only SENSEVAL2 and  sense sets of the target word appear , as seen in  SENSEVAL3 subsets of subjSENSEVAL, because  Figure 2. The information that the first sense set is  SENSEVAL1 relies on a sense inventory other than  subjective and second sense set is objective is not  available to the worker. The worker is presented  with the following text passage holding the target  word appear .  We chose randomly 8 target words that have a  distriIt's got so bad that I don't even know what  bution of subjective and objective instances in  subto say. Charles |target| appeared |target|  jSENSEVAL with less skew than 75%. That is, no  somewhat embarrassed by his own  behavmore than 75% of a word's senses are subjective or  ior.  The hidden speech was coming, I  objective. Our concern is that using skewed data  could tell.  might bias the workers to choose from the more  freIn this passage, the MTurk worker should be able  quent label without thinking much about the  probto understand that appeared refers to the outward  lem. Another important fact is that these words with  impression given by Charles . This use of appear is  low skew are more ambiguous and responsible for  most similar to the first entry in sense set one; thus,  more false hits. Thus, these target words are the ones  the correct answer for this problem is Sense Set-1.  for which we really need subjectivity word sense  disambiguation. For each of these 8 target words, we  Gold Standard  select 40 passages from subjSENSEVAL in which  The gold standard dataset, on which we evaluate  the target word appears, to include in our  experiMTurk worker annotations, is provided by (Akkaya  ments. Table 1 summarizes the selected target words  Word  Word  Group1  Location: USA  appear  Location: USA  Group2  Location: USA  Group3  Approved HITs > 500  Table 2: Constraints for each HIT group.  and their label distribution. In this table, frequent la-Group1 required only that the MTurk workers are  bel percentage (FLP) represents the skew for each  located in the US. This group is the least constrained  word. A word's FLP is equal to the percent of the  one. Group2 additionally required an approval rate  senses that are of the most frequently occurring type  greater than 96%. Group3 is the most constrained  of sense (subjective or objective) for that word.  one, requiring a lifetime approved HIT number to  We believe this annotation task is a good  candibe greater than 500, in addition to the qualifications  date for attracting spammers. This task requires only  in Group1 and Group2.  binary annotations, where the worker just chooses  We believe that neither location nor approval rate  from one of the two given sets, which is not a  difand location together is enough to avoid spammers.  ficult task. Since it is easy to provide labels, we  While being a US resident does to some extent  guarbelieve that there will be a distinct line, with  reantee English proficiency, it does not guarantee  wellspect to quality of annotations, between spammers  thought answers. Since there is no mechanism in  and mediocre annotators.  place preventing users from creating new MTurk  For our experiments, we created three different  worker accounts at will and since all worker  acHIT groups each having different qualification  recounts are initialized with a 100% approval rate, we  quirements but sharing the same data. To be  condo not think that approval rate is sufficient to avoid  crete, each HIT group consists of the same 320  inserial spammers and other poor annotators. We  hystances: 40 instances for each target word listed in  pothesize that the workers with high approval rate  Table 1. Each HIT presents an MTurk worker with  and a large number of approved HITs have a  reputafour instances of the same word in a text passage  tion to maintain, and thus will probably be careful in  this makes 80 HITs for each HIT group and  their answers. We think it is unlikely that spammers  asks him to choose the set to which the activated  will have both a high approval rate and a large  numsense belongs. We know for each HIT the mapping  ber of completed HITs. Thus, we anticipated that  between sense set numbers and subjectivity. Thus,  Group3's annotations will be of higher quality than  we can evaluate each HIT response on our  goldthose of the other groups.  standard data, as discussed in Section 4.2. We pay  Note that an MTurk worker who has access to the  seven cents per HIT. We consider this to be generous  HITs in one of the HIT groups also has access to  compensation for such a simple task.  HITs in less constrained groups. For example, an  There are many builtin qualifications in MTurk.  MTurk worker who has access to HITs in Group3  We concentrated only on three of them: location,  also has access to HITs in Group2 and Group1. We  HIT approval rate, and approved HITs, as discussed  in Section 2. In our experience, these qualifications  multiple HIT groups because we did not want to  are widely used for quality assurance. As mentioned  influence worker behavior, but instead simulate the  before, we created three different HIT groups in  order to see how well different built-in qualification  In addition to the qualifications described above,  combinations do with respect to filtering spammers.  we also required each worker to take a qualification  These groups starting from the least constrained to  test in order to prove their competence in the  annothe most constrained are listed in Table 2.  tation task. The qualification test consists of 10  simdiffers from the others. We evaluate each group  itself separately on the gold-standard data.  Additionally, we evaluate each worker's performance on the  gold-standard data and inspect their distribution in  Group Evaluation  As mentioned in the previous section, we collect  three annotations for each HIT. They are assigned to  respective trials in the order submitted by the  workers. The results are summarized in Table 3. Trials  are labeled as TX and MV is the majority vote  annotation among the three trials. The final column  contains the baseline agreement where a worker  labels each instance of a word with the most frequent  label of that word in the gold-standard data. It is  clear from this table that, since worker accuracy  always exceeds the baseline agreement,  subjectivple annotation questions identical in form to those  ity word sense annotation can be done reliably by  present in the HITs. These questions are split evenly  MTurk workers. This is very promising.  Considbetween two target words, appear and restraint .  ering the low cost and low time required to obtain  There are a total of five subjective and five objective  MTurk annotations, a large scale SWSD is  realisusages in the test. We required an accuracy of 90%  tic. For example, (Akkaya et al., 2009) shows that  in the qualification test, corresponding to a Kappa  the most frequent 80 lexicon keywords are  responscore of .80, before a worker was allowed to submit  sible for almost half of the false hits in the MPQA  any of our HITs. If a worker failed to achieve a score  Corpus3 (Wiebe et al., 2005; Wilson, 2008), a  corof 90% on an attempt, that worker could try the test  pus annotated for subjective expressions. Utilizing  again after a delay of 4 hours.  MTurk to collect training data for these 80 lexicon  We collected three sets of assignments within  keywords will be quick and cheap and most  imporeach HIT group. In other words, each HIT was  comtantly reliable.  pleted three times by three different workers in each  When we compare groups with each other, we  group. This gives us a total of 960 assignments in  see that the best trial result is achieved in Group3.  each HIT group. A total of 26 unique workers  parHowever, according to McNemar's test (Dietterich,  ticipated in the experiment: 17 in Group1, 17 in  1998), there is no statistically significant difference  Group2 and 8 in Group3. As mentioned before, a  between any trial of any group. On the other hand,  worker is able to participate in all the groups for  the best majority vote annotation is achieved in  which he is qualified. Thus the unique worker  numGroup2, but again there is no statistically significant  bers in each group does not sum up to the total  numdifference between any majority vote annotation of  ber of workers in the experiment, since some  workany group. These results are surprising to us, since  ers participated in the HITs for more than one group.  we do not see any significant difference in the  qualFigure 3 summarizes how workers are distributed  ity of the data throughout different groups.  Worker Evaluation  In this section, we evaluate all 26 workers and group  them as either spammers or well-meaning workers.  We are interested in how accurate the MTurk  annotaAll workers who deviate from the gold-standard by a  tions are with respect to gold-standard data. We are  also interested in how the accuracy of each group  Group3  Group2  Group1  Accuracy  Table 3: Accuracy and kappa scores for each group of workers.  Threshold  Spammer Count  Spammer Percentage  Table 4: Spammer representation in groups.  large margin beyond a certain threshold will be  conification.  sidered to be spammers. As discussed in Section 5,  we require all participating workers to pass a  qualiLearning Effect  fication test before answering HITs. Thus, we know  Expert annotators can learn to provide more  accuthat they are competent to do subjectivity sense  annotations, and providing consistently erroneous  anreports a learning effect early in the annotation  pronotations means that they are probably spammers.  cess. This might be due to the formal and informal  We think a kappa score of 0.6 is a good threshold  interaction between annotators. Another possibility  to distinguish spammers from well-meaning  workis that the annotators might get used to the  annotaers. For this threshold, we had 2 spammers  partion task over time. This is to be expected if there is  ticipating in Group1, 2 spammers in Group2 and  not an extensive training process before the  annota0 spammers in Group3. Table 4 presents spammer  tion takes place.  count and spammer percentage in each group for  On the other hand, the MTurk workers have no  various threshold values. We see that Group3 has  interaction among themselves. They do not receive  consistently fewer spammers and a smaller spammer  any formal training and do not have access to true  percentage. The lowest kappa scores for Group1,  annotations except a few examples if provided by  Group2, and Group3 are .35, .40, and .69,  respecthe requester. These properties make MTurk  worktively. The mean kappa scores for Group1, Group2,  ers a unique annotation workforce. We are interested  and Group3 are .73, .75, and .77, respectively.  if the learning effect common to expert annotators  These results indicate that Group3 is less prone  holds in this unique workforce in the absence of any  to spammers, apparently contradicting Section 6.1.  interaction and feedback. That may justify working  We see the reason when we inspect the data more  with the same set of workers over a long time by  closely. It turns out that spammers contributed in  creating private groups of workers.  Group1 and Group2 only minimally. On the other  We sort annotations of a worker after the  submishand there are two mediocre workers (Kappa of  sion date. This way, we get for each worker an  or0.69) who submit around 1/3 of the HITs in Group3.  dered list of annotations. We split the list into bins  This behavior might be a coincidence. In the face of  of size 40 and we test for an increasing trend in  contradicting results, we think that we need a more  the proportion of successes over time. We use the  extensive study to derive conclusions about the  relaChi-squared Test for binomial proportions (Rosner,  tion between spammer distribution and built-in  qual2006). Using this test, we find that all of the p-values 201  are substantially larger than 0.05. Thus, there is no do. (Snow et al., 2008) also provides a WSD anno-increasing trend in the proportion of successes and  tation task which is similar to our annotation task.  no learning effect. This is true for both mediocre  The difference is the MTurk workers are choosing  workers and very reliable workers. We think that the  results may differ for harder annotation tasks where  the input is more complex and requires some  adjustConclusion and Future Work  In this paper, we address the question of whether  built-in qualifications are enough to avoid  spamRelated Work  The investigation of worker performances  indicates that the lesser constrained a group is the  There has been recently an increasing interest in  more spammers it attracts. On the other hand, we did  Amazon Mechanical Turk. Many researchers have  not find any significant difference between the  qualutilized MTurk as a source of non-expert natural  ity of the annotations for each group. It turns out that language annotation to create labeled datasets. In  workers considered as spammers contributed only  minimally. We do not know if it is just a coincidence  create a corpus of why-questions and corresponding  or if it is correlated to the task definition. We did not answers on which QA systems may be developed.  get conclusive results. We need to do more extensive  (Kaisser and Lowe, 2008) work on a similar task.  experiments before arriving at conclusions.  They make use of MTurk workers to identify  senAnother aspect we investigated is the learning  eftences in documents as answers and create a corpus  fect. Our results show that there is no improvement  of question-answer sentence pairs. MTurk is also  in annotator reliability over time. We should not  exconsidered in other fields than natural language  propect MTurk workers to provide more consistent  ancessing. For example, (Sorokin and Forsyth, 2008)  notations over time. This will probably be the case  utilizes MTurk for image labeling. Our ultimate goal  in similar annotation tasks. For harder annotation  is similar; namely, to build training data (in our case  tasks (e.g. parse tree annotation) things may be  different. An interesting follow-up would be whether  Several studies have concentrated specifically on  showing the answers of other workers on the same  the quality aspect of the MTurk annotations. They  HIT will promote learning.  investigated methods to assess annotation quality  We presented our subjectivity sense annotation  and to aggregate multiple noisy annotations for high  task to the worker in a very simple way. The  anreliability. (Snow et al., 2008) report MTurk  annotation results prove that subjectivity word sense  notation quality on various NLP tasks (e.g. WSD,  annotation can be done reliably by MTurk workers.  Textual Entailment, Word Similarity) and define  This is very promising since the MTurk annotations  a bias correction method for non-expert  annotacan be collected for low costs in a short time  petors. (Callison-Burch, 2009) uses MTurk workers  riod. This implies that a large scale general SWSD  for manual evaluation of automatic translation  qualcomponent, which can help with various subjectivity  ity and experiments with weighed voting to  comand sentiment analysis tasks, is feasible. We plan to  work with selected workers to collect new annotated  fine various annotation quality measures and show  data for SWSD and use this data to train a SWSD  that they are useful for selecting annotations leading  to more accurate classifiers. Our work investigates  the effect of built-in qualifications on the quality of  Acknowledgments  MTurk annotations.  This material is based in part upon work  supported by National Science Foundation awards  IISment annotations on political blog snippets. (Snow  0916046 and IIS-0917170 and by Department of  et al., 2008) utilizes MTurk for affective text  annotaHomeland Security award N000140710152. The  aution task. In both works, MTurk workers annotated  thors are grateful to the three paper reviewers for  larger entities but on a more detailed scale than we  their helpful suggestions.  J. Wiebe and R. Mihalcea. 2006. Word sense and  subjectivity. In (ACL-06), Sydney, Australia.  Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.  Subjectivity word sense disambiguation. In  ConferAnnotating expressions of opinions and emotions in  ence on Empirical Methods in Natural Language  Prolanguage. Language Resources and Evaluation  (forcessing (EMNLP 2009).  merly Computers and the Humanities), 39(2/3):164  Chris Callison-Burch. 2009. Fast, cheap, and creative:  evaluating translation quality using amazon's  mechanTheresa Wilson. 2008. Fine-grained Subjectivity and  ical turk. In EMNLP '09: Proceedings of the 2009  Sentiment Analysis: Recognizing the Intensity, Polar-Conference on Empirical Methods in Natural  Lanity, and Attitudes of private states. Ph.D. thesis, Intel-guage Processing, pages 286 295, Morristown, NJ, ligent Systems Program, University of Pittsburgh.  USA. Association for Computational Linguistics.  Thomas G. Dietterich. 1998. Approximate statistical  tests for comparing supervised classification learning  algorithms. Neural Computation, 10:1895 1923.  2009. Data quality from crowdsourcing: a study of  annotation selection criteria. In HLT '09: Proceedings of the NAACL HLT 2009 Workshop on Active Learning  for Natural Language Processing, pages 27 35, Morristown, NJ, USA. Association for Computational  Lining a research collection of question answer  sentence pairs with amazons mechanical turk. In  Proceedings of the Sixth International Language  Resources and Evaluation (LREC'08). http://www.lrec-conf.org/proceedings/lrec2008/.  2008. Collecting a why-question corpus for  development and evaluation of an automatic QA-system. In  Proceedings of ACL-08: HLT, pages 443 451, Colum-bus, Ohio, June. Association for Computational  LinHwee Tou Ng. 1997. Getting serious about word sense  disambiguation. In Proceedings of the ACL SIGLEX  Workshop on Tagging Text with Lexical Semantics:  Why,What, and How?  Rebecca Passonneau, Nizar Habash, and Owen Rambow.  2006. Inter-annotator agreement on a multilingual  semantic annotation task. In Proceedings of the Fifth  International Conference on Language Resources and  Bernard Rosner. 2006. Fundamentals of Biostatistics.  Thompson Brooks/Cole.  Rion Snow, Brendan O'Connor, Daniel Jurafsky, and  Andrew Y. Ng. 2008. Cheap and fast but is it good?:  evaluating non-expert annotations for natural language  tasks. In EMNLP '08: Proceedings of the Conference  on Empirical Methods in Natural Language  Processing, pages 254 263, Morristown, NJ, USA. Association for Computational Linguistics.  A. Sorokin and D. Forsyth. 2008. Utility data annotation with amazon mechanical turk. pages 1 8, june. 