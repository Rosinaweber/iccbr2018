 Jordan Boyd-Graber and Philip Resnik. Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation. Empirical Methods in Natural Language Processing, 2010, 11 pages.  Author = {Jordan Boyd-Graber and Philip Resnik},  Booktitle = {Empirical Methods in Natural Language Processing}, Location = {Cambridge, MA},  Title = {Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation},  Downloaded from http://cs.colorado.edu/~jbg/docs/jbg-mlslda-2010.pdf  Holistic Sentiment Analysis Across Languages:  Multilingual Supervised Latent Dirichlet Allocation  UMD iSchool  Department of Linguistics  and UMIACS  and UMIACS  University of Maryland  University of Maryland  College Park, MD  Up to this point, multiple languages have been  addressed in sentiment analysis primarily by  transIn this paper, we develop multilingual  superferring knowledge from a resource-rich language to  vised latent Dirichlet allocation (MLSLDA),  a probabilistic generative model that allows  a less rich language (Banea et al., 2008), or by  iginsights gleaned from one language's data to  noring differences in languages via translation into  inform how the model captures properties of  English (Denecke, 2008). These approaches are  limother languages. MLSLDA accomplishes this  ited to a view of sentiment that takes place through  by jointly modeling two aspects of text: how  an English-centric lens, and they ignore the  potenmultilingual concepts are clustered into  themattial to share information between languages.  Ideically coherent topics and how topics  associally, learning sentiment cues holistically, across  lanated with text connect to an observed  regresguages, would result in a richer and more globally  sion variable (such as ratings on a sentiment  scale). Concepts are represented in a general  hierarchical framework that is flexible enough  In this paper, we introduce Multilingual  Superto express semantic ontologies, dictionaries,  vised Latent Dirichlet Allocation (M  clustering constraints, and, as a special,  degenerate case, conventional topic models. Both  model for sentiment analysis on a multilingual  corthe topics and the regression are discovered  pus. MLSLDA discovers a consistent, unified picture  via posterior inference from corpora. We show  of sentiment across multiple languages by learning  MLSLDA can build topics that are consistent  topics, probabilistic partitions of the vocabulary  across languages, discover sensible bilingual  that are consistent in terms of both meaning and  relevance to observed sentiment. Our approach makes  gual corpora to better predict sentiment.  neither parallel corpora nor machine translation.  Sentiment analysis (Pang and Lee, 2008) offers  the promise of automatically discerning how people  The rest of the paper proceeds as follows. In  Secfeel about a product, person, organization, or issue  tion 1, we describe the probabilistic tools that we use  based on what they write online, which is potentially  to create consistent topics bridging across languages  of great value to businesses and other organizations. and the MLSLDA model. In Section 2, we present However, the vast majority of sentiment resources  the inference process. We discuss our set of  semanand algorithms are limited to a single language, tic bridges between languages in Section 3, and our ally English (Wilson, 2008; Baccianella and experiments in Section 4 demonstrate that this ap-tiani, 2010). Since no single language captures a  proach functions as an effective multilingual topic  majority of the content online, adopting such a model, discovers sentiment-biased topics, and uses ited approach in an increasingly global community  risks missing important details and trends that might  dictions across languages. Sections 5 and 6 discuss  only be available when text in multiple languages is  related research and discusses future work,  respectaken into account.  tively.  1 Predictions from Multilingual Topics  p(h ao|z) all tend to be high at the same time, or low  at the same time. More generally, the structure of our  As its name suggests, MLSLDA is an extension of  model must encourage topics to be consistent across  Latent Dirichlet allocation (LDA) (Blei et al., 2003), languages, and Dirichlet distributions cannot encode a modeling approach that takes a corpus of correlations between elements.  notated documents as input and produces two  outOne possible solution to this problem is to use the  puts, a set of topics and assignments of documents  multivariate normal distribution, which can produce  to topics. Both the topics and the assignments are  correlated multinomials (Blei and Lafferty, 2005),  probabilistic: a topic is represented as a probability  in place of the Dirichlet distribution. This has been  distribution over words in the corpus, and each done successfully in multilingual settings (Cohen ument is assigned a probability distribution over all  and Smith, 2009). However, such models complicate  the topics. Topic models built on the foundations of  inference by not being conjugate.  LDA are appealing for sentiment analysis because  Instead, we appeal to tree-based extensions of the  the learned topics can cluster together Dirichlet distribution, which has been used to induce bearing words, and because topic distributions are a  correlation in semantic ontologies (Boyd-Graber et  parsimonious way to represent a document.1  al., 2007) and to encode clustering constraints  (AnLDA has been used to discover latent structure  drzejewski et al., 2009). The key idea in this  approach is to assume the vocabularies of all languages  al., 2006) and authorship (Rosen-Zvi et al., 2004)). are organized according to some shared semantic MLSLDA extends the approach by ensuring that this  structure that can be represented as a tree. For  conlatent structure the underlying topics is creteness in this section, we will use WordNet (Miller, tent across languages. We discuss multilingual topic  1990) as the representation of this multilingual  semodeling in Section 1.1, and in Section 1.2 we show  mantic bridge, since it is well known, offers  convehow this enables supervised regression regardless of  nient and intuitive terminology, and demonstrates the  full flexibility of our approach. However, the model  1.1 Capturing Semantic Correlations  we describe generalizes to any tree-structured  representation of multilingual knowledge; we discuss  Topic models posit a straightforward generative some alternatives in Section 3.  cess that creates an observed corpus. For each  docuWordNet organizes a vocabulary into a rooted,  diment d, some distribution d over unobserved topics  rected acyclic graph of nodes called synsets, short for  is chosen. Then, for each word position in the synonym sets. A synset is a child of another synset ument, a topic z is selected. Finally, the word for  if it satisfies a hyponomy relationship; each child is  that position is generated by selecting from the topic  a more specific instantiation of its parent concept  indexed by z. (Recall that in LDA, a topic is a  (thus, hyponomy is often called an isa relationship).  distribution over words).  For example, a dog is a canine is an animal is  In monolingual topic models, the topic distribution  a living thing, etc. As an approximation, it is not  is usually drawn from a Dirichlet distribution. unreasonable to assume that WordNet's structure of ing Dirichlet distributions makes it easy to specify  meaning is language independent, i.e. the concept  sparse priors, and it also simplifies posterior encoded by a synset can be realized using terms in ence because Dirichlet distributions are conjugate  different languages that share the same meaning. In  to multinomial distributions. However, drawing practice, this organization has been used to create ics from Dirichlet distributions will not suffice if  many alignments of international WordNets to the  our vocabulary includes multiple languages. If we  original English WordNet (Ordan and Wintner, 2007;  are working with English, German, and Chinese at  the same time, a Dirichlet prior has no way to  faUsing the structure of WordNet, we can now  devor distributions z such that p(good|z), p(gut|z), and scribe a generative process that produces a distribu-1The latter property has also made LDA popular for  infortion over a multilingual vocabulary, which  encourmation retrieval (Wei and Croft, 2006)).  ages correlations between words with similar  meanings regardless of what language each word is in. Topic 1 is about baseball in English and about travel For each synset h, we create a multilingual word  in German). Separating path from emission helps  distribution for that synset as follows:  ensure that topics are consistent across languages.  1. Draw transition probabilities  Having defined topic distributions in a way that can  2. Draw stop probabilities  preserve cross-language correspondences, we now  3. For each language l, draw emission probabilities for  use this distribution within a larger model that can  that synset  discover cross-language patterns of use that predict  For conciseness in the rest of the paper, we will refer  to this generative process as multilingual Dirichlet  1.2 The MLSLDA Model  hierarchy, or MULTDIRHIER( , , ).2 Each  observed token can be viewed as the end result of a  We will view sentiment analysis as a regression  probsequence of visited synsets  lem: given an input document, we want to predict  . At each node in the  tree, the path can end at node  a real-valued observation y that represents the  sentii with probability i,1,  or it can continue to a child synset with probability  ment of a document. Specifically, we build on  supervised latent Dirichlet allocation (SLDA, (Blei and  i,0. If the path continues to another child synset, it  visits child  McAuliffe, 2007)), which makes predictions based  j with probability i,j. If the path ends at  on the topics expressed in a document; this can be  k with probability i,l,k.3  The probability of a word being emitted from a path  thought of projecting the words in a document to low  with visited synsets  dimensional space of dimension equal to the number  r and final synset h in language  of topics. Blei et al. showed that using this latent  l is therefore  topic structure can offer improved predictions over  regressions based on words alone, and the approach fits  well with our current goals, since word-level cues are  unlikely to be identical across languages. In addition  to text, SLDA has been successfully applied to other  domains such as social networks (Chang and Blei,  Note that the stop probability h is independent of  2009) and image classification (Wang et al., 2009).  language, but the emission h,l is dependent on the  The key innovation in this paper is to extend SLDA  language. This is done to prevent the following by creating topics that are globally consistent across nario: while synset A is highly probable in a topic  languages, using the bridging approach above.  and words in language 1 attached to that synset have  We express our model in the form of a  probabilishigh probability, words in language 2 have low tic generative latent-variable model that generates ability. If this could happen for many synsets in  documents in multiple languages and assigns a  reala topic, an entire language would be effectively valued score to each document. The score comes lenced, which would lead to inconsistent topics (e.g. from a normal distribution whose sum is the dot prod-2Variables  uct between a regression parameter that encodes  h, h,l, and h are hyperparameters. Their mean  is fixed, but their magnitude is sampled during inference (i.e.  the influence of each topic on the observation and  is constant, but h,i is not). For the bushier bridges,  a variance 2. With this model in hand, we use  sta(e.g. dictionary and flat), their mean is uniform. For GermaNet, tistical inference to determine the distribution over  we took frequencies from two balanced corpora of German and  English: the British National Corpus (University of Oxford,  latent variables that, given the model, best explains  2006) and the Kern Corpus of the Digitales W rterbuch der  The generative model is as follows:  We took these frequencies and propagated them through the  multilingual hierarchy, following LDAWN's (Boyd-Graber et  1. For each topic i = 1 . . . K, draw a topic distribution  al., 2007) formulation of information content (Resnik, 1995) as a Bayesian prior. The variance of the priors was initialized to be  1.0, but could be sampled during inference.  2. For each document d = 1 . . . M with language ld:  3Note that the language and word are taken as given, but the  (a) Choose a distribution over topics d  path through the semantic hierarchy is a latent random variable.  (b) For each word in the document n = 1 . . . Nd, choose a topic assignment  and a path d,n ending at word wd,n according  to Equation 1 using { z ,  3. Choose a response variable from y  z, 2 , where  Crucially, note that the topics are not  independent of the sentiment task; the regression encourages  terms with similar effects on the observation y to  be in the same topic. The consistency of topics  described above allows the same regression to be done  for the entire corpus regardless of the language of the  Sentiment Prediction  Figure 1: Graphical model representing MLSLDA.  2 Inference  Shaded nodes represent observations, plates denote  replication, and lines show probabilistic dependencies.  Finding the model parameters most likely to explain  the data is a problem of statistical inference. We  employ stochastic EM (Diebolt and Ip, 1996), using a  probability of taking a path r is then  Gibbs sampler for the E-step to assign words to paths  and topics. After randomly initializing the topics,  we alternate between sampling the topic and path  of a word (z  d,n, d,n) and finding the regression  parameters that maximize the likelihood. We jointly  sample the topic and path conditioning on all of the  other path and document assignments in the corpus,  Transition  selecting a path and topic with probability  Each of these three terms reflects a different influence  Equation 3 reflects the multilingual aspect of this  on the topics from the vocabulary structure, the model.  The conditional topic distribution for  ument's topics, and the response variable. In the next  SLDA (Blei and McAuliffe, 2007) replaces this term  paragraphs, we will expand each of them to derive  with the standard Multinomial-Dirichlet. However,  the full conditional topic distribution.  we believe this is the first published SLDA-style  As discussed in Section 1.1, the structure of the  model using MCMC inference, as prior work has  topic distribution encourages terms with the same  used variational inference (Blei and McAuliffe, 2007;  meaning to be in the same topic, even across Chang and Blei, 2009; Wang et al., 2009).  guages. During inference, we marginalize over  posBecause the observed response variable depends  sible multinomial distributions , , and , using  on the topic assignments of a document, the  condithe observed transitions from i to j in topic k; Tk,i,j, tional topic distribution is shifted toward topics that stop counts in synset i in topic k, Ok,i,0; continue  explain the observed response. Topics that move the  counts in synsets i in topic k, Ok,i,1; and emission  predicted response yd toward the true yd will be  facounts in synset i in language l in topic k, Fk,i,l. The  vored. We drop terms that are constant across all  topics for the effect of the response variable,  3 Bridges Across Languages  In Section 1.1, we described connections across  languages as offered by semantic networks in a general  way, using WordNet as an example. In this section,  we provide more specifics, as well as alternative ways  Other words' influence  of building semantic connections across languages.  Flat First, we can consider a degenerate mapping  that is nearly equivalent to running SLDA  independently across multiple languages, relating topics only  This word's influence  based on the impact on the response variable.  ConThe above equation represents the supervised aspect  sider a degenerate tree with only one node, with all  of the model, which is inherited from SLDA.  words in all languages associated with that node. This  Finally, there is the effect of the topics already  is consistent with our model, but there is really no  assigned to a document; the conditional distribution  shared semantic space, as all emitted words must  favors topics already assigned in a document,  come from this degenerate synset and the model  only represents the output distribution for this single  WordNet We took the alignment of GermaNet to  This term represents the document focus of this  moved all synsets that were had no mapped German  model; it is present in all Gibbs sampling inference  words. Any German synsets that did not have English  schemes for LDA (Griffiths and Steyvers, 2004).  translations had their words mapped to the lowest  Multiplying together Equations 3, 4, and 5 allows  extant English hypernym (e.g. beinbruch, a  brous to sample a topic using the conditional distribution  ken leg, was mapped to fracture ). We stemmed  from Equation 2, based on the topic and path of the  all words to account for inflected forms not being  other words in all languages. After sampling the  present (Porter and Boulton, 1970). An example  path and topic for each word in a document, we then  of the paths for the German word wunsch (wish,  find new regression parameters that maximize the  request) is shown in Figure 2(a).  likelihood conditioned on the current state of the  sampler. This is simply a least squares regression  Dictionaries A dictionary can be viewed as a many  using the topic assignments  to many mapping, where each entry  or more words in one language  Prediction on documents for which we don't have  an observed  i in another language. Entries were taken  d is equivalent to marginalizing over  from an English-German dictionary (Richter, 2008)  yd and sampling topics for the document from  Equaa Chinese-English dictionary (Denisowski, 1997),  tions 3 and 5. The prediction for yd is then the dot  and a Chinese-German dictionary (Hefti, 2005). As  product of and the empirical topic distribution zd.  with WordNet, the words in entries for English and  We initially optimized all hyperparameters using  German were stemmed to improve coverage. An  slice sampling. However, we found that the example for German is shown in Figure 2(b).  sion variance 2 was not stable. Optimizing 2 seems  to balance between modeling the language in the Algorithmic Connections In addition to hand-uments and the prediction, and thus is sensitive to  curated connections across languages, one could also  documents' length. Given this sensitivity, we did  consider automatic means of mapping across  lannot optimize 2 for our prediction experiments in  guages, such as using edit distance or local  conSection 4, but instead kept it fixed at 0.25. We leave  text (Haghighi et al., 2008; Rapp, 1995) or  usoptimizing this variable, either through cross ing a lexical translation table obtained from paral-tion or adapting the model, to future work.  lel text (Melamed, 1998). While we experimented  abstraction.n.06  option.n.02  option  option  act  speech_act.n.01  wish.n.04  wish  (a) GermaNet  (b) Dictionary  Figure 2: Two methods for constructing multilingual distributions over words. On the left, paths to the German word  wunsch in GermaNet are shown. On the right, paths to the English word room are shown. Both English and German words are shown; some internal nodes in GermaNet have been omitted for space (represented by dashed lines). Note that different senses are denoted by different internal paths, and that internal paths are distinct from the per-language expression.  with these techniques, constructing appropriate (as there is no associated response variable for Eu-archies from these resources required many arbitrary  roparl documents); this experiment is to demonstrate  decisions about cutoffs and which words to include. the effectiveness of the multilingual aspect of the Thus, we do not consider them in this paper.  model. To test whether the topics learned by the  each document using the probability distribution d  over topic assignments. Each  LSLDA on three criteria: how well  d is a vector of length  it can discover consistent topics across languages  K and is a language-independent representation of  the document.  for matching parallel documents, how well it can  discover sentiment-correlated word lists from  nonFor each document in one language, we computed  aligned text, and how well it can predict sentiment.  the Hellinger distance between it and all of the  documents in the other language and sorted the documents  4.1 Matching on Multilingual Topics  by decreasing distance. The translation of the  document is somewhere in that set; the higher the  normalWe took the 1996 documents from the Europarl ized rank (the percentage of documents with a rank pus (Koehn, 2005) using three bridges: GermaNet, lower than the translation of the document), the better dictionary, and the uninformative flat matching.4 The  the underlying topic model connects languages.  model is unaware that the translations of documents  in one language are present in the other language.  We compare three bridges against what is to our  Note that this does not use the supervised framework  knowledge the only other topic model for unaligned  text, Multilingual Topics for Unaligned Text  (Boyd4For English and German documents in all experiments,  we removed stop words (Loper and Bird, 2002), stemmed  words (Porter and Boulton, 1970), and created a vocabulary  5The bipartite matching was initialized with the dictionary  of the most frequent 5000 words per language (this vocabulary  weights as specified by the Multilingual Topics for Unaligned  limit was mostly done to ensure that the dictionary-based bridge Text algorithm. The matching size was limited to 250 and the  was of manageable size). Documents shorter than fifty content  bipartite matching was only updated on the initial iteration then words were excluded.  held fixed. This yielded results comparable to when the matching  a WordNet-like resource is used as the bridge, the Dictionary  resulting topics are distributions over synsets, not just  As our demonstration corpus, we used the Amherst  Dictionary  Sentiment Corpus (Constant et al., 2009), as it has  documents in multiple languages (English, Chinese,  and German) with numerical assessments of  sentiment (number of stars assigned to the review). We  Dictionary  segmented the Chinese text (Tseng et al., 2005) and  Bridge  used a classifier trained on character n-grams to  remove English-language documents that were mixed  Dictionary  in among the Chinese and German language reviews.  Figure 4 shows extracted topics from  GermanEnglish and German-Chinese corpora. M  is able to distinguish sentiment-bearing topics from  content bearing topics. For example; in the  GermanEnglish corpus, food and children topics are  not associated with a consistent sentiment signal,  Figure 3: Average rank of paired translation document  while religion is associated with a more negative  recovered from the multilingual topic model. Random  sentiment. In contrast, in the German-Chinese  corguessing would yield 0.5; MLSLDA with a dictionary  pus, the religion/society topic is more neutral, and  based matching performed best.  the gender-oriented topic is viewed more negatively.  Negative sentiment-bearing topics have reasonable  Figure 3 shows the results of this experiment. The  words such as pages, k ong p (Chinese for I'm  dictionary-based bridge had the best performance on  afraid that . . . ) and tuo (Chienese for discard ),  the task, ranking a large proportion of documents  and positive sentiment-bearing topics have  reason(0.95) below the translated document once enough  able words such as great, good, and juwel  (Gertopics were available. Although GermaNet is richer, man for jewel ).  its coverage is incomplete; the dictionary structure  The qualitative topics also betray some of the  had a much larger vocabulary and could build a more  weaknesses of the model. For example, in one of  complete multilingual topics. Using comparable the negative sentiment topics, the German word gut  put information, this more flexible model performed  (good) is present. Because topics are distributions  better on the matching task than the existing over words, they can encode the presence of nega-lingual topic model available for unaligned text. The  tions like kein (no) and nicht (not), but not  collodegenerate flat bridge did no better than the baseline  cations like nicht gut. More elaborate topic models  of random guessing, as expected.  that can model local syntax and collocations  (Johnson, 2010) provide options for addressing such  prob4.2 Qualitative Sentiment-Correlated Topics  One of the key tasks in sentiment analysis has been  We do not report the results for sentiment  predicthe collection of lists of words that convey tion for this corpus because the baseline of predicting ment (Wilson, 2008; Riloff et al., 2003). These  a positive review is so strong; most algorithms do  exresources are often created using or in reference  tremely well by always predicting a positive review,  to resources like WordNet (Whitelaw et al., 2005;  ours included.  Baccianella and Sebastiani, 2010). MLSLDA 4.3 Sentiment Prediction vides a method for extracting topical and sentiment-correlated word lists from multilingual corpora. If  We gathered 330 film reviews from a German film  review site (Vetter et al., 2000) and combined them  was updated more frequently.  with a much larger English film review corpus of over  (harry) harry (harry)  (lord) herr  (both)  (universe) all (both)  (community)  life art  healthy rezepte  people thema  ([I'm afraid that...])  (that) dass (both)  (story) story (treasure)  (mostly)  (female) (attractive) attraktiv (both)  (good) gut ([really isn't])  books geschichte parents baby  separate story  (soon) bald (both)  (a) German / English  (b) German / Chinese  Figure 4: Topics, along with associated regression coefficient from a learned 25-topic model on German-English (left) and German-Chinese (right) documents. Notice that theme-related topics have regression parameter near zero, topics discussing the number of pages have negative regression parameters, topics with good, great, h ao (good) and  berzeugt (convinced) have positive regression parameters. For the German-Chinese corpus, note the presence of gut  (good) in one of the negative sentiment topics, showing the difficulty of learning collocations.  Train  Test GermaNet Dictionary Flat  One would expect that prediction improves with a  larger training set. For this model, such an  improveTable 1: Mean squared error on a film review corpus.  ment is seen even when the training set includes no  All results are on the same German test data, varying the  documents in the target language. Note that even the  training data. Over-fitting prevents the model learning on  the German data alone; adding English data to the mix  ful information. After introducing English data, the  allows the model to make better predictions.  model learns to prefer smaller regression parameters  (this can be seen as a form of regularization).  5000 film reviews (Pang and Lee, 2005) to create a  multilingual film review corpus.6  Performance is best when a reasonably large  corThe results for predicting sentiment in German  pus is available including some data in the target  documents with 25 topics are presented in Table 1. language. For each bridge, performance improves On a small monolingual corpus, prediction is very  dramatically, showing that MLSLDA is successfully  poor. The model over-fits, especially when it has  able to incorporate information learned from both  the entire vocabulary to select from. The slightly  languages to build a single, coherent picture of how  better performance using GermaNet and a dictionary  sentiment is expressed in both languages. With the  as topic priors can be viewed as basic feature GermaNet bridge, performance is better than both tion, removing proper names from the vocabulary to  the degenerate and dictionary based bridges, showing  6We followed Pang and Lee's method for creating a  nuthat the model is sharing information both through  merical score between 0 and 1 from a star rating.  the multilingual topics and the regression parameters.  then converted that to an integer by multiplying by 100;  Performance on English prediction is comparable  this was done because initial data preprocessing assumed  to previously published results on this dataset (Blei  integer values (although downstream processing did not  asand McAuliffe, 2007); with enough data, a  monolinsume integer values). The German movie review corpus  is available at http://www.umiacs.umd.edu/  gual model is no longer helped by adding additional  static/downloads_and_media.html  5 Relationship to Previous Research  learned from a corpus, as a prior to seed topics so  that they attract other sentiment bearing words (Mei  The advantages of MLSLDA reside largely in the  et al., 2007; Lin and He, 2009). Other approaches  assumptions that it makes and does not make: view sentiment or perspective as a perturbation of ments need not be parallel, sentiment is a normally  a log-linear topic model (Lin et al., 2008). Such  distributed document-level property, words are techniques could be combined with the multilingual changeable, and sentiment can be predicted as a approach presented here by using distributions over gression on a K-dimensional vector.  words that not only bridge different languages but  By not assuming parallel text, this approach can  also encode additional information. For example, the  be applied to a broad class of corpora. Other vocabulary hierarchies could be structured to encour-tilingual topic models require parallel text, either at  age topics that encourage correlation among similar  sentiment-bearing words (e.g. clustering words  assoor word-level (Kim and Khudanpur, 2004; Zhao and  ciated with price, size, etc.). Future work could also  Xing, 2006). Similarly, other multilingual sentiment  more rigorously validate that the multilingual topics  approaches also require parallel text, often supplied  discovered by MLSLDA are sentiment-bearing via  via automatic translation; after the translated text  is available, either monolingual analysis (Denecke,  In contrast, MLSLDA draws on techniques that  2008) or co-training is applied (Wan, 2009). In view sentiment as a regression problem based on the trast, our approach requires fewer resources for a topics used in a document, as in supervised latent guage: a dictionary (or similar knowledge structure  relating words to nodes in a graph) and comparable  2007) or in finer-grained parts of a document (Titov  text, instead of parallel text or a machine translation  and McDonald, 2008). Extending these models to  multilingual data would be more straightforward.  Rather than viewing one language through the  lens of another language, MLSLDA views all 6 Conclusions  guages through the lens of the topics present in a  document. This is a modeling decision with pros and  MLSLDA is a holistic statistical model for  multicons. It allows a language agnostic decision about  sentiment to be made, but it restricts the or expensive multilingual resources. It discovers ness of the model in terms of sentiment in two ways. connections across languages that can recover la-First, it throws away information important to tent structure in parallel corpora, discover sentiment-timent analysis like syntactic constructions (Greene  correlated word lists in multiple languages, and make  and Resnik, 2009) and document structure accurate predictions across languages that improve ald et al., 2007) that may impact the sentiment rating. with more multilingual data, as demonstrated in the Second, a single real number is not always sufficient  context of sentiment analysis.  to capture the nuances of sentiment. Less critically,  More generally, MLSLDA provides a formalism  assuming that sentiment is normally distributed is not  that can be used to incorporate the many insights of  true of all real-world corpora; review corpora often  topic modeling-driven sentiment analysis to  multihave a skew toward positive reviews. We standardize  lingual corpora by tying together word distributions  responses by the mean and variance of the training  across languages. MLSLDA can also contribute to  data to partially address this issue, but other response  the development of word list-based sentiment  sysdistributions are possible, such as generalized linear  tems: the topics discovered by MLSLDA can serve  models (Blei and McAuliffe, 2007) and vector as a first-pass means of sentiment-based word lists chines (Zhu et al., 2009), which would allow more  for languages that might lack annotated resources.  traditional classification predictions.  MLSLDA also can be viewed as a  sentimentOther probabilistic models for sentiment informed multilingual word sense disambiguation cation view sentiment as a word level feature. Some  (WSD) algorithm. When the multilingual bridge is an  models use sentiment word lists, either given or  explicit representation of sense such as WordNet, part  of the generative process is an explicit assignment Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.  of every word to sense (the path latent variable );  2007. A topic model for word sense disambiguation.  this is discovered during inference. The  dictionaryIn EMNLP.  based technique may be viewed as a disambiguation  Jonathan Chang and David M. Blei. 2009. Relational  via a transfer dictionary. How sentiment prediction  topic models for document networks. In AISTATS.  impacts the implicit WSD is left to future work.  Shay B. Cohen and Noah A. Smith. 2009. Shared  logistic normal distributions for soft parameter tying in  Better capturing local syntax and meaningful  colunsupervised grammar induction. In NAACL.  locations would also improve the model's ability to  Noah Constant, Christopher Davis, Christopher Potts, and  predict sentiment and model multilingual topics, as  Florian Schwarz. 2009. The pragmatics of expressive  would providing a better mechanism for  representcontent: Evidence from large corpora. Sprache und  ing words not included in our bridges. We intend to  develop such models as future work.  gual sentiment analysis. In ICDEW 2008.  7 Acknowledgments  CEDICT.  This research was funded in part by the Army Jean Diebolt and Eddie H.S. Ip, 1996. Markov Chain search Laboratory through ARL Cooperative Agree-Monte Carlo in Practice, chapter Stochastic EM:  ment W911NF-09-2-0072 and by the Office of the  method and application. Chapman and Hall, London.  Director of National Intelligence (ODNI), Alexander Geyken. 2007. The DWDS corpus: A ref-gence Advanced Research Projects Activity (IARPA),  erence corpus for the German language of the 20th  through the Army Research Laboratory. All  statecentury. In Idioms and Collocations: Corpus-based  Linguistic, Lexicographic Studies. Continuum Press.  ments of fact, opinion or conclusions contained  Stephan Greene and Philip Resnik. 2009. More than  herein are those of the authors and should not be  words: Syntactic packaging and implicit sentiment. In  construed as representing the official views or  policies of ARL, IARPA, the ODNI, or the U.S. Thomas L. Griffiths and Mark Steyvers. 2004. Finding ment. The authors thank the anonymous reviewers,  scientific topics. PNAS, 101(Suppl 1):5228 5235.  Jonathan Chang, Christiane Fellbaum, and Lawrence  Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and  Watts for helpful comments. The authors especially  Dan Klein. 2008. Learning bilingual lexicons from  thank Chris Potts for providing help in obtaining and  monolingual corpora. In ACL, Columbus, Ohio.  processing reviews.  the Japanese WordNet. In LREC.  Mark Johnson. 2010. PCFGs, topic models, adaptor  grammars and learning topical collocations and the  2009. Incorporating domain knowledge into topic  modstructure of proper names. In ACL.  eling via Dirichlet forest priors. In ICML.  triggers and latent semantic analysis for cross-lingual  2010. Sentiwordnet 3.0: An enhanced lexical resource  for sentiment analysis and opinion mining. In LREC.  Philipp Koehn. 2005. Europarl: A parallel corpus  Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer  for statistical machine translation. In MT Summit.  Hassan. 2008. Multilingual subjectivity analysis using  machine translation. In EMNLP.  David M. Blei and John D. Lafferty. 2005. Correlated  ing WordNets in a web-compliant format: The case of  topic models. In NIPS.  GermaNet. In Workshop on Wordnets Structures and  David M. Blei and Jon D. McAuliffe. 2007. Supervised  topic models. In NIPS. MIT Press.  Chenghua Lin and Yulan He. 2009. Joint sentiment/topic  model for sentiment analysis. In CIKM.  Latent Dirichlet allocation. JMLR, 3:993 1022.  2008. A joint topic and perspective model for  ideoEdward Loper and Steven Bird. 2002. NLTK: the natu-Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel  Jural language toolkit. In Tools and methodologies for  rafsky, and Christopher Manning. 2005. A conditional  teaching. ACL.  random field word segmenter. In SIGHAN Workshop  on Chinese Language Processing.  Wells, and Jeff Reynar. 2007. Structured models for  University of Oxford.  British  Nafine-to-coarse sentiment analysis. In ACL.  tional  Corpus.  Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and  ChengXiang Zhai. 2007. Topic sentiment mixture:  Tobias Vetter, Manfred Sauer, and Philipp Wallutat.  modeling facets and opinions in weblogs. In WWW.  Ilya Dan Melamed. 1998. Empirical methods for  exploiting parallel texts. Ph.D. thesis, University of  PennsylXiaojun Wan. 2009. Co-training for cross-lingual  sentiment classification. In ACL.  George A. Miller. 1990. Nouns in WordNet: A lexical  inheritance system. International Journal of  Lexicogneous image classification and annotation. In CVPR.  raphy, 3(4):245 264.  Xing Wei and Bruce Croft. 2006. LDA-based document  Smith, and Andrew McCallum. 2009. Polylingual  Casey Whitelaw, Navendu Garg, and Shlomo Argamon.  2005. Using appraisal groups for sentiment analysis.  In CIKM.  2009. Mining multilingual topics from Wikipedia. In  Theresa Ann Wilson. 2008. Fine-grained Subjectivity and  Sentiment Analysis: Recognizing the Intensity, Polarity,  Noam Ordan and Shuly Wintner. 2007. Hebrew  Wordand Attitudes of Private States. Ph.D. thesis, University  Net: a test case of aligning lexical databases across  lanof Pittsburgh.  guages. International Journal of Translation, 19(1):39  topic admixture models for word alignment. In ACL.  Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting  Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. Medlda:  class relationships for sentiment categorization with  maximum margin supervised topic models for  regresrespect to rating scales. In ACL.  sion and classification. In ICML.  Sentiment Analysis. Now Publishers Inc.  Martin Porter and Richard Boulton. 1970. Snowball  Matthew Purver, Konrad K rding, Thomas L. Griffiths,  and Joshua Tenenbaum. 2006. Unsupervised topic  modelling for multi-party spoken discourse. In ACL.  Reinhard Rapp. 1995. Identifying word translations in  Philip Resnik. 1995. Using information content to  evaluate semantic similarity in a taxonomy. In IJCAI, pages  Frank Richter. 2008. Dictionary nice grep.  http://wwwEllen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.  Learning subjective nouns using extraction pattern  bootMichal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers,  and Padhraic Smyth. 2004. The author-topic model for  authors and documents. In UAI.  Beno t Sagot and Darja Fi er. 2008. Building a Free  French WordNet from Multilingual Resources. In  OntoLex.  Ivan Titov and Ryan McDonald. 2008. A joint model of  text and aspect ratings for sentiment summarization. In 