 Joint Extraction of Entities and Relations for Opinion Recognition Yejin Choi and Eric Breck and Claire Cardie  Department of Computer Science  Cornell University  Ithaca, NY 14853  (2005) and Kim and Hovy (2005b)), to determine  the polarity and strength of opinion expressions  We present an approach for the joint  ex(e.g. Wilson et al. (2005)), and to recognize propo-traction of entities and relations in the  consitional opinions and their sources (e.g. Bethard  text of opinion recognition and analysis.  et al. (2004)) with reasonable accuracy. To date,  We identify two types of opinion-related  however, there has been no effort to  simultaneentities expressions of opinions and  ously identify arbitrary opinion expressions, their sources of opinions along with the link-sources, and the relations between them. Without  ing relation that exists between them.  Inprogress on the joint extraction of opinion enti-spired by Roth and Yih (2004), we employ  ties and their relations, the capabilities of opinion-an integer linear programming approach  based applications will remain limited.  to solve the joint opinion recognition task,  and show that global, constraint-based  inFortunately, research in machine learning has  ference can significantly boost the  perforproduced methods for global inference and joint  mance of both relation extraction and the  classification that can help to address this  defiextraction of opinion-related entities.  Perciency (e.g. Bunescu and Mooney (2004), Roth  formance further improves when a  semanand Yih (2004)). Moreover, it has been shown that  tic role labeling system is incorporated.  The resulting system achieves F-measures  lations via global inference not only solves the  of 79 and 69 for entity and relation  extracjoint extraction task, but often boosts performance tion, respectively, improving substantially  on the individual tasks when compared to  clasover prior results in the area.  sifiers that handle the tasks independently for  Introduction  (2004)), information extraction (e.g. Roth and Yih Information extraction tasks such as recognizing  entities and relations have long been considered  critical to many domain-specific NLP tasks (e.g.  In this paper, we present a global inference  apResearchers have further  proach (Roth and Yih, 2004) to the extraction  shown that opinion-oriented information extrac-of opinion-related entities and relations. In  partion can provide analogous benefits to a variety of ticular, we aim to identify two types of entities  practical applications including product reputation (i.e. spans of text): entities that express opin-tracking (Morinaga et al., 2002), opinion-oriented ions and entities that denote sources of opinions.  question answering (Stoyanov et al., 2005), and  More specifically, we use the term opinion expres-opinion-oriented summarization (e.g.  sion to denote all direct expressions of subjectiv-al. (2004), Liu et al. (2005)). Moreover, much  ity including opinions, emotions, beliefs,  sentiprogress has been made in the area of opinion  exment, etc., as well as all speech expressions that traction: it is possible to identify sources of opin-introduce subjective propositions; and use the term ions (i.e. the opinion holders) (e.g. Choi et al.  source to denote the person or entity (e.g. a re-431  Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 431 439, Sydney, July 2006. c  2006 Association for Computational Linguistics  port) that holds the opinion.1  In addition, we  of opinion expressions and sources, and a binary  aim to identify the relations between opinion  exclassifier to identify the link relation. The global pression entities and source entities. That is, for inference procedure is implemented via integer  a given opinion expression Oi and source entity  linear programming (ILP) to produce an optimal  and coherent extraction of entities and relations.  j , we determine whether the relation Li,j  Because many (60%) opinion-source relations  j expresses Oi) obtains, i.e. whether Sj is the  source of opinion expression O  appear as predicate-argument relations, where the  i. We refer to this  particular relation as the link relation in the rest predicate is a verb, we also hypothesize that se-of the paper. Consider, for example, the following mantic role labeling (SRL) will be very useful for sentences:  our task. We present two baseline methods for  the joint opinion-source recognition task that use S1. [ Bush](1) intends(1) to curb the increase in a state-of-the-art SRL system (Punyakanok et al.,  harmful gas emissions and is counting on(1)  2005), and describe two additional methods for  inthe good will(2) of [ US industrialists](2) .  corporating SRL into our ILP-based system.  S2. By questioning(3) [ the Imam](4)'s edict(4) [ the Our experiments show that the global inference  Islamic Republic of Iran](3) made [ the people approach not only improves relation extraction  of the world](5) understand(5)...  over the base classifier, but does the same for individual entity extractions. For source extraction The underlined phrases above are opinion expres-in particular, our system achieves an F-measure of sions and phrases marked with square brackets are  78.1, significantly outperforming previous results source entities. The numeric superscripts on en-in this area (Choi et al., 2005), which obtained an tities indicate link relations: a source entity and F-measure of 69.4 on the same corpus. In addition, an opinion expression with the same number sat-we achieve an F-measure of 68.9 for link relation  isfy the link relation. For instance, the source en-identification and 82.0 for opinion expression  extity Bush and the opinion expression intends  traction; for the latter task, our system achieves satisfy the link relation, and so do Bush and human-level performance.2  counting on. Notice that a sentence may contain more than one link relation, and link relations 2  High-Level Approach and Related  are not one-to-one mappings between sources and  Work  opinions. Also, the pair of entities in a link relation may not be the closest entities to each other, as Our system operates in three phases.  is the case in the second sentence, between ques-Opinion and Source Entity Extraction  tioning and the Islamic Republic of Iran.  begin by developing two separate token-level  We expect the extraction of opinion relations to  sequence-tagging classifiers for opinion  expresbe critical for many opinion-oriented NLP  applision extraction and source extraction, using linearcations. For instance, consider the following ques-chain Conditional Random Fields (CRFs)  (Laftion that might be given to a question-answering  ferty et al., 2001). The sequence-tagging  classifiers are trained using only local syntactic and  lex What is the Imam's opinion toward the Islamic ical information to extract each type of entity with-Republic of Iran?  out knowledge of any nearby or neighboring  entiWithout in-depth opinion analysis, the  questionties or relations. We collect n-best sequences from answering system might mistake example S2 as  each sequence tagger in order to boost the recall of relevant to the query, even though S2 exhibits the the final system.  opinion of the Islamic Republic of Iran toward  Link Relation Classification  We also develop  Imam, not the other way around.  a relation classifier that is trained and tested on Inspired by Roth and Yih (2004), we model  all pairs of opinion and source entities extracted our task as global, constraint-based inference over from the aforementioned n-best opinion expres-separately trained entity and relation classifiers.  sion and source sequences. The relation classifier In particular, we develop three base classifiers:  two sequence-tagging classifiers for the extraction 2Wiebe et al. (2005) reports human annotation agreement 1See Wiebe et al. (2005) for additional details.  for opinion expression as 82.0 by F1 measure.  et al., 2001), which are equivalent to maximum  enper sentence, and (4) link relations that span more tropy models. It is trained using only local syntac-than one sentence.  In addition, the link  relatic information potentially useful for connecting a tion model explicitly exploits mutual dependen-pair of entities, but has no knowledge of nearby or cies among entities and relations, while Bethard  neighboring extracted entities and link relations.  et al. (2004) does not directly capture the potential Integer Linear Programming  Finally, we  forKim and Hovy (2005b) and Choi et al. (2005)  each sentence using the results from the previous  focus only on the extraction of sources of  two phases. In particular, we specify a number  opinions, without extracting opinion expressions.  of soft and hard constraints among relations and  Specifically, Kim and Hovy (2005b) assume a  prientities that take into account the confidence valori existence of the opinion expressions and  exues provided by the supporting entity and relation tract a single source for each, while Choi et al.  classifiers, and that encode a number of heuristics (2005) do not explicitly extract opinion expres-to ensure coherent output. Given these constraints, sions nor link an opinion expression to a source  global inference via ILP finds the optimal,  cohereven though their model implicitly learns  approxient set of opinion-source pairs by exploiting  mumations of opinion expressions in order to identify tual dependencies among the entities and relations.  opinion sources. Other previous research focuses  While good performance in entity or relation  only on the extraction of opinion expressions (e.g.  extraction can contribute to better performance of Kim and Hovy (2005a), Munson et al. (2005) and  the final system, this is not always the case. Pun-Wilson et al. (2005)), omitting source  identificayakanok et al. (2004) notes that, in general, it is tion altogether.  better to have high recall from the classifiers in-There have also been previous efforts to  sicluded in the ILP formulation. For this reason, it is multaneously extract entities and relations by ex-not our goal to directly optimize the performance  ploiting their mutual dependencies.  Roth and  of our opinion and source entity extraction models Yih (2002) formulated global inference using a  or our relation classifier.  Bayesian network, where they captured the  influThe rest of the paper is organized as follows.  ence between a relation and a pair of entities via Related work is outlined below.  the conditional probability of a relation, given a scribes the components of the first phase of our  pair of entities. This approach however, could not system, the opinion and source extraction classi-exploit dependencies between relations. Roth and  fiers. Section 4 describes the construction of the Yih (2004) later formulated global inference using link relation classifier for phase two. Section 5  integer linear programming, which is the approach  describes the ILP formulation to perform global  that we apply here. In contrast to our work, Roth  inference over the results from the previous two  and Yih (2004) operated in the domain of factual  phases. Experimental results that compare our ILP  information extraction rather than opinion  extracapproach to a number of baselines are presented in tion, and assumed that the exact boundaries of en-Section 6. Section 7 describes how SRL can be  intities from the gold standard are known a priori,  corporated into our global inference system to fur-which may not be available in practice.  ther improve the performance. Final experimental  results and discussion comprise Section 8.  Extraction of Opinion and Source  Related Work  The definition of our  sourceexpresses-opinion task is similar to that of Bethard We develop two separate sequence tagging classi-et al. (2004); however, our definition of  opinfiers for opinion extraction and source extraction, ion and source entities are much more extensive,  using linear-chain Conditional Random Fields  going beyond single sentences and propositional  (CRFs) (Lafferty et al., 2001). The sequence  tagging is encoded as the typical BIO' scheme.4  our approach with respect to (1) a wide variety  Each training or test instance represents a  senof opinion expressions, (2) explicit and implicit3  tence, encoded as a linear chain of tokens and their sources, (3) multiple opinion-source link relations 4 B' is for the token that begins an entity, I' is for to-3 Implicit sources are those that are not explicitly men-kens that are inside an entity, and O' is for tokens outside an tioned. See Section 8 for more details.  entity.  associated features. Our feature set is based on  tity overlaps with a correct opinion or source  enthat of Choi et al. (2005) for source extraction5, tity per the gold standard. This training instance but we include additional lexical and WordNet-filtering helps to avoid confusion between  exambased features. For simplicity, we use the same  ples like the following (where entities marked in  features for opinion entity extraction and source  bold are the gold standard entities, and entities  extraction, and let the CRFs learn appropriate fea-in square brackets represent the n-best output  seture weights for each task.  quences from the entity extraction classifiers):  (1) [The president] s1 walked away from [the 3.1  Entity extraction features  For each token xi, we include the following  feapointment] o3 with the deal.  (2) [The monster] s2 walked away, [revealing] o4  word: words in a [-4, +4] window centered on xi.  a little box hidden underneath.  part-of-speech: POS tags in a [-2, +2] window.6  For these sentences, we construct training  ingrammatical role: grammatical role (subject, ob-stances for L1,1, L1,2, and L1,3, but not L2,4,  ject, prepositional phrase types) of x  which in fact has very similar sentential structure i derived from  as L1,2, and hence could confuse the learning  aldictionary: whether x  i is in the opinion  expression dictionary culled from the training data and  Relation extraction features  augmented by approximately 500 opinion words  The training and test instances for each (potential) from the MPQA Final Report8. Also computed  for tokens in a [-1, +1] window and for x  i,j (with opinion candidate entity Oi and  source candidate entity S  chunk in the dependency parse.  j ) include the following  features.  opinion entity word: the words contained in O  WordNet: the WordNet hypernym of x  phrase type: the syntactic category of the con-4  Relation Classification  stituent in which the entity is embedded, e.g. NP  or VP. We encode separate features for Oi and Sj.  We also develop a maximum entropy binary  clasgrammatical role: the grammatical role of the sifier for opinion-source link relation classifica-constituent in which the entity is embedded.  tion. Given an opinion-source pair, Oi-Sj, the  reGrammatical roles are derived from dependency  lation classifier decides whether the pair exhibits parse trees, as done for the entity extraction classi-a valid link relation, Li,j. The relation classifier fiers. We encode separate features for Oi and Sj.  focuses only on the syntactic structure and lexical position: a boolean value indicating whether Sj properties between the two entities of a given pair, precedes Oi.  without knowing whether the proposed entities are  distance: the distance between O  correct. Opinion and source entities are taken from i and Sj in numbers of tokens. We use four coarse categories:  adthe n-best sequences of the entity extraction  modjacent, very near, near, far.  els; therefore, some are invariably incorrect.  From each sentence, we create training and test  dependency path: the path through the depen-instances for all possible opinion-source pairings dency tree from the head of Sj to the head of Oi.  that do not overlap: we create an instance for L  only if the span of O  voice: whether the voice of O  i is passive or active.  For training, we also filter out instances for  syntactic frame: key intra-sentential relations be-which neither the proposed opinion nor source  entween Oi and Sj. The syntactic frames that we use  5We omit only the extraction pattern features.  1:role] [distance] [E2:role], where distance  7Provided by Rebecca Hwa, based on the Collins parser:  {adjacent, very near, near, far}, and Ei:role  ftp://ftp.cis.upenn.edu/pub/mcollins/PARSER.tar.gz is the grammatical role of Ei. Either E1 is an  opinion entity and E  2 is a source, or vice versa.  riloff/  publications.html#sundance)  1:phrase] [distance] [E2:phrase],  where  Ei:phrase is the phrasal type of entity Ei.  [E1:phrase] [E2:headword], where E2 must be  Objective function f  the opinion entity, and E  1 must be the source  entity (i.e. no lexicalized frames for sources). E  and E  2 can be contiguous.  1:role] [E2:headword], where E2 must be the  opinion entity, and E1 must be the source entity.  [E1:phrase] NP [E2:phrase]  indicates  the  presence of specific syntactic patterns, e.g.  VP NP VP' depending on the possible phrase  types of opinion and source entities. The three  phrases do not need to be contiguous.  [E1:phrase] VP [E2:phrase] (See above.)  [E1:phrase] [wh-word] [E2:phrase]  Src [distance] [x] [distance] Op, where x  {by, of, from, for, between, among, and, have,  Table 1: Binary ILP formulation  be, will, not, ], , . . . }.  When a syntactic frame is matched to a  sentence, the bracketed items should be instantiated  Oi = 1 means to discard the opinion entity. To  with particular values corresponding to the  senensure coherent assignments, we add equality  contence. Pattern elements without square brackets  Oi = 1. The weights wo and  are constants. For instance, the syntactic frame  wo for O  Oi respectively, are computed as  a negative conditional probability of the span of  1:phrase] NP [E2:phrase]' may be instantiated  as VP NP VP'. Some frames are lexicalized with  an entity to be extracted (or suppressed) given the respect to the head of an opinion entity to reflect labelings of the adjacent variables of the CRFs:  the fact that different verbs expect source  entiwhere xk = B'  Approach  where xm = O' for m [k, l]  As noted in the introduction, we model our task  where x  as global, constraint-based inference over the sep-i is the value assigned to the random  variable of the CRF corresponding to an entity O  arately trained entity and relation classifiers, and i.  Likewise, for each source entity, we add two  variimplement the inference procedure as binary  integer linear programming (ILP) ((Roth and Yih,  Sj and a constraint Sj +  Sj = 1. The  weights for source variables are computed in the  same way as opinion entities.  of an objective function which is a dot product  between a vector of variables and a vector of  Relation variables and weights  For each link  weights, and a set of equality and inequality  conrelation, we add two variables Li,j and  straints among variables. Given an objective  funcLi,j = 1. By the definition of  tion and a set of constraints, LP finds the  optia link, if Li,j = 1, then it is implied that Oi = 1  mal assignment of values to variables, i.e. one that and Sj = 1. That is, if a link is extracted, then the minimizes the objective function. In binary ILP,  pair of entities for the link must be also extracted.  the assignments to variables must be either 0 or 1.  Constraints to ensure this coherency are explained The variables and constraints defined for the opin-in the following subsection. The weights for link  ion recognition task are summarized in Table 1 and variables are based on probabilities from the bi-explained below.  nary link classifier.  Entity variables and weights  For each opinion  Constraints for link coherency  In our corpus, a  entity, we add two variables, Oi and  Oi, where  source entity can be linked to more than one  opinOi = 1 means to extract the opinion entity, and  ion entity, but an opinion entity is linked to only 435  one source. Nonetheless, the majority of  opinionsource pairs involve one-to-one mappings, which  we encode as hard and soft constraints as follows: We evaluate our system using the NRRC Multi-For each opinion entity, we add an equality  conPerspective Question Answering (MPQA) corpus  that contains 535 newswire articles that are  manj Li,j to enforce that only one  link can emanate from an opinion entity. For each  ually annotated for opinion-related information.  source entity, we add an equality constraint and an In particular, our gold standard opinion entities  inequality constraint that together allow a source correspond to direct subjective expression anno-P  to link to at most two opinions: S  tations and subjective speech event annotations j + Aj =  and A  (i.e. speech events that introduce opinions) in the j Sj 0, where Aj is an auxiliary variable, such that its weight is some positive constant MPQA corpus (Wiebe et al., 2005). Gold stan-value that suppresses A  dard source entities and link relations can be  exj from being assigned to 1.  And A  tracted from the agent attribute associated with j can be assigned to 1 only if Sj is already  assigned to 1. It is possible to add more auxiliary each opinion entity. We use 135 documents as a  variables to allow more than two opinions to link  development set and report 10-fold cross  validato a source, but for our experiments two seemed to tion results on the remaining 400 documents in all be a reasonable limit.  experiments below.  We evaluate entity and link extraction using  Constraints for entity coherency  both an overlap and exact matching scheme.12 Be-n-best sequences where n > 1, proposed entities cause the exact start and endpoints of the man-can overlap. Because this should not be the case  ual annotations are somewhat arbitrary, the  overin the final result, we add an equality constraint lap scheme is more reasonable for our task (Wiebe  Xi + Xj = 1, X {S, O} for all pairs of entities  et al., 2005). We report results according to both with overlapping spans.  matching schemes, but focus our discussion on  reAdjustments to weights  To balance the  precisults obtained using overlap matching.13  sion and recall, and to take into account the  perWe use the Mallet14 implementation of CRFs.  formance of different base classifiers, we apply ad-For brevity, we will refer to the opinion extraction justments to weights as follows.  classifier as CRF-OP, the source extraction classifier as CRF-SRC, and the link relation classifier as 1) We define six coefficients cx and  cx, where  CRF-LINK. For ILP, we use Matlab, which  prox {O, S, L} to modify a group of weights  duced the optimal assignment in a matter of few  as follows.  seconds for each sentence. The weight adjustment  constants defined for ILP are based on the  develIn general, increasing cx will promote recall,  while increasing  cx will promote precision.  The link-nearest baselines  For baselines, we  Also, setting co > cs will put higher  confifirst consider a link-nearest heuristic: for each dence on the opinion extraction classifier than  opinion entity extracted by CRF-OP, the  linkthe source extraction classifier.  nearest heuristic creates a link relation with the 2) We also define one constant cA to set the  closest source entity extracted by CRF-SRC.  Reweights for auxiliary variable Ai.  That is,  call that CRF-SRC and CRF-OP extract entities  from n-best sequences. We test the link-nearest  3) Finally, we adjust the confidence of the link  heuristic with n = {1, 2, 10} where larger n will  variable based on n-th-best sequences of the  enboost recall at the cost of precision. Results for the tity extraction classifiers as follows.  Given two links L1,1 = (O1, S1) and L2,2 = (O2, S2), L  exact matching requires the spans of O1 and O2, and the where d def  = 4/(3 + min(m, n)), when O  spans of S  1 and S2, to match exactly, while overlap matching requires the spans to overlap.  from an m-th sequence and Sj is from a n-th  13Wiebe et al. (2005) also reports the human annotation sequence.11  11This will smoothly degrade the confidence of a link 15co = 2.5, co = 1.0, cs = 1.5, cs = 1.0, cL = 2.5, cL =  based on the entities from higher n-th sequences. Values of d 2.5, cA = 0.2. Values are picked so as to boost recall while decrease as 4/4, 4/5, 4/6, 4/7....  reasonably suppressing incorrect links.  Exact Match  Exact Match  ILP+SRL-f -1  ILP+SRL-f -10  SRL+CRF-OP  ILP+SRL-f c-10  Table 3: Relation extraction with ILP and SRL  ILP-n : ILP applied to n-best sequences  Table 2: Relation extraction performance  ILP+SRL-f -n : ILP w/ SRL features, n-best  NEAREST-n : link-nearest heuristic w/ n-best  ILP+SRL-f c-n : ILP w/ SRL features,  and SRL constraints, n-best  SRL+CRF-OP : all V-A0 filtered by CRF-OP  ILP-n : ILP applied to n-best sequences  of NEAREST-1, because the 10-best sequences  include many incorrect entities whereas the  correlink-nearest heuristic on the full source-expresses-sponding ILP formulation can discard the bad  enopinion relation extraction task are shown in the  tities by considering dependencies among entities  first three rows of table 2. NEAREST-1 performs  and relations.17  the best in overlap-match F-measure, reaching  59.9. NEAREST-10 has higher recall (66.3%), but  Additional SRL Incorporation  the precision is really low (20.9%). Performance  We next explore two approaches for more directly  of the opinion and source entity classifiers will be incorporating SRL into our system.  discussed in Section 8.  Next, we consider two  baseExtra SRL Features for the Link classifier  lines that use a state-of-the-art SRL system  (Punincorporate SRL into the link classifier by adding yakanok et al., 2005).  In many link relations,  extra features based on SRL. We add boolean  feathe opinion expression entity is a verb phrase and tures to check whether the span of an SRL argu-the source entity is in an agent argument  posiment and an entity matches exactly. In addition,  tion. Hence our second baseline, S  we include syntactic frame features as follows: RL, extracts  all verb(V)-agent(A0) frames from the output of  [E1:srl-arg] [E2:srl-arg], where Ei:srl-arg indithe SRL system and provides an upper bound on  cates the SRL argument type of entity Ei.  recall (59.7%) for systems that use SRL in  isola [E1.srl-arg] [E1:headword] [E2:srl-arg], where  tion for our task. A more sophisticated baseline,  E1 must be an opinion entity, and E2 must be a  SRL+CRF-OP, extracts only those V-A0 frames  source entity.  whose verb overlaps with entities extracted by the Extra SRL Constraints for the ILP phase  opinion expression extractor, CRF-OP. As shown  also incorporate SRL into the ILP phase of our  in table 2, filtering out V-A0 frames that are  insystem by adding extra constraints based on SRL.  compatible with the opinion extractor boosts  preIn particular, we assign very high weights for links cision to 83.2%, but the F-measure (58.9) is lower that match V-A0 frames generated by SRL, in or-than that of NEAREST-1.  der to force the extraction of V-A0 frames.  The ILP-n system in table 2  denotes the results of the ILP approach applied to the A potential issue with overlap precision and recall is that the measures may drastically overestimate the system's pern-best sequences. ILP-10 reaches an F-measure  formance as follows: a system predicting a single link rela-of 68.0, a significant improvement over the  hightion whose source and opinion expression both overlap with est performing baseline16, and also a substantial  every token of a document would achieve 100% overlap precision and recall. We can ensure this does not happen by mea-improvement over ILP-1. Note that the  perforsuring the average number of (source, opinion) pairs to which mance of NEAREST-10 was much worse than that  each correct or predicted pair is aligned (excluding pairs not aligned at all). In our data, this does not exceed 1.08, (except 16Statistically significant by paired-t test, where p < for baselines), so we can conclude these evaluation measures 0.001.  are behaving reasonably.  Opinion  Source  Before ILP  CRF-OP/SRC/LINK with 1 best  After ILP  ILP-SRL-f -10  ILP-SRL-f -10 CRF-OP/SRC with 1 best  Table 4: Entity extraction performance (by overlap-matching) 8  CRF-OP and CRF-SRC, giving an upper bound for  recall for our approach. The third row presents  Results using SRL are shown in Table 3 (on the  results after the ILP phase is applied for the  10previous page). In the table, ILP+SRL-f denotes  best sequences, and we see that, in addition to the the ILP approach using the link classifier with  improved link extraction described in Section 7,  the extra SRL f 'eatures, and ILP+SRL-f c  dethe performance on source extraction is  substannotes the ILP approach using both the extra SRL  tially improved, from F-measure of 73.9 to 78.1.  f 'eatures and the SRL c'onstraints. For  comparPerformance on opinion expression extraction  deison, the ILP-1 and ILP-10 results from Table 2  creases from F-measure of 81.9 to 78.8. This  deare shown in rows 1 and 2.  crease is largely due to implicit links, which we The F-measure score of ILP+SRL-f -10 is 68.9,  will explain below. The fourth row takes the union about a 1 point increase from that of ILP-10,  of the entities from ILP-SRL-f -10 and the entities which shows that extra SRL features for the link  from the best sequences from CRF-OP and  CRFclassifier further improve the performance over  SRC. This process brings the F-measure of  CRFour previous best results.18 ILP+SRL-f c-10 also  OP up to 82.0, with a different precision-recall  performs better than ILP-10 in F-measure,  albreak down from those of 1-best sequences  withthough it is slightly worse than ILP+SRL-f -10.  out ILP phase. In particular, the recall on opinion This indicates that the link classifier with extra expressions now reaches 82.3%, while maintain-SRL features already makes good use of the V-A0  ing a high precision of 81.7%.  frames from the SRL system, so that forcing the  extraction of such frames via extra ILP constraints Overlap Match  Exact Match  only hurts performance by not allowing the  extraction of non-V-A0 pairs in the neighborhood that  could have been better choices.  Contribution of the ILP phase  In order to  Table 5: Relation extraction with ILP weight  adhighlight the contribution of the ILP phase for our justment. (All cases using I  LP+SRL-f -10)  in Table 4. The first row shows the performance  of the individual CRF-OP, CRF-SRC, and  CRFEffects of ILP weight adjustment  Finally, we  LINK classifiers before the ILP phase. Without the show the effect of weight adjustment in the ILP  ILP phase, the 1-best sequence generates the best  formulation in Table 5. The DEV.CONF row shows  scores. However, we also present the performance  relation extraction performance using a weight  with merged 10-best entity sequences19 in order  configuration based from the development data.  to demonstrate that using 10-best sequences  withIn order to see the effect of weight adjustment,  out ILP will only hurt performance. The precision  we ran an experiment, NO.CONF, using fixed  deof the merged 10-best sequences system is very  fault weights.20 Not surprisingly, our weight  adlow, however the recall level is above 95% for both justment tuned from the development set is not the optimal choice for cross-validation set. Neverthe-18Statistically significant by paired-t test, where p < 0.001.  less, the weight adjustment helps to balance the  19If an entity Ei extracted by the ith-best sequence over-precision and recall, i.e. it improves recall at the laps with an entity Ej extracted by the jth-best sequence, where i < j, then we discard E  To be precise, cx = 1.0,  lap, then we extract both entities.  but cA = 0.2 is the same as before.  cost of precision. The weight adjustment is more  effective when the gap between precision and  reLow-Level Annotations and Summary  Representations of Opinions for Multi-Perspective Question  call is large, as was the case with the development Answering. New Directions in Question Answering.  Y. Choi, C. Cardie, E. Riloff and S. Patwardhan 2005.  Implicit links  A good portion of errors stem  Identifying Sources of Opinions with Conditional  from the implicit link relation, which our system Random Fields and Extraction Patterns. In HLT-did not model directly. An implicit link relation  S. Kim and E. Hovy 2005. Automatic Detection of  holds for an opinion entity without an associated  Opinion Bearing Words and Sentences. In IJCNLP.  source entity. In this case, the opinion entity is S. Kim and E. Hovy  Identifying Opinion  linked to an implicit source. Consider the follow-Holders for Question Answering in Opinion Texts.  ing example.  In AAAI Workshop on Question Answering in  Re Anti-Soviet hysteria was firmly oppressed.  stricted Domains.  J. Lafferty, A. K. McCallum and F. Pereira 2001 Con-Notice that opinion expressions such as  Antiditional Random Fields: Probabilistic Models for  Soviet hysteria and firmly oppressed do not Segmenting and Labeling Sequence Data. In ICML.  have associated source entities, because sources of B. Liu, M. Hu and J. Cheng 2005 Opinion Observer:  these opinion expressions are not explicitly  menAnalyzing and Comparing Opinions on the Web. In  tioned in the text.  Because our system forces  each opinion to be linked with an explicit source  R. J. Mooney and R. Bunescu 2005 Mining  Knowledge from Text Using Information Extraction. In  entity, opinion expressions that do not have  explicit source entities will be dropped during the  global inference phase of our system.  Fukushima 2002. Mining product reputations on  links amount to 7% of the link relations in our  the Web. In KDD.  corpus, so the upper bound for recall for our ILP  M. A. Munson, C. Cardie and R. Caruana. 2005. Opti-system is 93%. In the future we will extend our  mizing to arbitrary NLP metrics using ensemble  selection. In HLT-EMNLP.  system to handle implicit links as well. Note that J. Prager, E. Brown, A. Coden and D. Radev 2000.  we report results against a gold standard that  inQuestion-answering by predictive annotation. In SI-cludes implicit links. Excluding them from the  gold standard, the performance of our final  system ILP+SRL-f -10 is 72.6% in recall, 72.4% in  ized Inference with Multiple Semantic Role  Labelprecision, and 72.5 in F-measure.  ing Systems (Shared Task Paper). In CoNLL.  Conclusion  Semantic Role Labeling via Integer Linear  Programming Inference. In COLING.  This paper presented a global inference approach  D. Roth and W. Yih 2004. A Linear Programming  Forto jointly extract entities and relations in the con-mulation for Global Inference in Natural Language  text of opinion oriented information extraction.  Tasks. In CoNLL.  The final system achieves performance levels that  D. Roth and W. Yih 2002. Probabilistic Reasoning for are potentially good enough for many practical  Entity and Relation Recognition. In COLING.  NLP applications.  V. Stoyanov, C. Cardie and J. Wiebe 2005.  MultiPerspective Question Answering Using the OpQA  Acknowledgments We thank the reviewers for their Corpus. In HLT-EMNLP.  many helpful comments and Vasin Punyakanok for running C. Sutton, K. Rohanimanesh and A. K. McCallum  our data through his SRL system.  This work was  sup2004. Dynamic Conditional Random Fields:  Factorized Probabilistic Models for Labeling and  Segported by the Advanced Research and Development Activity menting Sequence Data. In ICML.  (ARDA), by NSF Grants IIS-0535099 and IIS-0208028, and M. White, T. Korelsky, C. Cardie, V. Ng, D. Pierce and by gifts from Google and the Xerox Foundation.  K. Wagstaff 2001. Multi-document Summarization  via Information Extraction In HLT.  J. Wiebe and T. Wilson and C. Cardie 2005. Annotat-S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou and ing Expressions of Opinions and Emotions in Lan-D. Jurafsky 2004. Automatic Extraction of  Opinguage. In Language Resources and Evaluation, vol-ion Propositions and their Holders. In AAAI Spring ume 39, issue 2-3.  Symposium on Exploring Attitude and Affect in Text.  T. Wilson, J. Wiebe and P. Hoffmann 2005. Recogniz-R. Bunescu and R. J. Mooney 2004. Collective  Ining Contextual Polarity in Phrase-Level Sentiment  formation Extraction with Relational Markov 