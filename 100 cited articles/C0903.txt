 Experimental Support for a Categorical Compositional Distributional Model of Meaning  University of Oxford  University of Oxford  Department of Computer Science  Department of Computer Science  Wolfson Building, Parks Road  Wolfson Building, Parks Road  Oxford OX1 3QD, UK  Oxford OX1 3QD, UK  tences. Discovering the process of meaning  assignment in natural language is among the most  Modelling compositional meaning for  senchallenging and foundational questions of  linguistences using empirical distributional methods  has been a challenge for computational  lintics and computer science. The findings thereof will guists. We implement the abstract categorical  increase our understanding of cognition and intelli-model of Coecke et al. (2010) using data from  gence and shall assist in applications to automating the BNC and evaluate it. The implementation  language-related tasks such as document search.  is based on unsupervised learning of matrices  Compositional type-logical approaches  (Monfor relational words and applying them to the  tague, 1974; Lambek, 2008) and distributional  modvectors of their arguments. The evaluation is  based on the word disambiguation task  develels of lexical semantics (Schutze, 1998; Firth, 1957) oped by Mitchell and Lapata (2008) for intran-have provided two partial orthogonal solutions to the sitive sentences, and on a similar new experi-question. Compositional formal semantic models  stem from classical ideas from mathematical logic,  model matches the results of its competitors  mainly Frege's principle that the meaning of a  senin the first experiment, and betters them in the  tence is a function of the meaning of its parts (Frege, second. The general improvement in results  1892). Distributional models are more recent and  with increase in syntactic complexity  showcases the compositional power of our model.  can be related to Wittgenstein's later philosophy of  meaning is use', whereby meanings of words can be  determined from their context (Wittgenstein, 1953).  1 Introduction  The logical models relate to well known and robust  As competent language speakers, we humans can  allogical formalisms, hence offering a scalable theory most trivially make sense of sentences we've never  of meaning which can be used to reason  inferenseen or heard before. We are naturally good at  untially. The distributional models have found their  derstanding ambiguous words given a context, and  way into real world applications such as thesaurus  forming the meaning of a sentence from the  meanextraction (Grefenstette, 1994; Curran, 2004) or au-ing of its parts. But while human beings seem  tomated essay marking (Landauer, 1997), and have  comfortable doing this, machines fail to deliver.  connections to semantically motivated information  Search engines such as Google either fall back on  retrieval (Manning et al., 2008). This two-sortedness bag of words models ignoring syntax and lexical  of defining properties of meaning: logical form'  versus contextual use', has left the quest for what is semantics to retrieve pages with terms related to  the foundational structure of meaning?' even more  those in the query (Manning et al., 2008).  of a challenge.  However, such models fail to shine when it comes  Recently, Coecke et al. (2010) used high level  to processing the semantics of phrases and  sencross-disciplinary techniques from logic, category  Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394 1404, Edinburgh, Scotland, UK, July 27 31, 2011. c  2011 Association for Computational Linguistics  theory, and physics to bring the above two ap-Syntactic Analysis  proaches together. They developed a unified  mathematical framework whereby a sentence vector is by  definition a function of the Kronecker product of its  word vectors. A concrete instantiation of this  theory was exemplified on a toy hand crafted corpus  Figure 1: A simple model of formal semantics.  by Grefenstette et al. (2011). In this paper we implement it by training the model over the entire BNC.  The highlight of our implementation is that words  with relational types, such as verbs, adjectives, and  adverbs are matrices that act on their arguments. We provide a general algorithm for building (or indeed learning) these matrices from the corpus.  The implementation is evaluated against the task  Figure 2: A parse tree showing a semantic derivation.  provided by Mitchell and Lapata (2008) for  disambiguating intransitive verbs, as well as a similar new This methodology is used to translate sentences  experiment for transitive verbs. Our model improves of natural language into logical formulae, then use on the best method evaluated in Mitchell and Lapata computer-aided automation tools to reason about  (2008) and offers promising results for the transitive them (Alshawi, 1992). One major drawback is that  case, demonstrating its scalability in comparison to the result of such analysis can only deal with truth that of other models. But we still feel there is need or falsity as the meaning of a sentence, and says  for a different class of experiments to showcase mer-nothing about the closeness in meaning or topic of  its of compositionality in a statistically significant expressions beyond their truth-conditions and what  manner. Our work shows that the categorical  commodels satisfy them, hence do not perform well on  positional distributional model of meaning permits  language tasks such as search. Furthermore, an  una practical implementation and that this opens the  derlying domain of objects and a valuation function way to the production of large scale compositional  must be provided, as with any logic, leaving open  the question of how we might learn the meaning of  language using such a model, rather than just use it.  2 Two Orthogonal Semantic Models  Distributional Models Distributional models of  Formal Semantics To compute the meaning of a  semantics, on the other hand, dismiss the interaction sentence consisting of n words, meanings of these  between syntactically linked words and are solely  words must interact with one another. In formal  seconcerned with lexical semantics. Word meaning  mantics, this further interaction is represented as a is obtained empirically by examining the contexts1  function derived from the grammatical structure of  in which a word appears, and equating the meaning  the sentence, but meanings of words are amorphous  of a word with the distribution of contexts it shares.  objects of the domain: no distinction is made  beThe intuition is that context of use is what we  aptween words that have the same type. Such models  peal to in learning the meaning of a word, and that consist of a pairing of syntactic interpretation rules words that frequently have the same sort of context (in the form of a grammar) with semantic interpreta-in common are likely to be semantically related.  tion rules, as exemplified by the simple model  preFor instance, beer and sherry are both drinks,  alsented in Figure 1.  coholic, and often cause a hangover. We expect  The parse of a sentence such as cats like milk  these facts to be reflected in a sufficiently large cor-typically produces its semantic interpretation by  pus: the words beer' and sherry' occur within the substituting semantic representation for their gram-1E.g. words which appear in the same sentence or n-word matical constituents and applying -reduction where window, or words which hold particular grammatical or depen-needed. Such a derivation is shown in Figure 2.  dency relations to the word being learned.  context of identifying words such as drink',  alco(2010) use the abstract setting of category theory to holic' and hangover' more frequently than they oc-turn the grammatical structure of a sentence into a cur with other content words.  morphism compatible with the higher level logical  Such context distributions can be encoded as  vecstructure of vector spaces.  tors in a high dimensional space with contexts as  One pragmatic consequence of this abstract idea  basis vectors. For any word vector  word, the scalar  is as follows. In distributional models, there is a weight cword associated with each context basis vec-meaning vector for each word, e.g.  tor  ni is a function of the number of times the  milk. The logical recipe tells us to apply the mean-word has appeared in that context. Semantic vectors ing of the verb to the meanings of subject and object.  But how can a vector apply to other vectors? The so-1  ) are also denoted by sums  of such weight/basis vector pairs:  lution proposed above implies that one needs to have different levels of meaning for words with different  types. This is similar to logical models where verbs i  are relations and nouns are atomic sets. So verb vec-Learning a semantic vector is just learning its  bators should be built differently from noun vectors, sis weights from the corpus. This setting offers ge-for instance as matrices.  ometric means to reason about semantic similarity  The general information as to which words should  (e.g. via cosine measure or k-means clustering), as be matrices and which words atomic vectors is in  fact encoded in the type-logical representation of the The principal drawback of such models is their  grammatical structure of the sentence. This is the  non-compositional nature: they ignore grammatical  linear map with word vectors as input and sentence  structure and logical words, and hence cannot  comvectors as output. Hence, at least theoretically, one pute the meanings of phrases and sentences in the  should be able to build sentence vectors and  comsame efficient way that they do for words.  Compare their synonymity in exactly the same way as  mon operations discussed in (Mitchell and Lapata,  one measures word synonymity.  2008) such as vector addition (+) and  componentwise multiplication (  Pregroup Grammars The aforementioned linear  mutative, hence if  maps turn out to be the grammatical reductions  w , then  of a type-logic called a Lambek pregroup  gramwv, leading to unwelcome equalities such as  mar (Lambek, 2008)2. Pregroups and vector spaces  the dog bit the man  = the man bit the dog  share the same high level mathematical structure, referred to as a compact closed category, for a proof Non-commutative operations, such as the Kronecker  and details of this claim see Coecke et al. (2010); for product (cf. 4 for definition) can take word-order a friendly introduction to category theory, see Co-into account (Smolensky, 1990) or even some more  ecke and Paquette (2011). One consequence of this  complex syntactic relations, as described in Clark  parity is that the grammatical reductions of a  preand Pulman (2007). However, the dimensionality of  group grammar can be directly transformed into  linsentence vectors produced in this manner differs for ear maps that act on vectors.  sentences of different length, barring all sentences In a nutshell, pregroup types are either atomic  from being compared in the same vector space, and  or compound. Atomic types can be simple (e.g. n  growing exponentially with sentence length hence  for noun phrases, s for statements) or left/right  quickly becoming computationally intractable.  superscripted referred to as adjoint types (e.g. nr and  nl). An example of a compound type is that of  a verb nrsnl. The superscripted types express that  Whereas semantic compositional mechanisms for  the verb is a relation with two arguments of type n, set-theoretic constructions are well understood,  2The usage of pregroup types is not essential, the types of there are no obvious corresponding methods for vec-any other logic, for instance CCG can be used, but should be tor spaces. To solve this problem, Coecke et al.  translated into the language of pregroups.  which have to occur to the right and to the left of lower dimensions, hence the dimensional explosion  it, and that it outputs an argument of the type s. A problem for Kronecker products is avoided:  transitive sentence has types as shown in Figure 3.  Each type n cancels out with its right adjoint nr  from the right and its left adjoint nl from the left; citjh  mathematically speaking these mean3  wj are basis vectors of V and W . The inner  and  product h  vi i substitutes the weights of  into the first argument place of the verb (similarly Here 1 is the unit of concatenation: 1n = n1 =  for object and second argument place).  s  n. The corresponding grammatical reduction of a  t is a basis  vector of the sentence space  transitive sentence is  S in which meanings of  nnrsnl 1s1 = s. Each such  sentences live, regardless of their grammatical struc-reduction can be depicted as a wire diagram. The  diagram of a transitive sentence is shown in Figure 3.  The degree of synonymity of sentences is  obtained by taking the cosine measure of their vectors.  S is an abstract space: it needs to be instantiated n  to provide concrete meanings and synonymity  measures. For instance, a truth-theoretic model is  obtained by taking the sentence space S to be the  2dimensional space with basis vectors |1i (True) and Figure 3: The pregroup types and reduction diagram for  4 Building Matrices for Relational Words  Syntax-guided Semantic Composition  AccordIn this section we present a general scheme to build ing to Coecke et al. (2010) and based on a general  matrices for relational words. Recall that given  completeness theorem between compact categories,  wire diagrams, and vector spaces, the meaning of  A with basis {  ni}i, the Kronecker  product of two vectors  sentences can be canonically reduced to linear alge-v =  braic formulae. The following is the meaning vector i cbi ni is defined as follows:  of our transitive sentence:  where (  nj) is just the pairing of the basis of A,  Here f is the linear map that encodes the  grammatinj). The Kronecker product vectors belong  cal structure. The categorical morphism  correspondin the tensor product of A with itself: A A, hence ing to it is denoted by the tensor product of 3 compo-if A has dimension r, these will be of dimensionality nents:  V 1S W , where V and W are subject and  r. The point-wise multiplication of these vectors object spaces, S is the sentence space, the 's are the is defined as follows  cups, and 1S is the straight line in the diagram. The X  cups stand for taking inner products, which when  done with the basis vectors imitate substitution. The i  straight line stands for the identity map that does The intuition behind having a matrix for a rela-nothing. By the rules of the category, equation (I) re-tional word is that any relation R on sets X and Y , duces to the following linear algebraic formula with i.e. R X Y can be represented as a matrix,  namely one that has as row-bases x X and as  3The relation is the partial order of the pregroup. It corre-column-bases  sponds to implication  = in a logical reading thereof. If these  xy = 1 where  inequalities are replaced by equalities, i.e. if nln = 1 = nnr, (x, y) R and 0 otherwise. In a distributional set-then the pregroup collapses into a group where nl = nr.  ting, the weights, which are natural or real numbers, 1397  will represent more: the extent according to which trix is represented in vector form as follows:  x and y are related'. This can be determined in  different ways.  Suppose X is the set of animals, and chase' is a  relation on it: chase X X. Take x = dog' This vector lives in the tensor space and y = cat': with our type-logical glasses on, the N N N.  obvious choice would be to take  is computed  xy to be the  number of times dog' has chased cat', i.e. the number m  according to the procedure described in Figure 4.  of times the sentence the dog chases the cat' has  appeared in the corpus. But in the distributional set-1) Consider a sequence of words containing a  reting, this method will be too syntactic and dismissive lational word P' and its arguments w1, w2, , of the actual meaning of cat' and dog'. If instead wm, occurring in the same order as described in  the corpus contains the sentence the hound hunted  P's grammatical type . Refer to these sequences  the wild cat', cxy will be 0, restricting us to only as P'-relations. Suppose there are k of them.  assign meaning to sentences that have directly  ap2) Retrieve the vector  w l of each argument wl.  peared in the corpus. We propose to, instead, use a 3) Suppose w1 has weight c1 on basis vector  level of abstraction by taking words such as verbs to w2 has weight c2 on basis vector  be distributions over the semantic information in the wm has weight cm on basis vector  . Multiply  vectors of their context words, rather than over the these weights  context words themselves.  Start with an r-dimensional vector space N with  4) Repeat the above steps for all the k  P'n i} , in which meaning vectors of atomic  relations, and suma the corresponding weights  words, such as nouns, live. The basis vectors of N  are in principle all the words from the corpus, how-X  ever in practice and following Mitchell and Lapata  (2008) we had to restrict these to a subset of the  most occurring words. These basis vectors are not  We also experimented with multiplication, but the spar-sity of noun vectors resulted in most verb matrices being restricted to nouns: they can as well be verbs, adjec-empty.  tives, and adverbs, so that we can define the  meaning of a noun in all possible contexts as is usual  Figure 4: Procedure for learning weights for matrices of in context-based models and not only in the con-words P' with relational types of m arguments.  text of other nouns. Note that basis words with  relational types are treated as pure lexical items rather Linear algebraically, this procedure corresponds to than as semantic objects represented as matrices. In computing the following  short, we count how many times a noun has occurred  close to words of other syntactic types such as elect'  and scientific', rather than count how many times it k  has occurred close to their corresponding matrices: it is the lexical tokens that form the context, not their Type-logical examples of relational words are  verbs, adjectives, and adverbs. A transitive verb is represented as a 2 dimensional matrix since its type is nrsnl with two adjoint types nr and nl. The cor-Each relational word P with grammatical type  responding vector of this matrix is  and m adjoint types 1, 2, , m is encoded as an  (r . . . r) matrix with m dimensions. Since  our vector space N has a fixed basis, each such  maThe weight c  room scientific  ij corresponding to basis vector  n j, is the extent according to which words that have far  co-occurred with  n i have been the subject of the  verb' and words that have co-occurred with  have been the object of the verb'. This example  computation is demonstrated in Figure 5.  Table 2: Sample semantic matrix for show'.  1) Consider phrases containing verb', its subject  w1 and object w2. Suppose there are k of them.  multiplying weights of map' and location' on  1 and  i.e. 5.6 5.9 then adding these 46.2 + 33.04 and  1 has weight c1 on  c2 on  obtaining the total weight 79.24.  n j. Multiply these weights c1i c2j  4) Repeat the above steps for all k  verb'The same method is applied to build matrices for direlations and sum the corresponding weights  transitive verbs, which will have 3 dimensions, and k(c1  adjectives and adverbs, which will be of 1 dimension each.  Figure 5: Procedure for learning weights for matrices of transitive verbs.  5 Computing Sentence Vectors  Linear algebraically, we are computing  Meaning of sentences are vectors computed by  taking the variables of the categorical prescription of  meaning (the linear map f obtained from the  grammatical reduction of the sentence) to be determined As an example, consider the verb show' and sup-by the matrices of the relational words. For instance pose there are two show'-relations in the corpus:  the meaning of the transitive sentence sub verb obj'  s  table show result  s2  map show location  The vector of show' is  map location  We take V := W := N and S = N N, then  itj citj s t is determined by the matrix of the verb, Consider an N space with four basis vectors far',  i.e. substitute it by  room', scientific', and elect'.  The  TF/IDFweighted values for vectors of the above four nouns sub verb obj becomes:  (built from the BNC) are as shown in Table 1.  table map result location  This can be decomposed to point-wise  multiplication of two vectors as follows:  Table 1: Sample weights for selected noun vectors.  Part of the matrix of show' is presented in Table 2.  As a sample computation, the weight  Note that by doing so we are also reducing the verb space 11 for  from N (N N) N to N N, since for our construction (1, 1), i.e. (far, far) is computed by multiply-we only need tuples of the form  n j which  ing weights of table' and result' on  far, i.e. 6.6 7, are isomorphic to pairs (  The left argument is the Kronecker product of sub-built sentence vectors against a benchmark dataset  ject and object vectors and the right argument is the such as that provided by Mitchell and Lapata (2008).  vector of the verb, so we obtain  In this section, we briefly describe the evaluation of  our model against this dataset. Following this, we  present a new evaluation task extending the  experimental methodology of Mitchell and Lapata (2008)  Since is commutative, this provides us with a to transitive verb-centric sentences, and compare our tributional version of the type-logical meaning of the model to those discussed by Mitchell and Lapata  sentence: point-wise multiplication of the meaning  (2008) within this new experiment.  of the verb to the Kronecker product of its subject and object:  First Dataset Description The first experiment,  described in detail by Mitchell and Lapata (2008),  evaluates how well compositional models  disamThis mathematical operation can be informally  debiguate ambiguous words given the context of a  poscribed as a structured mixing' of the information tentially disambiguating noun. Each entry of the  of the subject and object, followed by it being fil-dataset provides a noun, a target verb and landmark tered' through the information of the verb applied  verb (both intransitive). The noun must be  comto them, in order to produce the information of the posed with both verbs to produce short phrase vec-sentence.  tors the similarity of which is measured by the can-In the transitive case, S = N N, hence  s  didate. Also provided with each entry is a classifi-t =  cation ( High or Low ) indicating whether or not  n j. More generally, the vector space  corresponding to the abstract sentence space S is the  the verbs are indeed semantically close within the  concrete tensor space (N . . . N) for m the context of the noun, as well as an evaluator-set simi-mension of the matrix of the verb'. As we have  larity score between 1 and 7 (along with an evaluator seen above, in practice we do not need to build this identifier), where 1 is low similarity and 7 is high.  tensor space, as the computations thereof reduce to point-wise multiplications and summations.  Evaluation Methodology Candidate models  proSimilar computations yield meanings of sentences  vide a similarity score for each entry. The scores  with adjectives and adverbs. For instance the  meanof high similarity entries and low similarity entries ing of a transitive sentence with a modified subject are averaged to produce a mean High score and  and a modified verb we have  mean Low score for the model. The correlation of  the model's similarity judgements with the human  judgements is also calculated using Spearman's , a  metric which is deemed to be more scrupulous, and  ultimately that by which models should be ranked,  After building vectors for sentences, we can  comby Mitchell and Lapata (2008). The mean for each  pare their meaning and measure their degree of  synmodel is on a [0, 1] scale, except for UpperBound  onymy by taking their cosine measure.  which is on the same [1, 7] scale the annotators used.  The scores are on a [ 1, 1] scale. It is assumed  that inter-annotator agreement provides the theoret-Evaluating such a framework is no easy task. What  to evaluate depends heavily on what sort of applicaThe cosine measure of the verb vectors, ignoring the tion a practical instantiation of the model is geared noun, is taken to be the baseline (no composition).  towards. In (Grefenstette et al., 2011), it is  suggested that the simplified model we presented and  Other Models The other models we compare  expanded here could be evaluated in the same way as ours to are those evaluated by Mitchell and Lap-lexical semantic models, measuring compositionally  ata (2008). We provide a selection of the results  from that paper for the worst (Add) and best5 (Mul-Model  High Low  tiply) performing models, as well as the previous  second-best performing model (Kintsch). The  additive and multiplicative models are simply applica-Kintsch  tions of vector addition and component-wise  multiMultiply  plication. We invite the reader to consult (Mitchell Categorical  and Lapata, 2008) for the description of Kintsch's  UpperBound 4.94  additive model and parametric choices.  Model Parameters To provide the most accurate  Table 3: Selected model means for High and Low similar-comparison with the existing multiplicative model,  ity items and correlation coefficients with human judgements, first experiment (Mitchell and Lapata, 2008). p < and exploiting the aforementioned feature that the  0.05 for each .  categorical model can be built on top of existing lexical distributional models, we used the parameters described by Mitchell and Lapata (2008) to  reproduce the vectors evaluated in the original experiment as our noun vectors. All vectors were built  from a lemmatised version of the BNC. The noun  basis was the 2000 most common context words,  basis weights were the probability of context words 0.7  given the target word divided by the overall  probability of the context word. Intransitive verb function-0.6  vectors were trained using the procedure presented  in 4. Since the dataset only contains intransitive verbs and nouns, we used S = N. The cosine mea-0.4  High  sure of vectors was used as a similarity metric.  First Experiment Results In Table 3 we present  Figure 6: Distribution of predicted similarities for the cat-the comparison of the selected models. Our  categoregorical distributional model on High and Low similarity ical model performs significantly better than the ex-items.  isting second-place (Kintsch) and obtains a quasi-identical to the multiplicative model, indicating sig-Second Dataset Description The second dataset6,  nificant correlation with the annotator scores.  developed by the authors, follows the format of the There is not a large difference between the mean  (Mitchell and Lapata, 2008) dataset used for the first High score and mean Low score, but the distri-experiment, with the exception that the target and  bution in Figure 6 shows that our model makes a  landmark verbs are transitive, and an object noun  non-negligible distinction between high similarity  is provided in addition to the subject noun, hence  phrases and low similarity phrases, despite the  abforming a small transitive sentence. The dataset  solute scores not being different by more than a few comprises 200 entries consisting of sentence pairs  (hence a total of 400 sentences) constructed by following the procedure outlined in 4 of (Mitchell and 5The multiplicative model presented here is what is quali-Lapata, 2008), using transitive verbs from CELEX7.  fied as best in (Mitchell and Lapata, 2008). However, they also For examples of these sentences, see Table 4. The  present a slightly better performing ( = 0.19) model which is a combination of their multiplicative model and a weighted dataset was split into four sections of 100 entries additive model. The difference in is qualified as not sta-each, with guaranteed 50% exclusive overlap with  tistically significant in the original paper, and furthermore the mixed model requires parametric optimisation hence was not 6http://www.cs.ox.ac.uk/activities/CompD  evaluated against the entire test set. For these reasons, we chose istMeaning/GS2011data.txt  not to include it in the comparison.  exactly two other datasets. Each section was given tors of our categorical model. Transitive verb vecto a group of evaluators, with a total of 25, who were tors were trained as described in 4 with S = N N.  asked to form simple transitive sentence pairs from the verbs, subject and object provided in each entry; Second Experiment Results The results for the  for instance the table showed the result' from table models evaluated against the second dataset are pre-show result'. The evaluators were then asked to rate sented in Table 5.  the semantic similarity of each verb pair within the Model  High Low  context of those sentences, and offer a score between  1 and 7 for each entry. Each entry was given an arbi-Baseline  trary classification of HIGH or LOW by the authors, Add  for the purpose of calculating mean high/low scores Multiply  for each model. For example, the first two pairs in Categorical  table 4 were classified as HIGH, whereas the second UpperBound 4.80  two pairs as LOW.  Table 5: Selected model means for High and Low similar-Sentence 1  ity items and correlation coefficients with human judge-table show result  ments, second experiment. p < 0.05 for each .  map show location map picture location  table show result  We observe a significant (according to p < 0.0.5) map show location map express location  improvement in the alignment of our categorical  model with the human judgements, from 0.17 to  Table 4: Example entries from the transitive dataset with-0.21. The additive model continues to make  litout annotator score, second experiment.  tle distinction between senses of the verb during  composition, and the multiplicative model's  alignment does not change, but becomes statistically  inmethodology for the second experiment was  distinguishable from the non-compositional baseline identical to that of the first, as are the scales for model.  means and scores. Here also, Spearman's  Once again we note that the high-low means are  deemed a more rigorous way of determining how  not very indicative of model performance, as the dif-well a model tracks difference in meaning. This is  ference between high mean and the low mean of the  both because of the imprecise nature of the classifi-categorical model is much smaller than that of the  cation of verb pairs as HIGH or LOW; and since the  both the baseline model and multiplicative model,  objective similarity scores produced by a model that despite better alignment with annotator judgements.  distinguishes sentences of different meaning from  those of similar meaning can be renormalised in  practice. Therefore the delta between HIGH means  In this paper, we described an implementation of the and LOW mean cannot serve as a definite indication  categorical model of meaning (Coecke et al., 2010), of the practical applicability (or lack thereof) of which combines the formal logical and the empiri-semantic models; the means are provided just to aid cal distributional frameworks into a unified seman-comparison with the results of the first experiment.  tic model. The implementation is based on  buildModel Parameters As in the first experiment, the  ing matrices for words with relational types  (adlexical vectors from (Mitchell and Lapata, 2008)  jectives, verbs), and vectors for words with atomic were used for the other models evaluated (additive, types (nouns), based on data from the BNC. We  multiplicative and baseline)8 and for the noun  vecthen show how to apply verbs to their subject/object, in order to compute the meaning of intransitive and 8Kintsch was not evaluated as it required optimising model transitive sentences.  parameters against a held-out segment of the test set, and we could not replicate the methodology of Mitchell and Lapata (2008) with full confidence.  Other work uses matrices to model meaning (Ba-These results show that the high level  categorironi and Zamparelli, 2010; Guevara, 2010), but only cal distributional model, uniting empirical data with for adjective-noun phrases. Our approach easily ap-logical form, can be implemented just like any other plies to such compositions, as well as to sentences concrete model. Furthermore it shows better results containing combinations of adjectives, nouns, verbs, in experiments involving higher syntactic complex-and adverbs. The other key difference is that they  ity. This is just the tip of the iceberg: the mathe-learn their matrices in a top-down fashion, i.e. by re-matics underlying the implementation ensures that  gression from the composite adjective-noun context  it uniformly scales to larger, more complicated sen-vectors, whereas our model is bottom-up: it learns  tences and enables it to compare synonymity of  sensentence/phrase meaning compositionally from the  tences that are of different grammatical structure.  vectors of the compartments of the composites.  Finally, very similar functions, for example a verb with 8 Future Work  argument alternations such as break' in Y breaks'  and X breaks Y', are not treated as unrelated. The Treatment of function words such as that', who',  matrix of the intransitive break' uses the  corpusas well as logical words such as quantifiers and con-observed information about the subject of break, in-junctives are left to future work. This will build  cluding that of Y', similarly the matrix of the tran-alongside the general guidelines of Coecke et al.  sitive break' uses information about its subject and (2010) and concrete insights from the work of Wid-object, including that of X' and Y'. We leave a  dows (2005). It is not yet entirely clear how  exthorough study of these phenomena, which fall  unisting set-theoretic approaches, for example that of der providing a modular representation of passive-discourse representation and generalised quantifiers, active similarities, to future work.  apply to our setting. Preliminary work on integration of the two has been presented by Preller (2007) and We evaluated our model in two ways: first against  more recently also by Preller and Sadrzadeh ( 2009).  the word disambiguation task of Mitchell and  Lapata (2008) for intransitive verbs, and then against a As mentioned by one of the reviewers, our pre-similar new experiment for transitive verbs, which  group approach to grammar flattens the sentence  we developed.  representation, in that the verb is applied to its sub-Our findings in the first experiment show that  ject and object at the same time; whereas in other  the categorical method performs on par with the  approaches such as CCG, it is first applied to the  leading existing approaches. This should not  surobject to produce a verb phrase, then applied to the prise us given that the context is so small and our subject to produce the sentence. The advantages and method becomes similar to the multiplicative model  disadvantages of this method and comparisons with  of Mitchell and Lapata (2008). However, our  apother systems, in particular CCG, constitutes  ongoproach is sensitive to grammatical structure,  leading us to develop a second experiment taking this  9 Acknowledgement  into account and differentiating it from models with commutative composition operations.  We wish to thank P. Blunsom, S. Clark, B. Coecke,  The second experiment's results deliver the  exS. Pulman, and the anonymous EMNLP  reviewpected qualitative difference between models, with  ers for discussions and comments. Support from  our categorical model outperforming the others and  EPSRC grant EP/F042728/1 is gratefully  acknowlshowing an increase in alignment with human  judgeedged by M. Sadrzadeh.  ments in correlation with the increase in sentence  complexity. We use this second evaluation  principally to show that there is a strong case for the devel-References  opment of more complex experiments measuring not  H. Alshawi (ed). 1992. The Core Language Engine.  only the disambiguating qualities of compositional  MIT Press.  models, but also their syntactic sensitivity, which is M. Baroni and R. Zamparelli. 2010. Nouns are vectors, not directly measured in the existing experiments.  adjectives are matrices. Proceedings of Conference  on Empirical Methods in Natural Language Processing R. Montague. 1974. English as a formal language.  ForS. Clark and S. Pulman. 2007. Combining Symbolic  J. Nivre. 2003. An efficient algorithm for projective and Distributional Models of Meaning. Proceedings  dependency parsing. Proceedings of the 8th  Internaof AAAI Spring Symposium on Quantum Interaction.  tional Workshop on Parsing Technologies (IWPT).  AAAI Press.  A. Preller. Towards Discourse Representation via Pre-B. Coecke, and E. Paquette. 2011. Categories for the group Grammars. Journal of Logic Language Infor-Practicing Physicist. New Structures for Physics, 167-mation 16 173 194.  271. B. Coecke (ed.). Lecture Notes in Physics 813.  A. Preller and M. Sadrzadeh. Semantic Vector  Models and Functional Models for Pregroup Grammars.  B. Coecke, M. Sadrzadeh and S. Clark. 2010. Mathemat-Journal of Logic Language Information.  ical Foundations for Distributed Compositional Model 10.1007/s10849-011-9132-2. to appear.  of Meaning. Lambek Festschrift. Linguistic Analysis J. Saffron, E. Newport, R. Asling. 1999. Word Segmenta-36, 345 384. J. van Benthem, M. Moortgat and W.  tion: The role of distributional cues. Journal of Mem-Buszkowski (eds.).  ory and Language 35, 606 621.  J. Curran. 2004. From Distributional to Semantic Simi-H. Schuetze. 1998. Automatic Word Sense  Discriminalarity. PhD Thesis, University of Edinburgh.  tion. Computational Linguistics 24, 97 123.  K. Erk and S. Pad . 2004. A Structured Vector Space P. Smolensky. 1990. Tensor product variable binding Model for Word Meaning in Context. Proceedings  and the representation of symbolic structures in con-of Conference on Empirical Methods in Natural  Lannectionist systems. Computational Linguistics 46, 1  guage Processing (EMNLP), 897 906.  D. Widdows. 2005. Geometry and Meaning. University  J. R. Firth. 1957. A synopsis of linguistic theory 1930-of Chicago Press.  1955. Studies in Linguistic Analysis.  L. Wittgenstein. 1953. Philosophical Investigations.  Blackwell.  E. Grefenstette, M. Sadrzadeh, S. Clark, B. Coecke, S. Pulman. 2011. Concrete Compositional Sentence  Spaces for a Compositional Distributional Model of  Meaning. International Conference on Computational  Semantics (IWCS'11). Oxford.  G. Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer.  E. Guevara. 2010. A Regression Model of  AdjectiveNoun Compositionality in Distributional Semantics.  Proceedings of the ACL GEMS Workshop.  Z. S. Harris. 1966. A Cycling Cancellation-Automaton for Sentence Well-Formedness. International Computation Centre Bulletin 5, 69 94.  R. Hudson. 1984. Word Grammar. Blackwell.  J. Lambek. 2008. From Word to Sentence. Polimetrica, Milan.  T. Landauer, and S. Dumais. 2008. A solution to Platos problem: The latent semantic analysis theory of ac-quisition, induction, and representation of knowledge.  Psychological review.  C. D. Manning, P. Raghavan, and H. Sch tze. 2008. Introduction to information retrieval. Cambridge  University Press.  J. Mitchell and M. Lapata. 2008. Vector-based  models of semantic composition. Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, 236 244. 