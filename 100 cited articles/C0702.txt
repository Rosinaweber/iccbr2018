 Extracting Aspect-Evaluation and Aspect-of Relations in Opinion Mining Nozomi Kobayashi Kentaro Inui, and Yuji Matsumoto Graduate School of Information Science,  Nara Institute of Science and Technology  reviews) can be classified into two approaches:  DocThe technology of opinion extraction allows  ument classification and information extraction. The  users to retrieve and analyze people's  opinformer is the task of classifying documents or  passages according to their semantic orientation such as  positive vs. negative. This direction has been  forming of the opinion holder, the subject being  ing the mainstream of research on opinion-sensitive  evaluated, the part or the attribute in which  text processing (Pang et al., 2002; Turney, 2002,  the subject is evaluated, and the value of the  etc.). The latter, on the other hand, focuses on the  evaluation that expresses a positive or  negtask of extracting opinions consisting of information  ative assessment. We use this definition as  about, for example, hwho feels how about which as-the basis for our opinion extraction task. We  pect of what producti from unstructured text data.  focus on two important subtasks of opinion  In this paper, we refer to this information  extractionextraction: (a) extracting aspect-evaluation  oriented task as opinion extraction. In contrast to relations, and (b) extracting aspect-of re-sentiment classification, opinion extraction aims at  lations, and we approach each task using  producing richer information and requires an  inmethods which combine contextual and  stadepth analysis of opinions, which has only recently  tistical clues. Our experiments on Japanese  been attempted by a growing but still relatively small  weblog posts show that the use of  contexresearch community (Yi et al., 2003; Hu and Liu,  tual clues improve the performance for both  Most previous work on customer opinion  extraction assumes the source of information to be  Introduction  customer reviews collected from customer review  The explosive increase in Web communication has  Liu et al., 2005). In contrast, in this paper, we  conattracted increasing interest in technologies for  autosider the task of extracting customer opinions from  matically mining personal opinions from Web  docunstructured weblog posts. Compared with  extracuments such as product reviews and weblogs. Such  tion from review articles, extraction from weblogs  technologies would benefit users who seek reviews  is more challenging because weblog posts tend to  on certain consumer products of interest.  exhibit greater diversity in topics, goals,  vocabuPrevious approaches to the task of mining a  largelary, style, etc. and are much more likely to  inscale document collection of customer opinions (or  clude descriptions irrelevant to the subject in  question. In this paper, we first describe our task  set Currently, NTT Cyber Space Laboratories,  ting of opinion extraction. We conducted a corpus  study and investigated the feasibility of the task  defProceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 1065 1074, Prague, June 2007. c 2007 Association for Computational Linguistics  inition by showing the statistics and inter-annotator G!H%I  agreement of our corpus annotation. Next, we show  that the crucial body of the above opinion  extraction task can be decomposed into two kinds of  relation extraction, i.e. aspect-evaluation relation  exC"7"D*$!9E  traction and aspect-of relation extraction. For  example, the passage I went out for lunch at the Deli  and ordered a curry with chicken.  It was pretty  good has an aspect-evaluation relation hcurry with Figure 1: Extraction of opinion units  chicken, was goodi and an aspect-of relation hThe Deli, curry with the chickeni. The former task can person (the author). We say the opinion holder  be regarded as a special type of predicate-argument  is unspecified if the opinion is mentioned as a  structure analysis or semantic role labeling. The  latter, on the other hand, can be regarded as  bridgSubject A named entity (product or company) of  ing reference resolution (Clark, 1977), which is the  a given particular class of interest (e.g. a car  task of identifying relations between definite noun  model name in the automobile domain).  phrases and discourse-new entities implicitly related  Aspect A part, member or related object, or an  atto some previously mentioned entities.  tribute (of a part) of the subject on which the  Most of the previous work on customer opinion  extraction, however, does not adopt the  state-of-theEvaluation An evaluative or subjective phrase used art techniques in those fields, relying only on sim-to express an evaluation or the opinion holder's  ple proximity-based or pattern-based methods. In  mental/emotional attitude ( good, poor,  powerthis context, this paper empirically shows that  incorful, stylish, (I) like, (I) am satisfied, etc.)  porating machine learning-based techniques devised  for predicate-argument structure analysis and  bridgAccording to this typology, the example in Figure 1  ing reference resolution improve the performance  has six constituents, the writer (opinion holder), of both aspect-evaluation and aspect-of relation ex-Powershot (subject), pictures (aspect), colors (as-traction. Furthermore, we also show that  combinpect), beautiful (evaluation), easy to grip (evalua-ing contextual clues with a common co-occurrence  tion), and constitute two units of opinions as  prestatistics-based technique for bridging reference  ressented in the right half of the figure. We call such  olution makes a significant improvement on  aspecta unit an opinion unit. In this paper, we only con-of relation extraction.  sider explicitly mentioned evaluative opinions as our  targets of extraction, excluding opinions indirectly  Opinion extraction: Task design  expressed through, for example, style or language  choice from our scope.  Our present goal is to build a computational model  Under this assumption, opinion extraction can be  to extract opinions from Web documents in such a  defined as a task of filling a fixed number of slots  form as: Who feels how on which aspects of which as above for each of the evaluations expressed in a  subjects. Given the passage presented in Figure 1, given text collection. Two issues then immediately  for example, the opinion we want to extract is: the  arise. First, it is necessary to make sure that the  defwriter feels that the colors of pictures taken with inition of the opinion units is clear enough for hu-Powershot (product) are beautiful. As suggested man annotators to be able to carry out the task with  by this example, we consider it reasonable to start  sufficient accuracy. Second, all the slots might not  with an assumption that most evaluative opinions  consist of simple expressions in that the filler of an  can be structured as a frame composed of the  folaspect slot may have a hierarchical structure in  itlowing constituents:  self. For example, the leather cover of the seats (of Opinion holder The person who is making an eval-a car) refers to a part of a part of a car. In theory, uation. An opinion holder is typically the first  such a hierarchical chain can be of any length, which  may affect the feasibility of the task. For tackling Table 1:  Statistics of opinion-annotated corpus  these issues, we built a corpus annotated with the  above sort of information and investigated the  feasiPhone  bility of the task.  Corpus study  # of opinion units  We first collected 116 Japanese weblog posts in the  Asp-Asp  restaurant domain by randomly sampling from a  collection of posts classified under the gourmet  category on a major blog site: http://blog.livedoor.com/.  We asked two annotators to annotate them  indeother  pendently of each other following the above  specNon-writer op. holder  ification. The annotators first identified evaluative  phrases, and then for each evaluative phrase judged  and agr( A 1 ||A 2) was 0.80 (F1 measure was 0.79), whether it was concerning a particular subject (i.e.  which show that the human annotators can carry out  a restaurant) in the given domain. If judged yes,  the task at a reasonable accuracy. Based on this  corthe annotators filled the opinion holder and subject  pus study, we believe that our definitions of two  reslots obligatorily. The annotators filled the aspect  lations are clear enough for constructing annotated  slot only when its filler appeared in the document  and identified the hierarchical relations between  aspects if any (e.g. noodle and its volume). Note that, 2.1.2  Opinion-annotated corpus  if a sentence has two or more evaluations, they have  Based on these results, we collected a larger set of  to make one opinion unit for each.  bile, cellular phone and video game. We then asked  Inter-annotator agreement  annotator A 1 to annotate them in the same annota-We investigated the degree of inter-annotator  tion scheme as above. The results are summarized in  agreement. In the task of identifying evaluations,  Table 1. I in the table shows the number of the iden-one annotator A 1 identified 450 evaluations while tified opinion units and relations, and II shows the the other A 2 identified 392, and 329 cases of them number of hierarchical chains of aspects. For exam-coincided. The two annotators did not identify the  ple, Nokia 6800 has a nice color screen is counted same number of evaluations, so instead of using  as Subj-Asp-Eval since this example includes a  kappa statistics, we use the following metric for subject Nokia 6800 , an aspect color screen and  an evaluation nice . Other indicates the number  agr( A 1 ||A 2) = # of tags agreed by A 1 and A 2  # of tags annotated by A 1  of the case where the length of hierarchical chains of  agr( A 1 ||A 2) was 0.73 and agr( A 2 ||A 1) was 0.83.  aspects is three or more. One observation is that, for  The F1 measure of the agreement between the two  all the domains, 90 % of all the opinion units have  was therefore 0.79, which indicate that humans can  a hierarchical chain of aspects whose length is two  identify evaluation at a reasonable level.  or less. From this, we can conclude that hierarchical  Next, we investigated the inter-annotator  agreechains longer than two are rare, and the problem is  ment of the aspect-evaluation and subject-evaluation  not so complicated, though they can be of any length  relations. Annotator A 1 identified 328 relations, and in theory.  A 2 identified 346 relations. 295 cases coincided, and The row of Non-writer op(inion) holder at the  agr( A 2 ||A 1) was 0.90 and agr( A 1 ||A 2) was 0.86  bottom of Table 1 shows the number of opinion units  (F1 measure was 0.88). This shows that we obtained  whose opinion holder is not the writer of the weblog.  high consistency.  Finally, for the subject-aspect  This result indicates that when an evaluative  expresand aspect-aspect relations, annotator A 1 identified sion is found, its opinion holder is highly likely to be  296 relations, while A 2 identified 293, 233 cases the writer of the blogs. Therefore, we put aside the  of which got agreement.  agr( A 2 ||A 1) was 0.79  task of filling the opinion holder slot in this paper.  Related work on task settings of opinion  There are several researches on customer opinion  extraction. Hu and Liu (2004) considered the task of  extracting hAspect, Sentence, Semantic-orientationi  Figure 2: The distributions of evaluation and aspect  triples in our terminology, where Sentence is the one expressions in the four domains  that includes the Aspect, and Semantic-orientation is either positive or negative.  of aspect types is nearly 3,200, and only 3% of them  The notion of Evaluation in our term has also  appear in two or more domains as shown in Figure 2.  been introduced by previous work (Popescu and  EtFor evaluation expressions, on the other hand, the  number of types is much smaller than that of aspect  expressions, and 27% of them appear in multiple  doprevious paper (Kobayashi et al., 2005) addresses  mains. This indicates that evaluation expressions are  the task of extracting h Subject,Aspect,Evaluation i.  more likely to be used commonly across different  However, none of those papers reports on such an  domains compared with aspects.  extensive corpus study as what we report in this  To prove this assumption, we created a dictionary  paper. In addition, in this paper, we consider not  of evaluation expressions from customer reviews of  only aspect-evaluation relations but also hierarchical  automobiles (230,000 sentences in total) using the  chains of subject-aspect and aspect-aspect relations,  semi-automatic method proposed by Kobayashi et  which has never been addressed in previous work.  al. (2004). We expanded the dictionary by hand with  Open-domain opinion extraction is another trend  external resources including publicly available  orof research on opinion extraction, which aims to  exdinal thesauri. As a result, we collected 5,550  entract a wider range of opinions from such texts as  tries. According to our investigation of the coverage  newspaper articles (Yu and Hatzivassiloglou, 2003;  by the dictionary, 0.84 (restaurant), 0.88 (cellular  phone), 0.91 (automobile), and 0.93 (video game) of  al., 2006). To the best of our knowledge, one of  the evaluations annotated in our corpus are covered  the most extensive corpus studies in this field has  by the dictionary. From this observation, we  conbeen conducted in the MPQA project (Wiebe et al.,  sider that it is reasonable to start opinion extraction  2005); while their concerns include the types of  with the identification of evaluation expressions. We  opinions we consider, they annotate newspaper  artitherefore design the process of extracting h Subject, cles, which presumably exhibit considerably differ-Aspect, Evaluation i as follows:  ent characteristics from customer-generated texts.  1. Aspect-evaluation relation extraction:  Though we do not discuss the problem of  detereach of the candidate evaluations that are  semining semantic orientation, we assume  availabillected from a given document by dictionary  ity of state-of-the-art methods that perform this task  look-up, identify the target of the evaluation.  Here the identified target may be a subject (e.g.  The problem of determining semantic orientation  IXY (is well-designed)) or an aspect of a  subwill be solved by using these techniques, so we  foject (e.g. the quality (is amazing)). Hereafter,  cus on the main issue: Extracting opinion units from  we use the term aspect to refer to both an  aspect and a subject itself, since the subject can  Method for opinion extraction  be regarded as the top element in the  hierarchical chain of aspects.  Before designing a model for our opinion  extrac2. Opinion-hood determination: Judge whether or  tion task, it is important to note that aspect phrases  not the obtained pair h aspect, evaluation i is an are open-class expressions and tend to be heavily  expression of an opinion by considering the  domain-dependent. In fact, according to our  investigiven context. If it is judged yes, go to step3;  gation on our opinion-annotated corpus, the number  otherwise, return to step 1 with a new candidate  model. Kim and Hovy (2006) proposed a method  3. Aspect-of relation extraction: If the identified for extracting opinion holders, topics and opinion  aspect is not a subject, search for its antecedent,  words, in which they use semantic role labeling as  i.e. an expression that is a higher aspect or a  an intermediate step to label opinion holders and  subject of the current aspect. Repeat step 3  untopics. However, these approaches do not address  til reaching a subject or no parent is found.  the task of extracting aspect-of relations and make  use of syntactic features only for labeling opinion  Related work on opinion extraction  holders and topics. In contrast, as we describe  beA common approach to the customer opinion  extraclow, we find the significant overlap between  aspecttion task mainly uses simple or  patternevaluation relation extraction and aspect-of relation  based techniques. For example, Tateishi et al. (2004)  extraction and apply the same approach to both  tasks, gaining the generality of the model.  al. (2005) use ten syntactic patterns. Such an  apAspect-of relations can be regarded as a sub-type  proach is limited in two respects. First, it assumes  of bridging reference (Clark, 1977), which is a  comthe availability of a list of potential aspect  expresmon linguistic phenomenon where the referent of a  sions as well as evaluation expressions; however  credefinite noun phrase refers to a discourse-new entity  ating such a list of aspects for a variety of domains  implicitly related to some previously mentioned  encan be prohibitively expensive because of the  dotity. For example, we can see a relation of  bridgmain dependency of aspect expressions. In contrast,  ing reference between the door and the room  our method does not require any aspect lexicon.  in She entered the room.  The door closed  auSecond, their approach lacks the perspective of  tomatically.  A common approach is to use  coviewing aspect-evaluation extraction as a specific  occurrence statistics between the referring  exprestype of predicate-argument structure analysis, i.e.  sion (e.g. the door in the above example) and the the task of identifying the arguments of a given  related entity ( the room ) (Bunescu, 2003; Poesio predicate in a given text, and fails to benefit from  et al., 2004). Our approach newly incorporates  authe state-of-the-art techniques of this rapidly  growtomatically induced syntactic patterns as contextual  ing field. The syntactic patterns used in their  research are analyzed by a dependency parser,  howsignificant improvements of accuracy.  ever, aspect-evaluation relations appear in diverse  syntactic patterns, which cannot be easily captured  Our approach  by a handful of manually devised rules.  Now we describe our approach to aspect-evaluation  An exception is the model reported by Kanayama  and aspect-of relation extraction. The key idea is  et al.(2004), which uses a component of an  existto combine the following two kinds of information  ing MT system to identify the aspect argument of  using a machine learning technique for both tasks.  a given evaluation predicate. However, the MT  Contextual clues: Syntactic patterns such as  component they use is not publicly available, and  hAspecti-ga  even if it were, it would be difficult to apply it to  hAspecti-NOM  tasks in hand due of the opaqueness of its  mechawhich matches such a sentence as  nism. Our approach aims to develop a more  generally applicable model of aspect-evaluation  extrachservicei-NOM be trained-CONJ hfeel comfortablei ( The waiters were well-trained, so I felt comfort-tion.  In open-domain opinion extraction, some  apare considered to be useful for extracting  relaproaches use syntactic features obtained from parsed  tions between slot fillers when they appear in a  input sentences (Choi et al., 2006; Kim and Hovy,  single sentence (Here, hi indicates a slot filler).  2006), as is commonly done in semantic role  labelWe employ a supervised learning technique to  ing. Choi et al. (2006) address the task of  extractsearch for such useful syntactic patterns.  ing opinion entities and their relations, and  incorContext-independent statistical clues: Statistics  porate syntactic features to their relation extraction  such as aspect-aspect and aspect-evaluation  Given a set of training examples represented as  ordered trees labeled either positive or negative class,  s  this algorithm learns a list of weighted decision  stumps as a discrimination function with the  BoostIJK  ing algorithm. Each decision stump is associated  with tuple hs, l, wi, where s is a subtree appearing 9Hu<vG  in the training set, l a label, and w a weight of this  pattern. The strength of this algorithm is that it  automatically acquires structured features and allows us  to analyze the utility of features.  Figure 3: Representation of input data  Given a t pair in an annotated sentence, tree encoding of this sentence is done as follows: First, we  use a dependency parser to obtain a dependency tree  co-occurrences are expected to be useful. We  as in Figure 3 (a). We assume k  eki ( cake) as the  obtain such statistical clues automatically from  candidate aspect c and oishii ( delicious) as the tar-a large collection of raw documents.  get evaluation t. We then find the path between t In what follows, we describe our method for  and c together with their daughter nodes. For exam-aspect-evaluation. The aspect-of relation extraction  ple, the node Darling-no ( Darling's) is kept since is done in an an analogous way.  it is a daughter of c. Then, all the content words are abstracted to either of the class types, evaluation, as-3.2.1  Supervised learning of contextual clues  pect or node, that is, c is renamed as aspect , t as Let us consider the problem of searching for the  evaluation and all other content words as node .  aspect of a given evaluation expression t. This prob-Other information of a content word and the  inforlem can be decomposed into binary classification  mation of functional words attaching to the content  problems of deciding whether each pair of candidate  word are represented as the leaf nodes as shown in  aspect c and target t is in an aspect-evaluation rela-Figure 3 (b). The features used in our experiments  tion or not. Our goal is to learn a discrimination  are summarized in Table 2.  function for this classification problem. If such a  We apply the same method to the aspect-of  relafunction is obtained, we can identify the most likely  tion extraction by replacing the evaluation label as  candidate aspect simply by selecting the best scored  the second aspect label.  t pair and, if its score is negative for all possible candidates, we conclude that t has no corresponding 3.3  Context-independent statistical clues  aspect in the candidate set.  We also introduce the following two kinds of  statisFor finding syntactic patterns that extract an  aspect c starting with an evaluation t, we first repre-i.  Co-occurrences of  aspect-evaluation/aspectsent all the sentences in the annotated corpus that  aspect:  Among various ways to estimate the  has both an aspect and its evaluation, as shown in  strength of association (e.g. the number of hits  reFigure 3. A sentence is analyzed by a dependency  turned from a search engine), in our experiments,  parser, then the dependency tree is converted so  we extracted aspect-aspect and aspect-evaluation  as to represent the relation between content words  co-occurrences in 1.7 million weblog posts  usclearly and to attach other information (such as POS  ing the patterns h aspect i ga/wa/mo h evaluation i labels and other morphological features of content  ( h aspect i is (subject-marker) h evaluation i) and words and the functional words attached to the  con h aspect A i no h aspect B i ga/wa ( h aspect B i of tent words) as shown in the lower part of Figure 3.  h aspect A i is) .  To avoid the data sparseness  Among various classifier induction algorithms for  problem, we use Probabilistic Latent Semantic  Intree-structured data, in our experiments, we have so  dexing (PLSI) (Hofmann, 1999) to estimate  confar examined Kudo and Matsumoto (2004)'s  algoditional probabilities P ( Aspect|Evaluation) and rithm, packaged as a free software named BACT.  P ( Aspect A|Aspect B). We then incorporate the 1070  information of these probability scores into the a given domain. In this paper, we conduct a prelim-learning model described in 3.2 by encoding them  inary experiment which uses the opinion-hood  deas a feature that indicates the relative score rank of  termination model learned by Support Vector  Maeach candidate in a given candidate set (see Table 2).  chines. We conduct the model using our  opinionii. Aspect-hood of candidate aspects: Aspect-hood  annotated corpus. The positive examples are  aspectis an index of the degree that measures how plausible  evaluation pairs annotated in the corpus. The  nega term is used as an aspect within a given domain.  ative examples are artificially generated as follows:  We consider that a phrase directly co-occurred with  We first identify the expression in the evaluation  dica subject often is likely to be an aspect of the  subtionary that appear in our annotated corpus. We then  ject, and extract the expression X which appears in apply the above aspect-evaluation extraction method  the form Subject no X ( X of Subject) and the ex-and get the most plausible candidate aspect. The  repression Y which appears in the form X no Y . We sult is regarded as a negative example if the extracted  calculate the aspect-hood of the expressions X and aspect is not the true aspect. The features we used in  Y by the pointwise mutual information. This score our experiments are summarized in Table 2.  is also used as a features (see Table 2).  Intra-/inter-sentential relation extraction  We conducted experiments with our Japanese  Syntactic pattern induction as described in 3.2.1 can  opinion-annotated corpus to empirically evaluate the  apply only when an aspect-evaluation (or aspect-of)  performance of our approach.  In these  experirelation appears in a single sentence. We therefore  ments, we separately evaluated the models of  aspectbuild a separate model for inter-sentential relation  evaluation relation extraction, aspect-of relation  exextraction, which is carried out after intra-sentential  traction, and opinion-hood determination.  relation extraction.  Common settings  target evaluation (or aspect), select the most likely  We chose 395 weblog posts in the restaurant  docandidate aspect c within the target sentence with main from our opinion-annotated corpus described  the intra-sentential model described in 3.2.1. If the  in 2.1, and conducted 5-fold cross validation on  score of c is positive, return c ; otherwise, go to the that dataset.  As preprocessing, we analyzed this  inter-sentential relation extraction phase.  corpus using the Japanese morphological analyzer  2) Inter-sentential relation identification: Search  ChaSen 1 and the Japanese dependency structure anfor the most likely candidate aspect in the sentences  alyzer CaboCha 2.  preceding the target evaluation (or aspect).  This  task can be regarded as a zero-anaphora resolution  problem. For this purpose, we employ the  superThe results are summarized in Tables 3 and 4. We  vised learning model for zero-anaphora resolution  evaluated the results by recall R and precision P de-proposed by (Iida et al., 2003).  fined as follows  Opinion-hood determination  correctly extracted relations  total number of relations  Evaluation phrases do not always extract correct  correctly extracted relations  opinion units in a given domain. Consider an  examtotal number of relations found by the system  ple from the digital camera domain, The weather  was good. so I went to the park to take some  picNote that, in aspect-of relations, we permit h A,C i tures .  good expresses the evaluation for the  to be correct when the data includes the chain of  weather , but the weather is not an aspect of digi-aspect-of relations h A,B i and h B,C i.  Therefore,  tal cameras. Therefore, hthe weather, goodi is not an we merged the and inter-sentential results as  opinion in the digital camera domain. We can  conshown in Table 4.  sider a binary classification task of judging whether  the obtained opinion unit is a real opinion or not in  Table 2: Feature list: t denotes a given target (eval-Table 3: The results of aspect-evaluation relation  uation or aspect) and c a candidate  Position of c / t in the sentence (beginning, end, other) R  Base phrase distance between c and t (1, 2, 3, 4, other) Contextual  Whether c and t has a immediate dependency relation R  Whether c precedes t  Whether c appears in a quoted sentence  +statistics  Part-of-speech of c / t  Suffix of c ( -sei, -sa (-ty), etc.) Table 4: The results of aspect-of relation  Character type of c ( English, Chinese, Katakana, etc.) precision  Semantic class of c derived from Nihongo Goi Taikei (Ike-Co-occurrence  Features for statistical clues  Contextual+statistics  Co-occurrence score rank of c (1st, 2nd, 3rd, 4th, other)  Aspect-hood score rank of c (1st, 2nd, 3rd, 4th, other) cision and 20% improvement in recall over the co-occurrence statistics-based model. We can say that  The Contextual and Contextual+statistics models contextual clues are also useful in aspect-of rela-are our proposed models where the former uses only  tion extraction. In comparing the Contextual and  contextual clues (3.2.1) and the latter uses both  conContextual+statistics models, on the other hand, we  textual and statistical clues. We prepared two  basecould get only a slight improvement, which indicates  line models, one for each of the above tasks. The  that we need to estimate the statistical clues more  Pattern model (in Table 3) simulates the pattern-precisely. We found that the unsophisticated  estibased method proposed by Tateishi at al. (2004),  mation of the statistical clues was a major source of  which uses the following patterns: h Aspect i case-errors in aspect-of relation extraction, however, this  particle h Evaluation i and h Evaluation i syntacti-estimation is not so easy since the correct  exprescally depends on h Aspect i .  The Co-occurrence  sions are appeared only once in large data. We are  model (in Table 4) simulates the co-occurrence  seeking efficient ways to avoid data sparseness  probstatistics-based model used in bridging reference  lem (e.g. categorize the aspects).  resolution (Bunescu, 2003): For an aspect  expresIn the aspect-evaluation relation extraction, we  sion, we select the nearest candidate that has the  evaluated the results against the human annotated  highest positive score of the pointwise mutual  ingold-standard in a strict manner. However,  accordformation regardless of its occurrence (i.e.  intering to our error analysis, some of the errors can be  Comparing the Pattern (  Coregarded as correct for some real applications. In  occurrence) model with the Contextual model shows the following example, a relation annotated by the  the effects of the supervised learning with  contextual clues, while comparison of the Contextual and Contextual+statistics models shows the joint effect misoshiru-wa hajii-ga  of combining contextual and statistical clues.  ( The taste of the miso soup is strong.)  However, there is no harm to consider that  misResults and discussions  oshiru (miso soup), koi-me (strong) is also correct.  As for the aspect-evaluation relation extraction,  conIf we judge these cases as correct, the Proposed  cerning the intra-sentential cases, we can see that  models achieve nearly 0.8 precision and 0.7 recall,  the models using the contextual clues show nearly  and the baseline model also get 7 % improvement  10% improvement in both precision and recall. This  (precision 0.63 and recall 0.6). Based on this  reindicates that the machine learning-based method  sult, we consider that we achieved reasonable  perhas a great advantage over the pattern-based  apformance in intra-sentential aspect-evaluation  relaproach. Similar results are seen in aspect-of  relation extraction.  tion extraction. The models using the contextual  As Table 3 shows, inter-sentential relation  exclues achieved more than 10% improvement in  pretraction achieved very poorly. In the case of  intersentential relations, our model tends to rely heavily Table 5: Comparing intra-sentential models among  on the statistical clues, because syntactic pattern  feathree domains (upper: aspect-eval, lower: aspect-of)  tures cannot be used. However, our current method  for estimating co-occurrence distributions is not  sophisticated as we discussed above. We need to seek  other  for more effective use of large scale domain  dependent data to obtain better statistics.  We also conducted a preliminary test of the  opinion-hood determination model using the  feaother  tures used in aspect-evaluation relation extraction.  As a result, we got 0.5 precision and 0.45 recall.  Opinion-hood determination problem includes two  dicates that the contextual clues learned in other  decisions: whether the evaluation candidate is an  domains are effective in another domain, showing  opinion or not, and whether the opinion is related  the cross-domain portability of our intra-sentential  to the given domain if the evaluation candidate is  an opinion. We plan to use various features known  to be effective in the sentence subjectivity  recogniConclusion  tion task. This task involves challenging problems.  In this paper, we described our opinion  extracFor example, sentence (1) includes the writer's  evaltion task, which extract opinion units consisting  uation on shrimps served at a particular restaurant.  of four constituents. We showed the feasibility of  the task definition based on our corpus study. We  press evaluation since it is a generic description of  consider the task as two kinds of relation  extracthe writer's taste.  tion tasks, aspect-evaluation relation extraction and  aspect-of relation extraction, and proposed a  mathe restaurant shrimp like  ( I like shrimps of the restaurant.)  chine learning-based method which combines  conresults show that the model using contextual clues  improved the performance for both tasks. We also  Thus we need to conduct further investigation in  orshowed domain portability of the contextual clues.  der to resolve this kind of problems.  Portability of intra-sentential model  We next evaluated effectiveness of the contextual  R. Bunescu. 2003. Associative anaphora resolution: a  webclues learned in the domains to other domains by  based approach. In Proceedings of the EACL Workshop on The Computational Treatment of Anaphora, pages 47 52.  testing a model trained on the certain domains to  Y. Choi, E. Breck, and C. Cardie. 2006. Joint extraction of enti-other domain. We selected two new domains,  celties and relations for opinion recognition. In Proceedings of lular phone and automobile, and annotated 290 we-the Conference on Empirical Methods in Natural Language Processing ( EMNLP), pages 431 439.  blog posts in each domain. For the restaurant  doH. H. Clark. 1977. Bridging. Thinking: readings in cognitive main, we randomly selected 290 posts from the pre-science. Cambridge : Cambridge University Press.  viously mentioned our annotated corpus. We then  T. Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of the 22nd Annual ACM Conference on Re-divide each data set to a training set and a test set  search and Development in Information Retrieval ( SIGIR), so that we had the same amount of training data for  each domain. Then we trained a model on the data  M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth International Conference for each domain, and applied it to each of the three  on Knowledge Discovery and Data Mining ( KDD), pages set of data. Table 5 shows the results of the experi-168 177.  ment. Compared with the model trained on the same  R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003. Incorporating contextual cues in trainable models for coreference domain, the models trained on different domains ex-resolution. In Proceedings of the EACL Workshop on The hibited almost comparable performance. This in-Computational Treatment of Anaphora, pages 23 30.  S. Ikehara, M. Miyazaki, S. Shirai A. Yokoo, H. Nakaiwa, J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expres-K. Ogura, Y. Ooyama, and Y. Hayashi. 1997. Nihongo Goi sions of opinions and emotions in language. Language Re-Taikei (in Japanese). Iwanami Shoten.  sources and Evaluation, 39:165 210.  J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003. Senti-ysis using machine translation technology. In Proc. of the ment analyzer: Extracting sentiments about a given topic us-20th International Conference on Computational Linguis-ing natural language processing techniques. In Proceedings tics( COLING), pages 494 500.  of the third IEEE International Conference on Data Mining S. Kim and E. Hovy. 2004. Determining the sentiment of  opinions. In Proceedings of the 20th International Conference on H. Yu and V. Hatzivassiloglou. 2003. Towards answering opin-Computational Linguistics ( COLING), pages 1367 1373.  ion questions: Separating facts from opinions and identify-S. Kim and E. Hovy. 2006. Extracting opinions, opinion hold-ing the polarity of opinion sentences. In Proceedings of the ers, and topics expressed in online news media text. In Pro-Conference on Empirical Methods in Natural Language Proceedings of the ACL Workshop on Sentiment and Subjectivity cessing ( EMNLP), pages 129 136.  in Text.  Collecting evaluative expressions  for opinion extraction. In Proceedings of the 1st International Joint Conference on Natural Language Processing ( IJCNLP) , pages 584 589.  Opinion extraction using a learning-based anaphora  resolution technique. In The Second International Joint Conference on Natural Language Processing ( IJCNLP) , Companion Volume to the Proceeding of Conference including Posters/Demos and Tutorial Abstracts, pages 175 180.  T. Kudo and Y. Matsumoto. 2004. A boosting algorithm for  classification of semi-structured text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP).  B. Liu, M. Hu, and J. Cheng. 2005. Opinion observer: Analyz-ing and comparing opinions on the web. In Proceedings of the 14th International World Wide Web Conference ( WWW), pages 342 351.  B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?  In Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP), pages 79 86.  M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004.  Learning to resolve bridging references. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL).  A. Popescu and O. Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP), pages 339 346.  Y. Suzuki, H. Takamura, and M. Okumura. 2006. Application of semi-supervised learning to evaluative expression classification. In Proceedings of the 6th International Conference on Intelligent Text Processing and Computational Linguistics ( CICLing).  H. Takamura, T. Inui, and M. Okumura. 2006. Latent variable models for semantic orientations of phrases. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics ( EACL) , pages 201 208.  K. Tateishi, T. Fukushima, N. Kobayashi, T. Takahashi, A. Fu-jita, K. Inui, and Y. Matsumoto. 2004. Web opinion extraction and summarization based on viewpoints of products. In IPSJ SIGNL Note 163, pages 1 8. (in Japanese).  P. D. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics ( ACL), pages 417 424. 