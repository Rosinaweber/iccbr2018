 Multiple Aspect Ranking using the Good Grief Algorithm Benjamin Snyder and Regina Barzilay  Computer Science and Artificial Intelligence Laboratory  Massachusetts Institute of Technology  restaurant. Rather than lumping these aspects into a  single score, we would like to capture each aspect of  We address the problem of analyzing  multhe writer's opinion separately, thereby providing a  tiple related opinions in a text. For  inmore fine-grained view of opinions in the review.  stance, in a restaurant review such  opinTo this end, we aim to predict a set of numeric  ions may include food, ambience and  serranks that reflects the user's satisfaction for each  asvice. We formulate this task as a multiple  pect. In the example above, we would assign a  nuaspect ranking problem, where the goal is  meric rank from 1-5 for each of: food quality,  serto produce a set of numerical scores, one  vice, and ambience.  for each aspect. We present an algorithm  A straightforward approach to this task would be  that jointly learns ranking models for  into rank1 the text independently for each aspect,  usdividual aspects by modeling the  depening standard ranking techniques such as regression  dencies between assigned ranks. This  alor classification. However, this approach fails to  exgorithm guides the prediction of  individual rankers by analyzing meta-relations  ments across different aspects. Knowledge of these  between opinions, such as agreement and  contrast. We prove that our  agreementranks, as a user's opinions on one aspect can  influbased joint model is more expressive than  ence his or her opinions on others.  The algorithm presented in this paper models  results further confirm the strength of the  the dependencies between different labels via the  model: the algorithm provides significant  agreement relation. The agreement relation captures improvement over both individual rankers  whether the user equally likes all aspects of the item  and a state-of-the-art joint ranking model.  or whether he or she expresses different degrees of  satisfaction. Since this relation can often be  deter1 Introduction  mined automatically for a given text (Marcu and  Echihabi, 2002), we can readily use it to improve  Previous work on sentiment categorization makes an  rank prediction.  implicit assumption that a single score can express  The Good Grief model consists of a ranking  the polarity of an opinion text (Pang et al., 2002;  model for each aspect as well as an agreement model  Turney, 2002; Yu and Hatzivassiloglou, 2003).  which predicts whether or not all rank aspects are  However, multiple opinions on related matters are  often intertwined throughout a text. For example,  1In this paper, ranking refers to the task of assigning an inte-a restaurant review may express judgment on food  ger from 1 to k to each instance. This task is sometimes referred to as ordinal regression (Crammer and Singer, 2001) and rat-quality as well as the service and ambience of the  ing prediction (Pang and Lee, 2005).  Proceedings of NAACL HLT 2007, pages 300 307,  2007 Association for Computational Linguistics  equal. The Good Grief decoding algorithm pre-which form the basis of our approach. The first is  dicts a set of ranks one for each aspect which  a model proposed by Crammer and Singer (2001).  maximally satisfy the preferences of the individual  The task is to predict a rank y {1, ..., k} for  evrankers and the agreement model. For example, if  ery input x Rn. Their model stores a weight  the agreement model predicts consensus but the  invector w Rn and a vector of increasing  bounddividual rankers select ranks h5, 5, 4i, then the  decoder decides whether to trust the the third ranker,  which divide the real line into k segments, one for  or alter its prediction and output h5, 5, 5i to be  coneach possible rank. The model first scores each input  sistent with the agreement prediction. To obtain a  with the weight vector: score(x) = w x. Finally,  model well-suited for this decoding, we also develop  the model locates score(x) on the real line and  rea joint training method that conjoins the training of  turns the appropriate rank as indicated by the  boundaries. Formally, the model returns the rank r such  We demonstrate that the agreement-based joint  that br 1 score(x) < br. The model is trained  model is more expressive than individual ranking  with the Perceptron Ranking algorithm (or PRank  models. That is, every training set that can be  peralgorithm ), which reacts to incorrect predictions on  fectly ranked by individual ranking models for each  the training set by updating the weight and boundary  aspect can also be perfectly ranked with our joint  vectors. The PRanking model and algorithm were  model. In addition, we give a simple example of a  tested on the EachMovie dataset with a separate  training set which cannot be perfectly ranked  withranking model learned for each user in the database.  out agreement-based joint inference. Our  experiAn extension of this model is provided by  Basilmental results further confirm the strength of the  ico and Hofmann (2004) in the context of  collaboraGood Grief model. Our model significantly  outpertive filtering. Instead of training a separate model for  forms individual ranking models as well as a  stateeach user, Basilico and Hofmann train a joint  rankof-the-art joint ranking model.  ing model which shares a set of boundaries across all  users. In addition to these shared boundaries,  user2 Related Work  specific weight vectors are stored. To compute the  score for input x and user i, the weight vectors for  Sentiment Classification Traditionally, categoriza-all users are employed:  tion of opinion texts has been cast as a binary  classification task (Pang et al., 2002; Turney, 2002; Yu and  recent work (Pang and Lee, 2005; Goldberg and  Zhu, 2006) has expanded this analysis to the  ranking framework where the goal is to assess review  where 0 sim(i, j) 1 is the cosine similarity  bepolarity on a multi-point scale. While this approach  tween users i and j, computed on the entire training  provides a richer representation of a single opinion,  set. Once the score has been computed, the  predicit still operates on the assumption of one opinion per  tion rule follows that of the PRanking model. The  text. Our work generalizes this setting to the  probmodel is trained using the PRank algorithm, with the  lem of analyzing multiple opinions or multiple  asexception of the new definition for the scoring  funcpects of an opinion. Since multiple opinions in a  sintion.2 While this model shares information between  gle text are related, it is insufficient to treat them as the different ranking problems, it fails to explicitly  separate single-aspect ranking tasks. This motivates  model relations between the rank predictions. In  our exploration of a new method for joint multiple  contrast, our algorithm uses an agreement model to  aspect ranking.  learn such relations and inform joint predictions.  Ranking The ranking, or ordinal regression,  problem has been extensivly studied in the Machine  Learning and Information Retrieval communities. In  In the notation of Basilico and Hofmann (2004), this  definition of scorei(x) corresponds to the kernel K = (Kid  this section we focus on two online ranking methods  3 The Algorithm  The agreement model is a vector of weights a  Rn. A value of a x > 0 predicts that the ranks of  The goal of our algorithm is to find a rank  assignall m aspects are equal, and a value of a x 0  ment that is consistent with predictions of  individindicates disagreement. The absolute value |a x|  ual rankers and the agreement model. To this end,  indicates the confidence in the agreement prediction.  we develop the Good Grief decoding procedure that  The goal of the decoding procedure is to predict a  minimizes the dissatisfaction ( grief ) of individual joint rank for the m aspects which satisfies the in-components with a joint prediction. In this section,  dividual ranking models as well as the agreement  we formally define the grief of each component, and  model. For a given input x, the individual model  a mechanism for its minimization. We then describe  for aspect i predicts a default rank y[i] based on its  our method for joint training of individual rankers  feature weight and boundary vectors hw[i], b[i]i. In  that takes into account the Good Grief decoding  proaddition, the agreement model makes a prediction  regarding rank consensus based on a x. However,  the default aspect predictions y[1] . . . y[m] may not  3.1 Problem Formulation  accord with the agreement model. For example, if  In an m-aspect ranking problem, we are given  a x > 0, but y[i] 6= y[j] for some i, j 1...m, then a training sequence of instance-label pairs  the agreement model predicts complete consensus,  Each instance xt is a  whereas the individual aspect models do not.  feature vector in Rn and the label yt is a vector of  We therefore adopt a joint prediction criterion  m ranks in Ym, where Y = {1, .., k} is the set of  which simultaneously takes into account all model possible ranks. The ith component of yt is the rank  components individual aspect models as well as  for the ith aspect, and will be denoted by y[i]t. The  the agreement model. For each possible  predicgoal is to learn a mapping from instances to rank  tion r = (r[1], ..., r[m]) this criterion assesses the  sets, H : X Ym, which minimizes the distance  level of grief associated with the ith-aspect ranking between predicted ranks and true ranks.  model, gi(x, r[i]). Similarly, we compute the grief  of the agreement model with the joint prediction,  3.2 The Model  ga(x, r) (both gi and ga are defined formally below).  Our m-aspect ranking model contains  The decoder then predicts the m ranks which  minimize the overall grief:  (hw[1], b[1]i, ..., hw[m], b[m]i, a). The first  m components are individual ranking models, one  for each aspect, and the final component is the  agreement model. For each aspect i 1...m, w[i] Rn  is a vector of weights on the input features, and  b[i] Rk 1 is a vector of boundaries which divide  If the default rank predictions for the aspect models,  the real line into k intervals, corresponding to the  y[m]), are in accord with the  agreek possible ranks. The default prediction of the  asment model (both indicating consensus or both  inpect ranking model simply uses the ranking rule of  dicating contrast), then the grief of all model  comthe PRank algorithm. This rule predicts the rank r  ponents will be zero, and we simply output y. On  such that b[i]  the other hand, if y indicates disagreement but the  agreement model predicts consensus, then we have  i(x) can be defined simply as the dot product  w[i] x, or it can take into account the weight vectors  the option of predicting y and bearing the grief of  for other aspects weighted by a measure of  interthe agreement model. Alternatively, we can predict  aspect similarity. We adopt the definition given in  some consensus y0 (i.e. with y0[i] = y0[j], i, j) and  equation 1, replacing the user-specific weight  vecbear the grief of the component ranking models. The  tors with our aspect-specific weight vectors.  decoder H chooses the option with lowest overall  3More precisely (taking into account the possibility of ties):  This decoding criterion assumes that the griefs of the com-302  Now we formally define the measures of grief as well as the (previously trained) agreement model  used in this criterion.  to determine the predicted rank for each aspect. In  concrete terms, for every training instance x, we  preAspect Model Grief We define the grief of the  ithdict the ranks of all aspects simultaneously (step 2 in  aspect ranking model with respect to a rank r to be  Figure 1). Then, for each aspect we make a separate  the smallest magnitude correction term which places  update based on this joint prediction (step 4 in  Figthe input's score into the rth segment of the real line:  ure 1), instead of using the individual models'  preAgreement model The agreement model a is  ass.t.  sumed to have been previously trained on the same  training data. An instance is labeled with a positive  label if all the ranks associated with this instance are  Agreement Model Grief Similarly, we define the  equal. The rest of the instances are labeled as  negagrief of the agreement model with respect to a joint  tive. This model can use any standard training  algorank r = (r[1], . . . , r[m]) as the smallest correction  rithm for binary classification such as Perceptron or  needed to bring the agreement score into accord with  SVM optimization.  the agreement relation between the individual ranks  3.4 Feature Representation  Ranking Models Following previous work on  sentiment classification (Pang et al., 2002), we represent  s.t.  each review as a vector of lexical features. More  specifically, we extract all unigrams and bigrams,  discarding those that appear fewer than three times.  This process yields about 30,000 features.  erates over lexicalized features. The effectiveness  Ranking models Pseudo-code for Good Grief  trainof these features for recognition of discourse  relaing is shown in Figure 1. This training algorithm  tions has been previously shown by Marcu and  Echiis based on PRanking (Crammer and Singer, 2001),  habi (2002). In addition to unigrams and bigrams,  an online perceptron algorithm. The training is  perwe also introduce a feature that measures the  maxiformed by iteratively ranking each training input x  mum contrastive distance between pairs of words in  and updating the model. If the predicted rank y is  a review. For example, the presence of delicious  equal to the true rank y, the weight and boundaries  and dirty indicate high contrast, whereas the pair vectors remain unchanged. On the other hand, if  expensive and slow indicate low contrast. The  y 6= y, then the weights and boundaries are updated  contrastive distance for a pair of words is computed  to improve the prediction for x (step 4.c in Figure 1).  by considering the difference in relative weight  asSee (Crammer and Singer, 2001) for explanation  signed to the words in individually trained PRanking  and analysis of this update rule.  Our algorithm departs from PRanking by  conjoining the updates for the  achieve this by using Good Grief decoding at each  In this section, we prove that our model is able to  step throughout training. Our decoder H(x) (from  perfectly rank a strict superset of the training  corequation 2) uses all the aspect component models pora perfectly rankable by m ranking models indi-ponent models are comparable. In practice, we take an uncali-vidually. We first show that if the independent  rankbrated agreement model a0 and reweight it with a tuning param-ing models can individually rank a training set  pereter: a = a0. The value of is estimated using the development set. We assume that the griefs of the ranking models are fectly, then our model can do so as well. Next, we  comparable since they are jointly trained.  show that our model is more expressive by providing  Input : (x1, y1), ..., (xT , yT ), Agreement model a, Decoder defintion H(x) (from equation 2).  Loop : For t = 1, 2, ..., T :  1. Get a new instance xt Rn.  2. Predict yt = H(x; wt, bt, a) (Equation 2).  4. For aspect i = 1, ..., m:  4.a For r = 1, ..., k 1 :  Figure 1: Good Grief Training. The algorithm is based on PRanking training algorithm. Our algorithm differs in the joint computation of all aspect predictions yt based on the Good Grief Criterion (step 2) and the calculation of updates for each aspect based on the joint prediction (step 4).  a simple illustrative example of a training set which  that Good Grief decoding with the ideal joint model  can only be perfectly ranked with the inclusion of an  the same output as the component ranking models  First we introduce some notation. For each  trainrun separately (since the grief will always be zero for  ing instance (xt, yt), each aspect i 1...m, and  the default rank predictions). Now consider the case  each rank r 1...k, define an auxiliary variable  where the training data is not linearly separable with  regard to agreement classification. Define the  marif y[i]t > r. In words, y[i]t indicates whether the  gin of the worst case error to be = max  true rank y[i]t is to the right or left of a potential (a xt)at < 0}. If < , then again Good Grief de-rank r.  coding will always produce the default results (since  that  the grief of the agreement model will be at most in  (x1, y1), ..., (xT , yT ) is perfectly rankable for  cases of error, whereas the grief of the ranking  modeach aspect independently.  That is, for each  els for any deviation from their default predictions  aspect i 1...m, there exists some ideal model  will be at least ). On the other hand, if , then  v[i] = (w[i] , b[i] ) such that the signed  disthe agreement model errors could potentially disrupt  tance from the prediction to the rth boundary:  the perfect ranking. However, we need only rescale  w[i] xt b[i] has the same sign as the  auxilw := w ( + ) and b := b ( + ) to ensure that  iary variable y[i]t . In other words, the minimum  the grief of the ranking models will always exceed  margin over all training instances and ranks,  the grief of the agreement model in cases where the  ing models can perfectly rank a training set, a joint  Now for the  ranking model with Good Grief decoding can do so  tth training instance, define an  agreeas well.  at, where at = 1 when all  aspects agree in rank and at = 1 when at least  Now we give a simple example of a training set  two aspects disagree in rank. First consider the case  which can only be perfectly ranked with the  addiwhere the agreement model a perfectly classifies all  tion of an agreement model. Consider a training set  training instances: (a xt)at > 0, t. It is clear  of four instances with two rank aspects:  domly select 3,488 reviews for training, 500 for  development and 500 for testing.  Parameter Tuning We used the development set  We can interpret these inputs as feature vectors  corto determine optimal numbers of training iterations  responding to the presence of good , bad , and  for our model and for the baseline models. Also,  but not in the following four sentences:  given an initial uncalibrated agreement model a0, we  The food was good, but not the ambience.  define our agreement model to be a = a0 for an  The food was good, and so was the ambience.  appropriate scaling factor . We tune the value of  The food was bad, but not the ambience.  on the development set.  The food was bad, and so was the ambience.  Corpus Statistics Our training corpus contains  We can further interpret the first rank aspect as the  528 among 55 = 3025 possible rank sets. The most  quality of food, and the second as the quality of the  frequent rank set h5, 5, 5, 5, 5i accounts for 30.5%  ambience, both on a scale of 1-2.  of the training set. However, no other rank set  comA simple ranking model which only considers the  prises more than 5% of the data. To cover 90% of  words good and bad perfectly ranks the food  asoccurrences in the training set, 227 rank sets are  repect. However, it is easy to see that no single model  quired. Therefore, treating a rank tuple as a single  perfectly ranks the ambience aspect. Consider any  label is not a viable option for this task. We also  model hw, b = (b)i. Note that w x1 < b and  find that reviews with full agreement across rank  asw x2 b together imply that w3 < 0, whereas  pects are quite common in our corpus, accounting  w x3 b and w x4 < b together imply that  for 38% of the training data. Thus an  agreementbased approach is natural and relevant.  perfectly rank this corpus.  A rank of 5 is the most common rank for all  asThe addition of an agreement model, however,  pects and thus a prediction of all 5's gives a  MAJORcan easily yield a perfect ranking. With a =  ITY baseline and a natural indication of task  diffi(0, 0, 5) (which predicts contrast with the presence  of the words but not ) and a ranking model for the  Evaluation Measures We evaluate our algorithm  ambience aspect such as w = (1, 1, 0), b = (0),  and the baseline using ranking loss (Crammer and  the Good Grief decoder will produce a perfect rank.  ing loss measures the average distance between  the true rank and the predicted rank. Formally,  We evaluate our multi-aspect ranking algorithm on a  given N test instances (x1, y1), ..., (xN , yN ) of an  corpus5 of restaurant reviews available on the  webm-aspect ranking problem and the corresponding  predictions  yN , ranking loss is defined as  from this website have been previously used in other  . Lower values of this measure  correspond to a better performance of the algorithm.  Each review is accompanied by a set of five ranks,  each on a scale of 1-5, covering food, ambience,  service, value, and overall experience. These ranks are  Comparison with Baselines Table 1 shows the  perprovided by consumers who wrote original reviews.  formance of the Good Grief training algorithm GG  TRAIN+DECODE along with various baselines,  insince all the reviews available on this website  concluding the simple MAJORITY baseline mentioned  tain both a review text and the values for all the five  in section 5. The first competitive baseline, PRANK,  aspects.  learns a separate ranker for each aspect using the  Training and Testing Division Our corpus  conPRank algorithm. The second competitive baseline,  5Data and code used in this paper are available at  SIM, shares the weight vectors across aspects using  a similarity measure (Basilico and Hofmann, 2004).  Food Service Value Atmosphere Experience  PRANK  Table 1: Ranking loss on the test set for variants of Good Grief and various baselines.  PRANK  Table 2: Ranking loss for our model and PRANK  computed separately on cases of actual consensus  formance on the 210 test instances where all the  target ranks agree and the remaining 290 instances  where there is some contrast. As Table 2 shows, we  Figure 2: Rank loss for our algorithm and baselines  outperform the PRANK baseline in both cases.  Howas a function of training round.  ever on the consensus instances we achieve a relative  reduction in error of 21.8% compared to only a 1.1%  reduction for the other set. In cases of consensus,  Both of these methods are described in detail in  Secthe agreement model can guide the ranking models  tion 2. In addition, we consider two variants of our  by reducing the decision space to five rank sets. In  algorithm: GG DECODE employs the PRank  training algorithm to independently train all component  provide sufficient constraints as the vast majority of  ranking models and only applies Good Grief  decodranking sets remain viable. This explains the  perforing at test time. GG ORACLE uses Good Grief  trainmance of GG ORACLE, the variant of our algorithm  ing and decoding but in both cases is given perfect  with perfect knowledge of agreement/disagreement  knowledge of whether or not the true ranks all agree  facts. As shown in Table 1, GG ORACLE yields  sub(instead of using the trained agreement model).  stantial improvement over our algorithm, but most  Our model achieves a rank error of 0.632,  comof this gain comes from consensus instances (see  Tapared to 0.675 for PRANK and 0.663 for SIM. Both  of these differences are statistically significant at  We also examine the impact of the agreement  p < 0.002 by a Fisher Sign Test. The gain in  performodel accuracy on our algorithm. The agreement  mance is observed across all five aspects. Our model  model, when considered on its own, achieves  clasalso yields significant improvement (p < 0.05) over  sification accuracy of 67% on the test set, compared  the decoding-only variant GG DECODE,  confirmto a majority baseline of 58%. However, those  ining the importance of joint training. As shown in  stances with high confidence |a x| exhibit  substantially higher classification accuracy. Figure 3 shows  provement over the baselines across all the training  the performance of the agreement model as a  funcrounds.  tion of the confidence value. The 10% of the data  Model Analysis We separately analyze our  perwith highest confidence values can be classified by  sis shows, this relation does not provide sufficient constraints for non-consensus instances. An avenue  for future research is to consider the impact of  additional rhetorical relations between aspects. We also  plan to theoretically analyze the convergence  properties of this and other joint perceptron algorithms.  Acknowledgments  The authors acknowledge the support of the National  Science Foundation (CAREER grant IIS-0448168 and grant  IIS0415865) and the Microsoft Research Faculty Fellowship.  Thanks to Michael Collins, Pawan Deshpande, Jacob  Eisenstein, Igor Malioutov, Luke Zettlemoyer, and the anonymous reviewers for helpful comments and suggestions. Thanks also Figure 3: Accuracy of the agreement model on sub-to Vasumathi Raman for programming assistance. Any  opinsets of test instances with highest confidence |a x|.  ions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the the agreement model with 90% accuracy, and the  views of the NSF.  third of the data with highest confidence can be  classified at 80% accuracy.  This property explains why the agreement model  helps in joint ranking even though its overall  accuJ. Basilico, T. Hofmann. 2004. Unifying  collaboraracy may seem low. Under the Good Grief criterion,  tive and content-based filtering. In Proceedings of the ICML, 65 72.  the agreement model's prediction will only be  enK. Crammer, Y. Singer. 2001. Pranking with ranking. In  forced when its grief outweighs that of the ranking  models. Thus in cases where the prediction  confiK. Dave, S. Lawrence, D. Pennock. 2003. Mining  the peanut gallery: Opinion extraction and semantic  |a x|) is relatively low,6 the agreement model  classification of product reviews. In Proceedings of  will essentially be ignored.  A. B. Goldberg, X. Zhu. 2006. Seeing stars when there  7 Conclusion and Future Work  aren't many stars: Graph-based semi-supervised  learning for sentiment categorization. In Proceedings of  We considered the problem of analyzing multiple  reHLT/NAACL workshop on TextGraphs, 45 52.  lated aspects of user reviews. The algorithm  preing to generate naturalistic utterances using reviews  sented jointly learns ranking models for individual  in spoken dialogue systems. In Proceedings of  COLaspects by modeling the dependencies between  assigned ranks. The strength of our algorithm lies  in its ability to guide the prediction of individual  to recognizing discourse relations. In Proceedings of rankers using rhetorical relations between aspects  B. Pang, L. Lee. 2005. Seeing stars: Exploiting class  such as agreement and contrast. Our method yields  relationships for sentiment categorization with respect  to rating scales. In Proceedings of the ACL, 115 124.  rankers as well as a state-of-the-art joint ranking  B. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs up?  sentiment classification using machine learning  techniques. In Proceedings of EMNLP, 79 86.  P. Turney. 2002. Thumbs up or thumbs down? semantic  orientation applied to unsupervised classsification of  dencies between different opinions. As our  analyreviews. In Proceedings of the ACL, 417 424.  H. Yu, V. Hatzivassiloglou. 2003. Towards answering  6What counts as relatively low will depend on both the  opinion questions: Separating facts from opinions and  value of the tuning parameter and the confidence of the com-identifying the polarity of opinion sentences. In Pro-ponent ranking models for a particular input x.  ceedings of EMNLP, 129 136. 