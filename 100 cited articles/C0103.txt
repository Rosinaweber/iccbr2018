 When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging  Concordia University  Concordia University  and Gamon (2005), demonstrated that sentiment  annotation classifiers trained in one domain do not  perThis study presents a novel approach to the  form well on other domains. A number of methods  problem of system portability across  differhas been proposed in order to overcome this system  that integrates a corpus-based classifier trained  portability limitation by using out-of-domain data,  on a small set of annotated in-domain data  unlabelled in-domain corpora or a combination of  and a lexicon-based system trained on  Wordin-domain and out-of-domain examples (Aue and  Net. The paper explores the challenges of  system portability across domains and text  genres (movie reviews, news, blogs, and product  reviews), highlights the factors affecting  sysIn this paper, we present a novel approach to the  tem performance on out-of-domain and  smallproblem of system portability across different  doset in-domain data, and presents a new  sysmains by developing a sentiment annotation  system consisting of the ensemble of two  classitem that integrates a corpus-based classifier with  fiers with precision-based vote weighting, that  provides significant gains in accuracy and  rea lexicon-based system trained on WordNet.  call over the corpus-based classifier and the  adopting this approach, we sought to develop a  lexicon-based system taken individually.  system that relies on both general and  domainspecific knowledge, as humans do when analyzing  Introduction  a text. The information contained in  lexicographical sources, such as WordNet, reflects a lay person's  One of the emerging directions in NLP is the  degeneral knowledge about the world, while  domainvelopment of machine learning methods that  perspecific knowledge can be acquired through  classiform well not only on the domain on which they  fier training on a small set of in-domain data.  were trained, but also on other domains, for which  training data is not available or is not sufficient to The first part of this paper reviews the extant lit-ensure adequate machine learning. Many  applicaerature on domain adaptation in sentiment  analytions require reliable processing of heterogeneous  sis and highlights promising directions for research.  corpora, such as the World Wide Web, where the  The second part establishes a baseline for system  diversity of genres and domains present in the  Interevaluation by drawing comparisons of system  pernet limits the feasibility of in-domain training. In  this paper, sentiment annotation is defined as the  movie reviews, news, blogs, and product reviews.  assignment of positive, negative or neutral  sentiThe final, third part of the paper presents our  sysment values to texts, sentences, and other linguistic  tem, composed of an ensemble of two classifiers  units. Recent experiments assessing system  portaone trained on WordNet glosses and synsets and the  bility across different domains, conducted by Aue  other trained on a small in-domain training set.  Proceedings of ACL-08: HLT, pages 290 298,  Columbus, Ohio, USA, June 2008. c  2008 Association for Computational Linguistics  Domain Adaptation in Sentiment  in-domain and 72.39% for the best out-of-domain  Research  training on a large training set.  (2007) applied structural  correMost text-level sentiment classifiers use standard  spondence learning (Drezde et al., 2007) to the task  machine learning techniques to learn and select  feaof domain adaptation for sentiment classification of  tures from labeled corpora. Such approaches work  product reviews. They showed that, depending on  well in situations where large labeled corpora are  the domain, a small number (e.g., 50) of labeled  available for training and validation (e.g., movie  reexamples allows to adapt the model learned on  anviews), but they do not perform well when training  other corpus to a new domain. However, they note  data is scarce or when it comes from a different  dothat the success of such adaptation and the  nummain (Aue and Gamon, 2005; Read, 2005), topic  ber of necessary in-domain examples depends on  (Read, 2005) or time period (Read, 2005). There are  the similarity between the original domain and the  two alternatives to supervised machine learning that  new one. Similarly, Tan et al. (2007) suggested to  can be used to get around this problem: on the one  combine out-of-domain labeled examples with  unlahand, general lists of sentiment clues/features can be belled ones from the target domain in order to solve  acquired from domain-independent sources such as  the domain-transfer problem. They applied an  outdictionaries or the Internet, on the other hand,  unsuof-domain-trained SVM classifier to label examples  pervised and weakly-supervised approaches can be  from the target domain and then retrained the  classiused to take advantage of a small number of  annofier using these new examples. In order to maximize  tated in-domain examples and/or of unlabelled  inthe utility of the examples from the target domain,  these examples were selected using Similarity  RankThe first approach, using general word lists  auing and Relative Similarity Ranking algorithms (Tan  tomatically acquired from the Internet or from  dicet al., 2007). Depending on the similarity between  tionaries, outperforms corpus-based classifiers when  domains, this method brought up to 15% gain  comsuch classifiers use out-of-domain training data or  pared to the baseline SVM.  when the training corpus is not sufficiently large to  Overall, the development of semi-supervised  apaccumulate the necessary feature frequency  inforproaches to sentiment tagging is a promising  direcmation. But such general word lists were shown to  tion of the research in this area but so far, based  perform worse than statistical models built on  sufon reported results, the performance of such  methficiently large in-domain training sets of movie  reods is inferior to the supervised approaches with  inviews (Pang et al., 2002). On other domains, such  domain training and to the methods that use general  as product reviews, the performance of systems that  word lists. It also strongly depends on the similarity use general word lists is comparable to the perfor-between the domains as has been shown by (Drezde  mance of supervised machine learning approaches  (Gamon and Aue, 2005).  The recognition of major performance  deficienFactors Affecting System Performance  cies of supervised machine learning methods with  insufficient or out-of-domain training brought about  The comparison of system performance across  difan increased interest in unsupervised and  weaklyferent domains involves a number of factors that can  supervised approaches to feature learning. For  insignificantly affect system performance from  trainstance, Aue and Gamon (2005) proposed training  ing set size to level of analysis (sentence or entire  on a samll number of labeled examples and large  document), document domain/genre and many other  quantities of unlabelled in-domain data. This  sysfactors. In this section we present a series of experi-tem performed well even when compared to  sysments conducted to assess the effects of different  extems trained on a large set of in-domain examples:  ternal factors (i.e., factors unrelated to the merits of on feedback messages from a web survey on knowl-the system itself) on system performance in order to  edge bases, Aue and Gamon report 73.86%  accuestablish the baseline for performance comparisons  racy using unlabelled data compared to 77.34% for  Level of Analysis  (further, news).  The full set of sentences  Research on sentiment annotation is usually  conwas annotated by one judge. 200 sentences  ducted at the text (Aue and Gamon, 2005; Pang et  from this corpus (100 positive and 100  negal., 2002; Pang and Lee, 2004; Riloff et al., 2006;  ative) were also randomly selected from the  Turney, 2002; Turney and Littman, 2003) or at the  corpus for an inter-annotator agreement study  sentence levels (Gamon and Aue, 2005; Hu and Liu,  and were manually annotated by two  indepen2004; Kim and Hovy, 2005; Riloff et al., 2006). It  dent annotators. The pairwise agreement  beshould be noted that each of these levels presents dif-tween annotators was calculated as the percent  ferent challenges for sentiment annotation. For  exof same tags divided by the number of  senample, it has been observed that texts often contain  tences with this tag in the gold standard. The  multiple opinions on different topics (Turney, 2002;  pair-wise agreement between the three  annoWiebe et al., 2001), which makes assignment of the  tators ranged from 92.5 to 95.9% ( =0.74 and  overall sentiment to the whole document  problem0.75 respectively) on positive vs. negative tags.  atic. On the other hand, each individual sentence  A set of sentences taken from personal  contains a limited number of sentiment clues, which  weblogs (further,  blogs) posted on  Liveoften negatively affects the accuracy and recall if  Journal (http://www.livejournal.com) and on  that single sentiment clue encountered in the  senThis corpus  tence was not learned by the system.  is composed of 800 sentences (400 sentences  Since the comparison of sentiment annotation  with positive and 400 sentences with negative  system performance on texts and on sentences  In order to establish the  interhas not been attempted to date, we also sought  to close this gap in the literature by conducting  were asked to annotate 200 sentences from this  the first set of our comparative experiments on  corpus. The agreement between the two  andata sets of 2,002 movie review texts and 10,662  notators on positive vs. negative tags reached  movie review snippets (5331 with positive and  5331 with negative sentiment) provided by Bo Pang  A set of 1200 product review (PR) sentences  extracted from the annotated corpus made  available by Bing Liu (Hu and Liu, 2004)  Domain Effects  The second set of our experiments explores system  The data set sizes are summarized in Table 1.  For this we used four different data sets of sentences Movies  News Blogs  annotated with sentiment tags:  A set of movie review snippets (further: movie)  from (Pang and Lee, 2005). This dataset of  10,662 snippets was collected automatically  from www.rottentomatoes.com website.  sentences in reviews marked rotten were  conEstablishing a Baseline for a Corpus-based  sidered negative and snippets from fresh  rethe results obtained on this dataset comparable  Supervised statistical methods have been very  sucto other domains, a randomly selected subset of  cessful in sentiment tagging of texts: on movie  re1066 snippets was used in the experiments.  view texts they reach accuracies of 85-90% (Aue  and Gamon, 2005; Pang and Lee, 2004). These  A balanced corpus of 800 manually annotated  methods perform particularly well when a large  volsentences extracted from 83 newspaper texts  ume of labeled data from the same domain as the  test set is available for training (Aue and Gamon, 4  2005). For this reason, most of the research on  sentiment tagging using statistical classifiers was limited 4.1  System Performance on Texts vs. Sentences  to product and movie reviews, where review authors  The experiments comparing in-domain trained  sysusually indicate their sentiment in a form of a  stantem performance on texts vs. sentences were  condardized score that accompanies the texts of their  reducted on 2,002 movie review texts and on 10,662  The results with 10-fold  The lack of sufficient data for training appears to  cross-validation are reported in Table 22.  be the main reason for the virtual absence of  experiments with statistical classifiers in sentiment  tagTrained on Texts  Trained on Sent.  ging at the sentence level. To our knowledge, the  Tested on Tested on Tested on Tested on  only work that describes the application of  statisTexts  Texts  classification is (Gamon and Aue, 2005)1. The  average performance of the system on ternary  classification (positive, negative, and neutral) was  beTable 2: Accuracy of Na ve Bayes on movie reviews.  tween 0.50 and 0.52 for both average precision and  recall. The results reported by (Riloff et al., 2006)  Consistent with findings in the literature (Cui et  for binary classification of sentences in a related  al., 2006; Dave et al., 2003; Gamon and Aue, 2005),  domain of subjectivity tagging (i.e., the separation  on the large corpus of movie review texts, the  indomain-trained system based solely on unigrams  that statistical classifiers can perform well on this  had lower accuracy than the similar system trained  task: the authors have reached 74.9% accuracy on  on bigrams. But the trigrams fared slightly worse  the MPQA corpus (Riloff et al., 2006).  than bigrams. On sentences, however, we have  obIn order to explore the performance of  different approaches in sentiment annotation at the  ter than bigrams and trigrams. These results  hightext and sentence levels, we used a basic Na ve  light a special property of sentence-level  annotaBayes classifier.  It has been shown that both  tion: greater sensitivity to sparseness of the model:  Na ve Bayes and SVMs perform with similar  acOn texts, classifier error on one particular sentiment curacy on different sentiment tagging tasks (Pang  marker is often compensated by a number of  corand Lee, 2004).  These observations were  conrectly identified other sentiment clues. Since  senfirmed with our own experiments with SVMs and  tences usually contain a much smaller number of  Na ve Bayes (Table 3). We used the Weka  packsentiment clues than texts, sentence-level  annotaage (http://www.cs.waikato.ac.nz/ml/weka/) with  tion more readily yields errors when a single  sentiment clue is incorrectly identified or missed by  In the sections that follow, we describe a set  the system. Due to lower frequency of higher-order  of comparative experiments with SVMs and Na ve  n-grams (as opposed to unigrams), higher-order  nBayes classifiers (1) on texts and sentences and (2)  gram language models are more sparse, which  inon four different domains (movie reviews, news,  creases the probability of missing a particular  senblogs, and product reviews). System runs with  untiment marker in a sentence (Table 33). Very large  igrams, bigrams, and trigrams as features and with  2All results are statistically significant at = 0.01 with two different training set sizes are presented.  exceptions: the difference between trigrams and bigrams for the system trained and tested on texts is statistically significant at alpha=0.1 and for the system trained on sentences and tested on 1Recently, a similar task has been addressed by the Affective texts is not statistically significant at = 0.01.  Text Task at SemEval-1 where even shorter units headlines 3The results for movie reviews are lower than those reported  were classified into positive, negative and neutral categories in Table 2 since the dataset is 10 times smaller, which results using a variety of techniques (Strapparava and Mihalcea, 2007).  in less accurate classification. The statistical significance of the 293  training sets are required to overcome this higher n-It is interesting to note that on sentences,  regardless of the domain used in system training and  regardless of the domain used in system testing,  unigrams tend to perform better than higher-order  ngrams. This observation suggests that, given the  constraints on the size of the available training sets, SVM  unigram-based systems may be better suited for  nb features  Lexicon-Based Approach  The search for a base-learner that can produce  greatnb features  est synergies with a classifier trained on small-set  in-domain data has turned our attention to  lexiconbased systems. Since the benefits from combining  classifiers that always make similar decisions is  minimal, the two (or more) base-learners should  comnb features  plement each other (Alpaydin, 2004). Since a  sysTable 3: Accuracy of unigram, bigram and trigram  modtem based on a fairly different learning approach  is more likely to produce a different decision  under a given set of circumstances, the diversity of  approaches integrated in the ensemble of classifiers  System Performance on Different Domains  was expected to have a beneficial effect on the  overIn the second set of experiments we sought to  comall system performance.  pare system results on sentences using in-domain  A lexicon-based approach capitalizes on the  and out-of-domain training. Table 4 shows that  infact that dictionaries, such as WordNet  (Felldomain training, as expected, consistently yields  subaum, 1998), contain a comprehensive and  domainperior accuracy than out-of-domain training across  independent set of sentiment clues that exist in  all four datasets: movie reviews (Movies), news,  general English.  A system trained on such  genblogs, and product reviews (PRs). The numbers for  eral data, therefore, should be less sensitive to  doin-domain trained runs are highlighted in bold.  main changes. This robustness, however is expected  to come at some cost, since some domain-specific  sentiment clues may not be covered in the  dictioTraining Data Movies News Blogs  nary. Our hypothesis was, therefore, that a  lexiconbased system will perform worse than an in-domain  News  trained classifier but possibly better than a classifier Blogs  trained on out-of domain data.  One of the limitations of general lexicons and  dictionaries, such as WordNet (Fellbaum, 1998), as  Table 4: Accuracy of SVM with unigram model  training sets for sentiment tagging systems is that  they contain only definitions of individual words  results depends on the genre and size of the n-gram: on prod-and, hence, only unigrams could be effectively  uct reviews, all results are statistically significant at = 0.025  level; on movie reviews, the difference between Na ve Bayes learned from dictionary entries.  Since the  strucand SVM is statistically significant at = 0.01 but the signif-ture of WordNet glosses is fairly different from  icance diminishes as the size of the n-gram increases; on news, that of other types of corpora, we developed a sys-only bi-grams produce a statistically significant ( = 0.01) dif-tem that used the list of human-annotated  adjecference between the two machine learning methods, while on blogs the difference between SVMs and Na ve Bayes is most tives from (Hatzivassiloglou and McKeown, 1997)  pronounced when unigrams are used ( = 0.025).  as a seed list and then learned additional unigrams  from WordNet synsets and glosses with up to 88%  Movies News Blogs PRs  accuracy, when evaluated against General Inquirer  (Stone et al., 1966) (GI) on the intersection of our  automatically acquired list with GI. In order to  exSVM out-of-dom.  pand the list coverage for our experiments at the text and sentence levels, we then augmented the list by  Table 5: System accuracy on best runs on sentences  adding to it all the words annotated with Positiv  or Negativ tags in GI, that were not picked up by  trained corpus-based classifiers, and with similar  the system. The resulting list of features contained  or better accuracy than the corpus-based classifiers  11,000 unigrams with the degree of membership in  trained on out-of-domain data. Thus, the  lexiconthe category of positive or negative sentiment  asbased approach is characterized by a bounded but  signed to each of them.  stable performance when the system is ported across  In order to assign the membership score to each  These performance characteristics of  corpus-based and lexicon-based approaches prompt  intersecting seed lists drawn from manually  annofurther investigation into the possibility to combine  tated list of positive and negative adjectives from  the portability of dictionary-trained systems with the (Hatzivassiloglou and McKeown, 1997).  The 58  accuracy of in-domain trained systems.  runs were then collapsed into a single set of 7,813  unique words. For each word we computed a score  Integrating the Corpus-based and  by subtracting the total number of runs assigning  Dictionary-based Approaches  this word a negative sentiment from the total of the  runs that consider it positive. The resulting measure, The strategy of integration of two or more sys-termed Net Overlap Score (NOS), reflected the  numtems in a single ensemble of classifiers has been  ber of ties linking a given word with other  sentimentactively used on different tasks within NLP. In  senladen words in WordNet, and hence, could be used  timent tagging and related areas, Aue and Gamon  as a measure of the words' centrality in the fuzzy  (2005) demonstrated that combining classifiers can  category of sentiment. The NOSs were then  normalbe a valuable tool in domain adaptation for  sentiized into the interval from -1 to +1 using a sigmoid  ment analysis. In the ensemble of classifiers, they  fuzzy membership function (Zadeh, 1975)4. Only  used a combination of nine SVM-based classifiers  words with fuzzy membership degree not equal to  deployed to learn unigrams, bigrams, and trigrams  zero were retained in the list.  The resulting list  on three different domains, while the fourth domain  contained 10,809 sentiment-bearing words of  differwas used as an evaluation set. Using then an SVM  ent parts of speech. The sentiment determination at  meta-classifier trained on a small number of target  the sentence and text level was then done by  sumdomain examples to combine the nine base  clasming up the scores of all identified positive unigrams sifiers, they obtained a statistically significant  im(NOS>0) and all negative unigrams (NOS<0)  (Anprovement on out-of-domain texts from book  reviews, knowledge-base feedback, and product  support services survey data. No improvement occurred  Establishing a Baseline for the  Lexicon-Based System (LBS)  Pang and Lee (2004) applied two different  clasThe baseline performance of the Lexicon-Based  sifiers to perform sentiment annotation in two  seSystem (LBS) described above is presented in  Taquential steps: the first classifier separated  subjecble 5, along with the performance results of the  and out-of-domain-trained SVM classifier.  ones and then they used the second classifier to  clasTable 5 confirms the predicted pattern:  the  sify the subjective texts into positive and negative.  LBS performs with lower accuracy than  Das and Chen (2004) used five classifiers to  determine market sentiment on Yahoo! postings. Simple  4With coefficients: =1, =15.  majority vote was applied to make decisions within  the ensemble of classifiers and achieved accuracy of and high precision on negatives6. Such complemen-62% on ternary in-domain classification.  tary distribution of errors produced by the two  sysIn this study we describe a system that attempts to  tems was observed on different data sets from  differcombine the portability of a dictionary-trained  sysent domains, which suggests that the observed  distem (LBS) with the accuracy of an in-domain trained  tribution pattern reflects the properties of each of  corpus-based system (CBS). The selection of these  the classifiers, rather than the specifics of the  dotwo classifiers for this system, thus, was  theorybased. The section that follows describes the  classiIn order to take advantage of the observed  comfier integration and presents the performance results  plementarity of the two systems, the following  proof the system consisting of an ensemble CBS and  cedure was used. First, a small set of in-domain  LBS classifier and a precision-based vote weighting  data was used to train the CBS system. Then both  CBS and LBS systems were run separately on the  same training set, and for each classifier, the  preciThe Classifier Integration Procedure and  sion measures were calculated separately for those  System Evaluation  sentences that the classifier considered positive and  those it considered negative. The chance-level  perThe comparative analysis of the corpus-based and  formance (50%) was then subtracted from the  prelexicon-based systems described above revealed that  cision figures to ensure that the final weights reflect the errors produced by CBS and LBS were to a  by how much the classifier's precision exceeds the  great extent complementary (i.e., where one  classichance level. The resulting chance-adjusted  precifier makes an error, the other tends to give the  corsion numbers of the two classifiers were then  norrect answer). This provided further justification to  malized, so that the weights of CBS and LBS  clasthe integration of corpus-based and lexicon-based  sifiers sum up to 100% on positive and to 100% on  approaches in a single system.  negative sentences. These weights were then used  Table 6 below illustrates the complementarity of  to adjust the contribution of each classifier to the de-the performance CBS and LBS classifiers on the  cision of the ensemble system. The choice of the  positive and negative categories. In this experiment,  weight applied to the classifier decision, thus, varied the corpus-based classifier was trained on 400 an-depending on whether the classifier scored a given  notated product review sentences5. The two systems  sentence as positive or as negative. The resulting  were then evaluated on a test set of another 400  prodsystem was then tested on a separate test set of  senuct review sentences. The results reported in Table 6  tences7. The small-set training and evaluation  experare statistically significant at = 0.01.  iments with the system were performed on different  domains using 3-fold validation.  CBS  The experiments conducted with the Ensemble  Precision positives  system were designed to explore system  perforPrecision negatives 55.5% 81.5%  mance under conditions of limited availability of  anPos/Neg Precision  notated data for classifier training. For this reason, the numbers reported for the corpus-based classifier  Table 6: Base-learners' precision and recall on product reviews on test data.  do not reflect the full potential of machine  learning approaches when sufficient in-domain training  Table 6 shows that the corpus-based system has a  data is available. Table 7 presents the results of  very good precision on those sentences that it  classithese experiments by domain/genre.  The results  fies as positive but makes a lot of errors on those sen-6  tences that it deems negative. At the same time, the  These results are consistent with an observation in  (Kennedy and Inkpen, 2006), where a lexicon-based system lexicon-based system has low precision on positives  performed with a better precision on negative than on positive texts.  5The small training set explains relatively low overall per-7The size of the test set varied in different experiments due formance of the CBS system.  to the availability of annotated data for a particular domain.  are statistically significant at = 0.01, except the sifiers are complementary, i.e., where one classifier  runs on movie reviews where the difference between  makes an error, the other tends to give the correct  the LBS and Ensemble classifiers was significant at  answer, (2) the classifier errors are not fully random = 0.05.  and occur more often in a certain segment (or  category) of classifier results, and (3) there is a way for LBS CBS Ensemble  a system to identify that low-precision segment and  News  reduce the weights of that classifier's results on that F  segment accordingly. The two classifiers used in this  study corpus-based and lexicon-based provided  an interesting illustration of potential performance  gains associated with these three conditions. The  use of precision of classifier results on the positives PRs  and negatives proved to be an effective technique for  classifier vote weighting within the ensemble.  Average Acc 60.7 54.2 71.1  Conclusion  Table 7: Performance of the ensemble classifier  This study contributes to the research on sentiment  tagging, domain adaptation, and the development of  Table 7 shows that the combination of two  classiensembles of classifiers (1) by proposing a novel  apfiers into an ensemble using the weighting technique  proach for sentiment determination at sentence level  described above leads to consistent improvement in  and delineating the conditions under which  greatsystem performance across all domains/genres. In  est synergies among combined classifiers can be  the ensemble system, the average gain in accuracy  achieved, (2) by describing a precision-based  techacross the four domains was 16.9% relative to CBS  nique for assigning differential weights to classifier and 10.3% relative to LBS. Moreover, the gain in  results on different categories identified by the clas-accuracy and precision was not offset by decreases  in recall: the net gain in recall was 7.4% relative to tences), and (3) by proposing a new method for sen-CBS and 13.5% vs. LBS. The ensemble system on  timent annotation in situations where the annotated  average reached 99.1% recall. The F-measure has  in-domain data is scarce and insufficient to ensure  increased from 0.77 and 0.72 for LBS and CBS  clasadequate performance of the corpus-based classifier,  sifiers respectively to 0.83 for the whole ensemble  which still remains the preferred choice when large  volumes of annotated data are available for system  Among the most promising directions for future  research in the direction laid out in this paper is the determination systems poses a substantial challenge  deployment of more advanced classifiers and  feafor researchers in NLP and artificial intelligence.  ture selection techniques that can further enhance  The results presented in this study suggest that the  the performance of the ensemble of classifiers. The  integration of two fairly different classifier learning precision-based vote weighting technique may prove  approaches in a single ensemble of classifiers can  to be effective also in situations, where more than  yield substantial gains in system performance on all  two classifiers are integrated into a single system.  measures. The most substantial gains occurred in  We expect that these more advanced  ensemble-ofrecall, accuracy, and F-measure.  classifiers systems would inherit the benefits of  mulThis study permits to highlight a set of factors  that enable substantial performance gains with the  tation and will be able to achieve better and more  ensemble of classifiers approach. Such gains are  stable accuracy on in-domain, as well as on  out-ofmost likely when (1) the errors made by the  clason Natural Language Processing, Companion Volume,  Ethem Alpaydin. 2004. Introduction to Machine  LearnBo Pang and Lilian Lee. 2004. A sentiment education:  ing. The MIT Press, Cambridge, MA.  Sentiment analysis using subjectivity summarization  based on minimum cuts. In Proceedings of the 42nd  WordNet for a fuzzy sentiment: Sentiment tag  extracMeeting of the Association for Computational  Linguistion from WordNet glosses. In Proceedings the 11th  tics.  Conference of the European Chapter of the  AssociaBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting tion for Computational Linguistics, Trento, IT.  class relationships for sentiment categorization with  Anthony Aue and Michael Gamon. 2005. Customizing  respect to rating scales. In Proceedings of the 43nd  sentiment classifiers to new domains: a case study. In Meeting of the Association for Computational Linguis-Proccedings of the International Conference on Recent  tics, Ann Arbor, US.  Advances in Natural Language Processing, Borovets,  Bo Pang, Lilian Lee, and Shrivakumar Vaithyanathan.  2002. Thumbs up? Sentiment classification using  machine learning techniques. In Conference on  Empirilearning parsimonious models for extracting consumer  cal Methods in Natural Language Processing.  opinions. In Proceedings of the 38th Annual Hawaii  Jonathon Read. 2005. Using emoticons to reduce  depenInternational Conference on System Sciences,  Washington, DC.  classification. In Proceedings of the ACL-2005  StuHang Cui, Vibhu Mittal, and Mayur Datar. 2006.  Comdent Research Workshop, Ann Arbor, MI.  parative experiments on sentiment classification for  Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe.  online product reviews. In Proceedings of the 21st  2006. Feature subsumption for opinion analysis. In  International Conference on Artificial Intelligence,  Proceedings of the Conference on Empirical Methods  Boston, MA.  in Natural Language Processing, Sydney, AUS.  Kushal Dave, Steve Lawrence, and David M. Pennock.  P.J. Stone, D.C. Dumphy, M.S. Smith, and D.M. Ogilvie.  2003. Mining the Peanut gallery: opinion extraction  1966. The General Inquirer: a computer approach to  and semantic classification of product reviews. In Pro-content analysis. M.I.T. studies in comparative  policeedings of WWW03, Budapest, HU.  tics. M.I.T. Press, Cambridge, MA.  Biographies, Bollywood, Boom-boxes and Blenders:  2007 Task 14: Affective Text. In Proceedings of the  Domain Adaptation for Sentiment Classification. In  4th International Workshop on Semantic Evaluations,  Proceedings of the 45th Annual Meeting of the  Association for Computational Linguistics, Prague, CZ.  Christiane Fellbaum, editor. 1998. WordNet: An  ElecCheng. 2007. A Novel Scheme for Domain-transfer  tronic Lexical Database. MIT Press, Cambridge, MA.  Problem in the context of Sentiment Analysis. In  ProMichael Gamon and Anthony Aue. 2005. Automatic  ceedings of CIKM 2007.  identification of sentiment vocabulary: exploiting low Peter Turney and Michael Littman. 2003. Measuring  association with known sentiment terms. In  Proceedpraise and criticism: inference of semantic orientation ings of the ACL-05 Workshop on Feature Engineering  from association. ACM Transactions on Information  for Machine Learning in Natural Language  ProcessPeter Turney. 2002. Thumbs up or thumbs down?  Se1997. Predicting the Semantic Orientation of  Adjection of reviews. In Proceedings of the 40th Annual  tives. In Proceedings of the the 40th Annual Meeting  Meeting of the Association of Computational  Linguisof the Association of Computational Linguistics.  tics.  Janyce Wiebe, Rebecca Bruce, Matthew Bell, Melanie  ing customer reviews. In KDD-04, pages 168 177.  Martin, and Theresa Wilson. 2001. A corpus study of  Evaluative and Speculative Language. In Proceedings  Alistair Kennedy and Diana Inkpen.  of the 2nd ACL SIGDial Workshop on Discourse and  ment Classification of Movie Reviews Using  Contextual Valence Shifters. Computational Intelligence,  Lotfy A. Zadeh. 1975. Calculus of Fuzzy Restrictions.  In L.A. Zadeh et al., editor, Fuzzy Sets and their  ApSoo-Min Kim and Eduard Hovy. 2005. Automatic  detecplications to cognitive and decision processes, pages  tion of opinion bearing words and sentences. In  Pro1 40. Academic Press Inc., New-York.  ceedings of the Second International Joint Conference 