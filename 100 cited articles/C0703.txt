 Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 79-86.  Association for Computational Linguistics.  Thumbs up? Sentiment Classification using Machine Learning Techniques  Bo Pang and Lillian Lee  Department of Computer Science  IBM Almaden Research Center  Cornell University  650 Harry Rd.  Ithaca, NY 14853 USA  San Jose, CA 95120 USA  use. Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye's We consider the problem of classifying doc-Lexant system1) and recommender systems (e.g.,  uments not by topic, but by overall  sentiTerveen et al. (1997), Tatemura (2000)), where user  ment, e.g., determining whether a review  input and feedback could be quickly summarized;  inis positive or negative.  deed, in general, free-form survey responses given in views as data, we find that standard ma-natural language format could be processed using  chine learning techniques definitively  outsentiment categorization. Moreover, there are also  perform human-produced baselines.  Howpotential applications to message filtering; for exam-ever, the three machine learning methods  ple, one might be able to use sentiment information  we employed (Naive Bayes, maximum  ento recognize and discard flames (Spertus, 1997).  tropy classification, and support vector  maIn this paper, we examine the effectiveness of  apchines) do not perform as well on sentiment  plying machine learning techniques to the sentiment  classification as on traditional topic-based  classification problem. A challenging aspect of this categorization. We conclude by examining  problem that seems to distinguish it from traditional factors that make the sentiment classifica-topic-based classification is that while topics are of-tion problem more challenging.  ten identifiable by keywords alone, sentiment can be expressed in a more subtle manner. For example, the  Introduction  sentence How could anyone sit through this movie?  contains no single word that is obviously negative.  Today, very large amounts of information are  avail(See Section 7 for more examples). Thus, sentiment  able in on-line documents. As part of the effort to  seems to require more understanding than the usual  better organize this information for users, researchers topic-based classification. So, apart from presenting have been actively investigating the problem of au-our results obtained via machine learning techniques, tomatic text categorization.  we also analyze the problem to gain a better  underThe bulk of such work has focused on topical  catstanding of how difficult it is.  egorization, attempting to sort documents  according to their subject matter (e.g., sports vs.  poliPrevious Work  tics). However, recent years have seen rapid growth  in on-line discussion groups and review sites (e.g., This section briefly surveys previous work on non-the New York Times' Books web page) where a  crutopic-based text categorization.  cial characteristic of the posted articles is their senti-One area of research concentrates on classifying  ment, or overall opinion towards the subject matter  documents according to their source or source style,  for example, whether a product review is  poswith statistically-detected stylistic variation (Biber, itive or negative. Labeling these articles with their 1988) serving as an important cue. Examples in-sentiment would provide succinct summaries to  readclude author, publisher (e.g., the New York Times vs.  ers; indeed, these labels are part of the appeal and The Daily News), native-language background, and  value-add of such sites as www.rottentomatoes.com,  brow (e.g., high-brow vs. popular , or low-brow)  which both labels movie reviews that do not  con(Mosteller and Wallace, 1984; Argamon-Engelson et  tain explicit rating indicators and normalizes the  different rating schemes that individual reviewers  al., 1998; Tomokiyo and Jones, 2001; Kessler et al., difficult of several domains for sentiment classifica-1997).  tion, reporting an accuracy of 65.83% on a  120Another, more related area of research is that of  determining the genre of texts; subjective genres,  But we stress that the machine learning methods and  such as editorial , are often one of the possible  features we use are not specific to movie reviews, and categories (Karlgren and Cutting, 1994; Kessler et  should be easily applicable to other domains as long al., 1997; Finn et al., 2002). Other work explicitly as sufficient training data exists.  attempts to find features indicating that subjective Our data source was the Internet Movie Database  (IMDb) archive of the rec.arts.movies.reviews  2000; Wiebe et al., 2001). But, while techniques for newsgroup.3 We selected only reviews where the au-genre categorization and subjectivity detection can  thor rating was expressed either with stars or some  help us recognize documents that express an  opinnumerical value (other conventions varied too widely ion, they do not address our specific classification to allow for automatic processing). Ratings were  task of determining what that opinion actually is.  automatically extracted and converted into one of  Most previous research on sentiment-based  classithree categories: positive, negative, or neutral. For fication has been at least partially knowledge-based.  the work described in this paper, we concentrated  Some of this work focuses on classifying the semantic only on discriminating between positive and nega-orientation of individual words or phrases, using lintive sentiment. To avoid domination of the corpus  guistic heuristics or a pre-selected set of seed words by a small number of prolific reviewers, we imposed  (Hatzivassiloglou and McKeown, 1997; Turney and  a limit of fewer than 20 reviews per author per  senLittman, 2002). Past work on sentiment-based  cattiment category, yielding a corpus of 752 negative  egorization of entire documents has often involved  and 1301 positive reviews, with a total of 144  reeither the use of models inspired by cognitive  linviewers represented. This dataset will be available  guistics (Hearst, 1992; Sack, 1994) or the manual or on-line at  http://www.cs.cornell.edu/people/pabo/semi-manual construction of discriminant-word  lexmovie-review-data/ (the URL contains hyphens only  icons (Huettner and Subasic, 2000; Das and Chen,  around the word review ).  2001; Tong, 2001). Interestingly, our baseline experiments, described in Section 4, show that humans  A Closer Look At the Problem  may not always have the best intuition for choosing  Intuitions seem to differ as to the difficulty of the sen-discriminating words.  timent detection problem. An expert on using  maTurney's (2002) work on classification of reviews  chine learning for text categorization predicted rela-is perhaps the closest to ours.2 He applied a  spetively low performance for automatic methods. On  cific unsupervised learning technique based on the  the other hand, it seems that distinguishing positive mutual information between document phrases and  from negative reviews is relatively easy for humans, the words excellent and poor , where the mu-especially in comparison to the standard text catego-tual information is computed using statistics  gathrization problem, where topics can be closely related.  ered by a search engine. In contrast, we utilize sev-One might also suspect that there are certain words  eral completely prior-knowledge-free supervised  mapeople tend to use to express strong sentiments, so  chine learning methods, with the goal of  understandthat it might suffice to simply produce a list of such ing the inherent difficulty of the task.  words by introspection and rely on them alone to  classify the texts.  The Movie-Review Domain  To test this latter hypothesis, we asked two  graduFor our experiments, we chose to work with movie  ate students in computer science to (independently)  reviews. This domain is experimentally convenient  choose good indicator words for positive and  negabecause there are large on-line collections of such retive sentiments in movie reviews. Their selections,  views, and because reviewers often summarize their  shown in Figure 1, seem intuitively plausible. We  overall sentiment with a machine-extractable  ratthen converted their responses into simple decision  ing indicator, such as a number of stars; hence, we  procedures that essentially count the number of the  did not need to hand-label the data for supervised  proposed positive and negative words in a given  doclearning or evaluation purposes. We also note that  ument. We applied these procedures to  uniformlyTurney (2002) found movie reviews to be the most  distributed data, so that the random-choice baseline result would be 50%. As shown in Figure 1, the  2 Indeed, although our choice of title was completely independent of his, our selections were eerily similar.  Proposed word lists  Accuracy  negative: suck, terrible, awful, unwatchable, hideous Human 2  positive: gripping, mesmerizing, riveting, spectacular, cool, 64%  awesome, thrilling, badass, excellent, moving, exciting negative: bad, cliched, sucks, boring, stupid, slow  Figure 1: Baseline results for human word lists. Data: 700 positive and 700 negative reviews.  Proposed word lists  Accuracy  positive: love, wonderful, best, great, superb, still, beautiful 69%  negative: bad, worst, stupid, waste, boring, ?, !  Figure 2: Results for baseline using introspection and simple statistics of the data (including test data).  accuracy percentage of documents classified  cornot claim that our list was the optimal set of  fourrectly for the human-based classifiers were 58%  and 64%, respectively.4 Note that the tie rates  percentage of documents where the two sentiments  Machine Learning Methods  were rated equally likely are quite high5 (we chose a tie breaking policy that maximized the accuracy of Our aim in this work was to examine whether it suf-the baselines).  fices to treat sentiment classification simply as a spe-While the tie rates suggest that the brevity of  cial case of topic-based categorization (with the two the human-produced lists is a factor in the relatively  topics being positive sentiment and negative  senpoor performance results, it is not the case that size timent), or whether special sentiment-categorization alone necessarily limits accuracy. Based on a very  methods need to be developed. We experimented  preliminary examination of frequency counts in the  with three standard algorithms: Naive Bayes  clasentire corpus (including test data) plus introspection, sification, maximum entropy classification, and sup-we created a list of seven positive and seven negative port vector machines. The philosophies behind these  words (including punctuation), shown in Figure 2.  three algorithms are quite different, but each has  As that figure indicates, using these words raised the been shown to be effective in previous text catego-accuracy to 69%. Also, although this third list is of rization studies.  comparable length to the other two, it has a much  To implement these machine learning algorithms  lower tie rate of 16%. We further observe that some  on our document data, we used the following  stanof the items in this third list, such as ? or still , dard bag-of-features framework. Let {f1, . . . , fm} be would probably not have been proposed as possible  a predefined set of m features that can appear in  candidates merely through introspection, although  a document; examples include the word still or  upon reflection one sees their merit (the question  the bigram really stinks . Let ni(d) be the  nummark tends to occur in sentences like What was the  ber of times fi occurs in document d. Then, each  director thinking? ; still appears in sentences like document d is represented by the document vector  Still, though, it was worth seeing ).  We conclude from these preliminary experiments  that it is worthwhile to explore corpus-based  techNaive Bayes  niques, rather than relying on prior intuitions, to se-One approach to text classification is to assign to a lect good indicator features and to perform sentiment given document d the class c = arg maxc P (c | d).  classification in general. These experiments also pro-We derive the Naive Bayes (NB) classifier by first  vide us with baselines for experimental comparison;  observing that by Bayes' rule,  in particular, the third baseline of 69% might  actually be considered somewhat difficult to beat, since P (c)P (d | c)  it was achieved by examination of the test data  (although our examination was rather cursory; we do  4 Later experiments using these words as features for where P (d) plays no role in selecting c . To estimate machine learning methods did not yield better results.  the term P (d | c), Naive Bayes decomposes it by  as5 This is largely due to 0-0 ties.  suming the fi's are conditionally independent given  class c. The parameter values are set so as to  maximize the entropy of the induced distribution (hence P (c) Qm P (fi | c)ni(d)  the classifier's name) subject to the constraint that NB(c | d) :=  the expected values of the feature/class functions  with respect to the model are equal to their expected Our training method consists of relative-frequency  values with respect to the training data: the  underestimation of P (c) and P (fi | c), using add-one  lying philosophy is that we should choose the model  smoothing.  making the fewest assumptions about the data while  Despite its simplicity and the fact that its  constill remaining consistent with it, which makes intuditional independence assumption clearly does not  itive sense. We use ten iterations of the improved  hold in real-world situations, Naive Bayes-based text iterative scaling algorithm (Della Pietra et al., 1997) categorization still tends to perform surprisingly well for parameter training (this was a sufficient  num(Lewis, 1998); indeed, Domingos and Pazzani (1997)  ber of iterations for convergence of training-data ac-show that Naive Bayes is optimal for certain problem curacy), together with a Gaussian prior to prevent  classes with highly dependent features. On the other overfitting (Chen and Rosenfeld, 2000).  hand, more sophisticated algorithms might (and  often do) yield better results; we examine two such  Support Vector Machines  algorithms next.  Support vector machines (SVMs) have been shown to  be highly effective at traditional text categorization, Maximum entropy classification (MaxEnt, or ME,  generally outperforming Naive Bayes (Joachims,  for short) is an alternative technique which has  1998). They are large-margin, rather than  probaproven effective in a number of natural  lanbilistic, classifiers, in contrast to Naive Bayes and guage processing applications (Berger et al., 1996).  MaxEnt. In the two-category case, the basic idea  beNigam et al. (1999) show that it sometimes, but not  hind the training procedure is to find a hyperplane, always, outperforms Naive Bayes at standard text  represented by vector ~  w, that not only separates  classification. Its estimate of P (c | d) takes the fol-the document vectors in one class from those in the  lowing exponential form:  other, but for which the separation, or margin, is as large as possible. This search corresponds to a  constrained optimization problem; letting cj {1, 1}  (corresponding to positive and negative) be the  correct class of document dj, the solution can be written as  where Z(d) is a normalization function. Fi,c is a fea-X  ture/class function for feature  i and class c, defined  as follows:6  where the  j 's are obtained by solving a dual  optiotherwise  mization problem. Those ~  dj such that j is greater  than zero are called support vectors, since they are For instance, a particular feature/class function  the only document vectors contributing to ~  w.  Clasmight fire if and only if the bigram still hate ap-sification of test instances consists simply of deter-pears and the document's sentiment is hypothesized  mining which side of ~  w's hyperplane they fall on.  to be negative.7 Importantly, unlike Naive Bayes,  We used Joachim's (1999) SV M light package8 for  MaxEnt makes no assumptions about the  relationtraining and testing, with all parameters set to their ships between features, and so might potentially per-default values, after first length-normalizing the doc-form better when conditional independence  assumpument vectors, as is standard (neglecting to  normalize generally hurt performance slightly).  The i,c's are feature-weight parameters;  inspection of the definition of PME shows that a large i,c 6  means that fi is considered a strong indicator for  6 We use a restricted definition of feature/class functions so that MaxEnt relies on the same sort of feature We used documents from the movie-review corpus  information as Naive Bayes.  described in Section 3. To create a data set with uni-7 The dependence on class is necessary for parameter form class distribution (studying the effect of skewed induction. See Nigam et al. (1999) for additional moti-vation.  Features  # of  features  adjectives  Figure 3: Average three-fold cross-validation accuracies, in percent. Boldface: best performance for a given setting (row). Recall that our baseline results ranged from 50% to 69%.  class distributions was out of the scope of this study), general) to be an orthogonal way to incorporate con-we randomly selected 700 positive-sentiment and 700  negative-sentiment documents. We then divided this  data into three equal-sized folds, maintaining  balanced class distributions in each fold. (We did not  The classification  accuuse a larger number of folds due to the slowness of  racies resulting from using only unigrams as  feathe MaxEnt training procedure.) All results reported tures are shown in line (1) of Figure 3. As a whole, below, as well as the baseline results from Section 4, the machine learning algorithms clearly surpass the  are the average three-fold cross-validation results on random-choice baseline of 50%.  They also  handthis data (of course, the baseline algorithms had no ily beat our two human-selected-unigram baselines  of 58% and 64%, and, furthermore, perform well in  To prepare the documents, we automatically  recomparison to the 69% baseline achieved via limited  moved the rating indicators and extracted the  texaccess to the test-data statistics, although the  imtual information from the original HTML  docuprovement in the case of SVMs is not so large.  ment format, treating punctuation as separate  lexOn the other hand, in topic-based classification,  ical items. No stemming or stoplists were used.  all three classifiers have been reported to use  bagOne unconventional step we took was to attempt  of-unigram features to achieve accuracies of 90%  to model the potentially important contextual effect and above for particular categories (Joachims, 1998; of negation: clearly good and not very good in-Nigam et al., 1999)9 and such results are for  settings with more than two classes.  This provides  technique of Das and Chen (2001), we added the tag  suggestive evidence that sentiment categorization is NOT to every word between a negation word ( not ,  more difficult than topic classification, which  corand the first punctuation  responds to the intuitions of the text  categorizamark following the negation word. (Preliminary  extion expert mentioned above.10 Nonetheless, we still periments indicate that removing the negation tag  wanted to investigate ways to improve our  sentihad a negligible, but on average slightly harmful, ef-ment categorization results; these experiments are  fect on performance.)  reported below.  For this study, we focused on features based on  Recall that we  unigrams (with negation tagging) and bigrams.  Berepresent each document d by a feature-count vector  cause training MaxEnt is expensive in the number of  (n1(d), . . . , nm(d)). However, the definition of the features, we limited consideration to (1) the 16165  unigrams appearing at least four times in our  14009 Joachims (1998) used stemming and stoplists; in  document corpus (lower count cutoffs did not yield  some of their experiments, Nigam et al. (1999), like us, significantly different results), and (2) the 16165 bi-did not.  grams occurring most often in the same data (the  We could not perform the natural experiment of  attempting topic-based categorization on our data because selected bigrams all occurred at least seven times).  the only obvious topics would be the film being reviewed; Note that we did not add negation tags to the bi-unfortunately, in our data, the maximum number of re-grams, since we consider bigrams (and n-grams in  views per movie is 27, too small for meaningful results.  MaxEnt feature/class functions Fi,c only reflects the Parts of speech  We also experimented with  appresence or absence of a feature, rather than directly pending POS tags to every word via Oliver Mason's  incorporating feature frequency. In order to investi-Qtag program.12 This serves as a crude form of word  gate whether reliance on frequency information could sense disambiguation (Wilks and Stevenson, 1998):  account for the higher accuracies of Naive Bayes and for example, it would distinguish the different usages SVMs, we binarized the document vectors, setting  of love in I love this movie (indicating sentiment ni(d) to 1 if and only feature fi appears in d, and  orientation) versus This is a love story (neutral  reran Naive Bayes and SV M light on these new  vecwith respect to sentiment). However, the effect of  tors.11  this information seems to be a wash: as depicted in  As can be seen from line (2) of Figure 3,  line (5) of Figure 3, the accuracy improves slightly better performance (much better performance for  for Naive Bayes but declines for SVMs, and the  perSVMs) is achieved by accounting only for  feaformance of MaxEnt is unchanged.  ture presence, not feature frequency. Interestingly, Since adjectives have been a focus of previous work  this is in direct opposition to the observations of  in sentiment detection (Hatzivassiloglou and Wiebe,  McCallum and Nigam (1998) with respect to Naive  2000; Turney, 2002)13, we looked at the performance  Bayes topic classification. We speculate that this in-of using adjectives alone. Intuitively, we might  exdicates a difference between sentiment and topic catpect that adjectives carry a great deal of  informaegorization perhaps due to topic being conveyed  tion regarding a document's sentiment; indeed, the  mostly by particular content words that tend to be  human-produced lists from Section 4 contain almost  repeated but this remains to be verified. In any  no other parts of speech. Yet, the results, shown in event, as a result of this finding, we did not incor-line (6) of Figure 3, are relatively poor: the 2633  porate frequency information into Naive Bayes and  adjectives provide less useful information than  uniSVMs in any of the following experiments.  gram presence. Indeed, line (7) shows that simply  using the 2633 most frequent unigrams is a better  In addition to looking specifically for  choice, yielding performance comparable to that of  negation words in the context of a word, we also  using (the presence of) all 16165 (line (2)). This may studied the use of bigrams to capture more context  imply that applying explicit feature-selection  algoNote that bigrams and unigrams are  rithms on unigrams could improve performance.  surely not conditionally independent, meaning that  the feature set they comprise violates Naive Bayes'  An additional intuition we had was that  conditional-independence assumptions; on the other  the position of a word in the text might make a  difhand, recall that this does not imply that Naive  ference: movie reviews, in particular, might begin  Bayes will necessarily do poorly (Domingos and  Pazwith an overall sentiment statement, proceed with  a plot discussion, and conclude by summarizing the  Line (3) of the results table shows that bigram  author's views. As a rough approximation to  deterinformation does not improve performance beyond  mining this kind of structure, we tagged each word  that of unigram presence, although adding in the  biaccording to whether it appeared in the first  quargrams does not seriously impact the results, even for ter, last quarter, or middle half of the document14.  Naive Bayes. This would not rule out the possibility The results (line (8)) didn't differ greatly from using that bigram presence is as equally useful a feature  unigrams alone, but more refined notions of position as unigram presence; in fact, Pedersen (2001) found  might be more successful.  that bigrams alone can be effective features for word sense disambiguation. However, comparing line (4)  to line (2) shows that relying just on bigrams causes accuracy to decline by as much as 5.8 percentage  The results produced via machine learning  techpoints. Hence, if context is in fact important, as our niques are quite good in comparison to the human-intuitions suggest, bigrams are not effective at cap-generated baselines discussed in Section 4. In terms turing it in our setting.  of relative performance, Naive Bayes tends to do the worst and SVMs tend to do the best, although the  11 Alternatively, we could have tried integrating fre-12  quency information into MaxEnt. However, feature/class http://www.english.bham.ac.uk/staff/oliver/soft-functions are traditionally defined as binary (Berger et ware/tagger/index.htm  al., 1996); hence, explicitly incorporating frequencies 13 Turney's (2002) unsupervised algorithm uses bi-would require different functions for each count (or count grams containing an adjective or an adverb.  bin), making training impractical. But cf. (Nigam et al., 14 We tried a few other settings, e.g., first third vs. last 1999).  third vs middle third, and found them to be less effective.  differences aren't very large.  niques than our positional feature mentioned above), On the other hand, we were not able to achieve ac-or at least some way of determining the focus of each curacies on the sentiment classification problem com-sentence, so that one can decide when the author is  parable to those reported for standard topic-based  talking about the film itself. (Turney (2002) makes  categorization, despite the several different types of a similar point, noting that for reviews, the whole features we tried.  Unigram presence information  is not necessarily the sum of the parts .)  Furtherturned out to be the most effective; in fact, none of more, it seems likely that this thwarted-expectations the alternative features we employed provided consis-rhetorical device will appear in many types of texts tently better performance once unigram presence was  (e.g., editorials) devoted to expressing an overall  incorporated. Interestingly, though, the superiority opinion about some topic. Hence, we believe that an  of presence information in comparison to frequency  important next step is the identification of features information in our setting contradicts previous obser-indicating whether sentences are on-topic (which is  vations made in topic-classification work (McCallum  a kind of co-reference problem); we look forward to  addressing this challenge in future work.  What accounts for these two differences  difficulty and types of information proving useful  Acknowledgments  between topic and sentiment classification, and how  We thank Joshua Goodman, Thorsten Joachims, Jon  might we improve the latter? To answer these  questions, we examined the data further. (All examples  below are drawn from the full 2053-document  corney, and the anonymous reviewers for many valuable  comments and helpful suggestions, and Hubie Chen  As it turns out, a common phenomenon in the  docand Tony Faradjian for participating in our baseline uments was a kind of thwarted expectations narra-experiments. Portions of this work were done while  tive, where the author sets up a deliberate contrast the first author was visiting IBM Almaden. This pa-to earlier discussion: for example, This film should per is based upon work supported in part by the Na-be brilliant. It sounds like a great plot, the actors are tional Science Foundation under ITR/IM grant IIS-first grade, and the supporting cast is good as well, and 0081334. Any opinions, findings, and conclusions or  Stallone is attempting to deliver a good performance.  recommendations expressed above are those of the  However, it can't hold up or I hate the Spice Girls.  authors and do not necessarily reflect the views of  ...[3 things the author hates about them]... Why I saw the National Science Foundation.  this movie is a really, really, really long story, but I did, and one would think I'd despise every minute of it. But... Okay, I'm really ashamed of it, but I enjoyed References  it. I mean, I admit it's a really awful movie ...the ninth Shlomo Argamon-Engelson, Moshe Koppel, and  floor of hell...The plot is such a mess that it's terrible.  Galit Avneri. 1998. Style-based text  categorizaBut I loved it. 15  tion: What newspaper am I reading? In Proc. of  In these examples, a human would easily detect  the AAAI Workshop on Text Categorization, pages  the true sentiment of the review, but bag-of-features 1 4.  classifiers would presumably find these instances difficult, since there are many words indicative of the Adam L. Berger, Stephen A. Della Pietra, and Vin-opposite sentiment to that of the entire review. Funcent J. Della Pietra. 1996. A maximum entropy  approach to natural language processing.  Compudamentally, it seems that some form of discourse  tational Linguistics, 22(1):39 71.  analysis is necessary (using more sophisticated tech-Douglas Biber. 1988. Variation across Speech and  15 This phenomenon is related to another common  Writing. Cambridge University Press.  theme, that of a good actor trapped in a bad movie :  AN AMERICAN WEREWOLF IN PARIS is a failed  atStanley Chen and Ronald Rosenfeld. 2000. A survey  tempt... Julie Delpy is far too good for this movie. She im-of smoothing techniques for ME models. IEEE  bues Serafine with spirit, spunk, and humanity. This isn't Trans. Speech and Audio Processing, 8(1):37 50.  necessarily a good thing, since it prevents us from relax-ing and enjoying AN AMERICAN WEREWOLF IN PARIS  as a completely mindless, campy entertainment experience.  Delpy's injection of class into an otherwise classless produc-Amazon: Extracting market sentiment from stock  tion raises the specter of what this film could have been message boards. In Proc. of the 8th Asia Pacific  with a better script and a better cast ... She was radiant, Finance Association Annual Conference (APFA  charismatic, and effective ....  Stephen Della Pietra, Vincent Della Pietra, and John Frederick Mosteller and David L. Wallace. 1984. Ap-Lafferty. 1997. Inducing features of random fields.  plied Bayesian and Classical Inference: The Case  IEEE Transactions on Pattern Analysis and  Maof the Federalist Papers. Springer-Verlag.  Kamal Nigam, John Lafferty, and Andrew  McCallum. 1999. Using maximum entropy for text  clasthe optimality of the simple Bayesian classifier  unsification. In Proc. of the IJCAI-99 Workshop on  der zero-one loss. Machine Learning, 29(2-3):103  Machine Learning for Information Filtering, pages  Aidan Finn, Nicholas Kushmerick, and Barry Smyth.  Ted Pedersen. 2001. A decision tree of bigrams is an 2002.  Genre classification and domain transfer  accurate predictor of word sense. In Proc. of the  for information filtering.  In Proc. of the  European Colloquium on Information Retrieval  Research, pages 353 362, Glasgow.  Warren Sack. 1994. On the computation of point of  view. In Proc. of the Twelfth AAAI, page 1488.  Vasileios Hatzivassiloglou and Kathleen McKeown.  Student abstract.  1997. Predicting the semantic orientation of  adjectives. In Proc. of the 35th ACL/8th EACL, pages  Ellen Spertus. 1997. Smokey: Automatic  recognition of hostile messages. In Proc. of  Innovative Applications of Artificial Intelligence (IAAI), Vasileios Hatzivassiloglou and Janyce Wiebe. 2000.  Effects of adjective orientation and gradability on  sentence subjectivity. In Proc. of COLING.  Junichi Tatemura. 2000. Virtual reviewers for  colMarti Hearst. 1992. Direction-based text  interprelaborative exploration of movie reviews. In Proc.  tation as an information access refinement.  of the 5th International Conference on Intelligent  Paul Jacobs, editor, Text-Based Intelligent  Systems. Lawrence Erlbaum Associates.  Alison Huettner and Pero Subasic.  Donald, and Josh Creter. 1997. PHOAKS: A  systyping for document management.  In ACL  tem for sharing recommendations.  Communica2000 Companion Volume: Tutorial Abstracts and  tions of the ACM, 40(3):59 62.  Laura Mayfield Tomokiyo and Rosie Jones. 2001.  Thorsten Joachims. 1998. Text categorization with  You're not from round here, are you? Naive Bayes  support vector machines: Learning with many  reldetection of non-native utterance text. In Proc. of  evant features. In Proc. of the European  Conferthe Second NAACL, pages 239 246.  ence on Machine Learning (ECML), pages 137  Richard M. Tong. 2001. An operational system for  detecting and tracking opinions in on-line  discusThorsten Joachims. 1999. Making large-scale SVM  sion. Workshop note, SIGIR 2001 Workshop on  learning practical.  Operational Text Classification.  Alexander Smola, editors, Advances in Kernel  Methods Support Vector Learning, pages 44 56.  Peter D. Turney and Michael L. Littman. 2002.  UnMIT Press.  supervised learning of semantic orientation from  a hundred-billion-word corpus. Technical Report  Jussi Karlgren and Douglass Cutting. 1994.  RecogEGB-1094, National Research Council Canada.  nizing text genres with simple metrics using  discriminant analysis. In Proc. of COLING.  Peter Turney. 2002. Thumbs up or thumbs down?  Semantic orientation applied to unsupervised  classification of reviews. In Proc. of the ACL.  Sch tze. 1997. Automatic detection of text genre.  In Proc. of the 35th ACL/8th EACL, pages 32 38.  Janyce M. Wiebe, Theresa Wilson, and Matthew  Bell. 2001. Identifying collocations for recognizing David D. Lewis. 1998. Naive (Bayes) at forty: The  opinions. In Proc. of the ACL/EACL Workshop  independence assumption in information retrieval.  on Collocation.  In Proc. of the European Conference on Machine  Learning (ECML), pages 4 15. Invited talk.  Yorick Wilks and Mark Stevenson. 1998. The  grammar of sense: Using part-of-speech tags as a first  Andrew McCallum and Kamal Nigam. 1998. A  comstep in semantic disambiguation. Journal of  Natparison of event models for Naive Bayes text  classification. In Proc. of the AAAI-98 Workshop on  Learning for Text Categorization, pages 41 48. 