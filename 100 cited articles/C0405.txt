 Beyond NomBank:  A Study of Implicit Arguments for Nominal Predicates Matthew Gerber and Joyce Y. Chai  Department of Computer Science  Michigan State University  East Lansing, Michigan, USA  producer and arg1 is the produced entity. The  second sentence contains an instance of the nominal  Despite its substantial coverage,  Nompredicate shipping that is not associated with argu-Bank does not account for all  withinments in NomBank (Meyers, 2007).  From the sentences in Example 1, the reader can  sentential arguments altogether. These  arinfer that The two companies refers to the agents  guments, which we call implicit, are  im(arg0) of the shipping predicate. The reader can  portant to semantic processing, and their  also infer that market pulp, containerboard and  recovery could potentially benefit many  white paper refers to the shipped entities (arg1  NLP applications. We present a study of  of shipping).1 These extra-sentential arguments  implicit arguments for a select group of  have not been annotated for the shipping  predifrequent nominal predicates. We show that  cate and cannot be identified by a system that  reimplicit arguments are pervasive for these  stricts the argument search space to the sentence  predicates, adding 65% to the coverage of  containing the predicate. NomBank also ignores  NomBank. We demonstrate the  feasibilmany within-sentence arguments. This is shown  ity of recovering implicit arguments with  in the second sentence of Example 1, where The  goods can be interpreted as the arg1 of shipping.  sults and analyses provide a baseline for  These examples demonstrate the presence of  argufuture work on this emerging task.  ments that are not included in NomBank and  cannot easily be identified by systems trained on the 1  Introduction  resource. We refer to these arguments as implicit.  Verbal and nominal semantic role labeling (SRL)  This paper presents our study of implicit  arhave been studied independently of each other  guments for nominal predicates. We began our  study by annotating implicit arguments for a  seas well as jointly (Surdeanu et al., 2008; Haji et lect group of predicates. For these predicates, we al., 2009). These studies have demonstrated the  found that implicit arguments add 65% to the  exmaturity of SRL within an evaluation setting that  isting role coverage of NomBank.2 This increase  restricts the argument search space to the sentence has implications for tasks (e.g., question answer-containing the predicate of interest. However, as  ing, information extraction, and summarization)  shown by the following example from the Penn  that benefit from semantic analysis. Using our  anTreeBank (Marcus et al., 1993), this restriction ex-notations, we constructed a feature-based model  for automatic implicit argument identification that unifies standard verbal and nominal SRL. Our  re(1) [arg0 The two companies] [pred produce]  [arg1 market pulp, containerboard and white  gain in F1 over an informed baseline. Our  analypaper]. The goods could be manufactured  ses highlight strengths and weaknesses of the  apcloser to customers, saving [pred shipping]  proach, providing insights for future work on this costs.  The first sentence in Example 1 includes the  Prop1In PropBank and NomBank, the interpretation of each Bank (Kingsbury et al., 2002) analysis of the ver-role (e.g., arg0) is specific to a predicate sense.  bal predicate produce, where arg  Role coverage indicates the percentage of roles filled.  0 is the agentive  Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1583 1592, Uppsala, Sweden, 11-16 July 2010. c  2010 Association for Computational Linguistics  In the following section, we review related re-analysis of naturally occurring coreference  patsearch, which is historically sparse but recently  terns to aid implicit argument identification.  gaining traction. We present our annotation effort Most recently, Ruppenhofer et al. (2009) con-in Section 3, and follow with our implicit  arguducted SemEval Task 10, Linking Events and  ment identification model in Section 4. In Section Their Participants in Discourse , which evaluated  5, we describe the evaluation setting and present  implicit argument identification systems over a  our experimental results. We analyze these results common test set. The task organizers annotated  in Section 6 and conclude in Section 7.  ing in data that cover many distinct predicates,  Related work  each associated with a small number of annotated  Palmer et al. (1986) made one of the earliest  atinstances. In contrast, our study focused on a  setempts to automatically recover extra-sentential  lect group of nominal predicates, each associated  arguments. Their approach used a fine-grained  dowith a large number of annotated instances.  main model to assess the compatibility of  candiData annotation and analysis  date arguments and the slots needing to be filled.  A phenomenon similar to the implicit  argument has been studied in the context of Japanese  Implicit arguments have not been annotated within  anaphora resolution, where a missing case-marked  the Penn TreeBank, which is the textual and  synconstituent is viewed as a zero-anaphoric  exprestactic basis for NomBank.  Thus, to facilitate  sion whose antecedent is treated as the implicit ar-our study, we annotated implicit arguments for  gument of the predicate of interest. This behavior instances of nominal predicates within the stan-has been annotated manually by Iida et al. (2007), dard training, development, and testing sections of and researchers have applied standard SRL tech-the TreeBank. We limited our attention to  nomniques to this corpus, resulting in systems that  inal predicates with unambiguous role sets (i.e.,  are able to identify missing case-marked  expressenses) that are derived from verbal role sets. We sions in the surrounding discourse (Imamura et  then ranked this set of predicates using two pieces al., 2009). Sasano et al. (2004) conducted sim-of information: (1) the average difference between ilar work with Japanese indirect anaphora. The  the number of roles expressed in nominal form (in  authors used automatically derived nominal case  frames to identify antecedents. However, as noted  (2) the frequency of the nominal form in the  corby Iida et al., grammatical cases do not stand in  pus. We assumed that the former gives an  indicaa one-to-one relationship with semantic roles in  tion as to how many implicit roles an instance of  Japanese (the same is true for English).  the nominal predicate might have. The product of  Fillmore and Baker (2001) provided a detailed  (1) and (2) thus indicates the potential prevalence case study of implicit arguments (termed null in-of implicit arguments for a predicate. To focus our stantiations in that work), but did not provide con-study, we ranked the predicates in NomBank  accrete methods to account for them automatically.  cording to this product and selected the top ten,  Previously, we demonstrated the importance of  filshown in Table 1.  tering out nominal predicates that take no local ar-We annotated implicit arguments  document-byguments (Gerber et al., 2009); however, this work  document, selecting all singular and plural nouns  did not address the identification of implicit  arderived from the predicates in Table 1. For each  missing argument position of each predicate  inproaches to implicit argument identification based stance, we inspected the local discourse for a suit-on observed coreference patterns; however, the  auable implicit argument. We limited our attention to thors did not implement and evaluate such meth-the current sentence as well as all preceding  senWe draw insights from all three of these  tences in the document, annotating all mentions of studies. We show that the identification of im-an implicit argument within this window.  plicit arguments for nominal predicates leads to  In the remainder of this paper, we will use iargn  fuller semantic interpretations when compared to  to refer to an implicit argument position n. We  traditional SRL methods. Furthermore, motivated  will use argn to refer to an argument provided by  by Burchardt et al., our model uses a quantitative PropBank or NomBank. We will use p to mark  Pre-annotation  Post-annotation  Noun  investor  investment  Table 1: Predicates targeted for annotation. The second column gives the number of predicate instances annotated. Pre-annotation numbers only include NomBank annotations, whereas Post-annotation numbers include NomBank and implicit argument annotations. Role coverage indicates the percentage of roles filled. Role average indicates how many roles, on average, are filled for an instance of a predicate's noun form or verb form within the TreeBank. Verbal role averages were computed using PropBank.  predicate instances. Below, we give an example  of roles in the predicate's lexicon entry. Role cov-annotation for an instance of the investment predi-erage for the marked predicate in Example 2 is  0/3 for NomBank-only arguments and 3/3 when  the annotated implicit arguments are also  consid(2) [iarg0 Participants] will be able to transfer  ered. Returning to Table 1, the third column gives  [iarg1 money] to [iarg2 other investment  role coverage percentages for NomBank-only  arfunds]. The [p investment] choices are  guments. The sixth column gives role coverage  limited to [iarg2 a stock fund and a  percentages when both NomBank arguments and  the annotated implicit arguments are considered.  NomBank does not associate this instance of  inOverall, the addition of implicit arguments created vestment with any arguments; however, we were  a 65% relative (18-point absolute) gain in role covable to identify the investor (iarg  erage across the 1,253 predicate instances that we 0), the thing invested (iarg  annotated.  1), and two mentions of the thing  inThe predicates in Table 1 are typically  associOur data set was also independently annotated  ated with fewer arguments on average than their  by an undergraduate linguistics student. For each  corresponding verbal predicates. When  considmissing argument position, the student was asked  ering NomBank-only arguments, this difference  to identify the closest acceptable implicit  argu(compare columns four and five) varies from zero  ment within the current and preceding sentences.  (for price) to a factor of five (for fund). When im-The argument position was left unfilled if no  acplicit arguments are included in the comparison,  ceptable constituent could be found. For a  missthese differences are reduced and many nominal  ing argument position, the student's annotation  predicates express approximately the same  numagreed with our own if both identified the same  ber of arguments on average as their verbal  counconstituent or both left the position unfilled. Anal-terparts (compare the fifth and seventh columns).  ysis indicated an agreement of 67% using Cohen's  In addition to role coverage and average count,  kappa coefficient (Cohen, 1960).  we examined the location of implicit arguments.  Figure 1 shows that approximately 56% of the  imAnnotation analysis  plicit arguments in our data can be resolved within Role coverage for a predicate instance is equal to the sentence containing the predicate. The remain-the number of filled roles divided by the number  ing implicit arguments require up to forty-six sen-1585  from the surrounding discourse that constituent c  (referring to Mexico) is the thing being invested in tsn  (the iarg  When determining whether c is the  2 of investment, one can draw evidence from  other mentions in c's coreference chain. Example  it a  s  3 states that Mexico needs investment. Example  4 states that Mexico regulates investment. These  propositions, which can be derived via traditional 0.4  SRL analyses, should increase our confidence that  c is the iarg2 of investment in Example 5.  Thus, the unit of classification for a  candidate constituent c is the three-tuple hp, iarg  Figure 1: Location of implicit arguments. For  where c0 is a coreference chain comprising c and  missing argument positions with an implicit filler, its coreferent constituents.3 We defined a binary  the y-axis indicates the likelihood of the filler be-classification function P r(+| hp, iarg  ing found at least once in the previous x sentences.  n, c0i) that  predicts the probability that the entity referred to by c fills the missing argument position iargn of  tences for resolution; however, a vast majority of predicate instance p. In the remainder of this pa-these can be resolved within the previous few  senper, we will refer to c as the primary filler, dif-tences. Section 6 discusses implications of this  ferentiating it from other mentions in the corefer-skewed distribution.  ence chain c0. In the following section, we present the feature set used to represent each three-tuple 4  within the classification function.  Model formulation  In our study, we assumed that each sentence in a  Starting with a wide range of features, we  perdocument had been analyzed for PropBank and  formed floating forward feature selection (Pudil  Bank includes a lexicon listing the possible  arprising implicit argument annotations from section gument positions for a predicate, allowing us to  24 of the Penn TreeBank. As part of the feature  identify missing argument positions with a simple  selection process, we conducted a grid search for  lookup. Given a nominal predicate instance p with  the best per-class cost within LibLinear's logistic a missing argument position iargn, the task is to  regression solver (Fan et al., 2008). This was done search the surrounding discourse for a constituent to reduce the negative effects of data imbalance,  c that fills iargn. Our model conducts this search which is severe even when selecting candidates  over all constituents annotated by either PropBank from the current and previous few sentences. Ta-or NomBank with non-adjunct labels.  ble 2 shows the selected features, which are quite A candidate constituent c will often form a  different from those used in our previous work to  coreference chain with other constituents in the  identify traditional semantic arguments (Gerber et discourse. Consider the following abridged sen-al., 2009).4 Below, we give further explanations  tences, which are adjacent in their Penn TreeBank  for some of the features.  Feature 1 models the semantic role relationship  (3) [Mexico] desperately needs investment.  between each mention in c0 and the missing  argument position iargn. To reduce data sparsity, this (4) Conservative Japanese investors are put off  feature generalizes predicates and argument  posiby [Mexico's] investment regulations.  tions to their VerbNet (Kipper, 2005) classes and  (5) Japan is the fourth largest investor in  3We used OpenNLP for coreference identification:  [c Mexico], with 5% of the total  4We have omitted many of the lowest-ranked features.  Descriptions of these features can be obtained by contacting NomBank does not associate the labeled instance  the authors.  of investment with any arguments, but it is clear  For every f , the VerbNet class/role of pf /argf concatenated with the class/role of p/iargn.  Average pointwise mutual information between hp, iargni and any hpf , argf i.  Percentage of all f that are definite noun phrases.  Minimum absolute sentence distance from any f to p.  Minimum pointwise mutual information between hp, iargni and any hpf , argf i.  Frequency of the nominal form of p within the document that contains it.  Nominal form of p concatenated with iargn.  Nominal form of p concatenated with the sorted integer argument indexes from all argn of p.  Number of mentions in c0.  Head word of p's right sibling node.  For every f , the synset (Fellbaum, 1998) for the head of f concatenated with p and iargn.  Part of speech of the head of p's parent node.  Average absolute sentence distance from any f to p.  Discourse relation whose two discourse units cover c (the primary filler) and p.  Number of left siblings of p.  Whether p is the head of its parent node.  Number of right siblings of p.  Table 2: Features for determining whether c fills iargn of predicate p. For each mention f (denoting a f iller) in the coreference chain c0, we define pf and argf to be the predicate and argument position of f .  Features are sorted in descending order of feature selection gain. Unless otherwise noted, all predicates were normalized to their verbal form and all argument positions (e.g., argn and iargn) were interpreted as labels instead of word content. Features marked with an asterisk are explained in Section 4.2.  semantic roles using SemLink.5 For explanation  the caption for Table 2):  purposes, consider again Example 1, where we are  trying to fill the iarg  0 of shipping. Let c0 contain  a single mention, The two companies, which is the  arg0 of produce. As described in Table 2,  feature 1 is instantiated with a value of  create.agentsend.agent, where create and send are the VerbNet  classes that contain produce and ship, respectively.  To compute Equation 6, we first labeled a subset of In the conversion to LibLinear's instance repre-the Gigaword corpus (Graff, 2003) using the  versentation, this instantiation is converted into a sin-bal SRL system of Punyakanok et al. (2008) and  gle binary feature create.agent-send.agent whose  the nominal SRL system of Gerber et al. (2009).  value is one. Features 1 and 11 are instantiated  We then identified coreferent pairs of arguments  once for each mention in c0, allowing the model  using OpenNLP. Suppose the resulting data has  to consider information from multiple mentions of  N coreferential pairs of argument positions. Also  the same entity.  suppose that M of these pairs comprise hp, argni  Features 2 and 5 are inspired by the work  and hpf , argf i. The numerator in Equation 6 is  of Chambers and Jurafsky (2008), who  invesdefined as M . Each term in the denominator is  tigated unsupervised learning of narrative event  obtained similarly, except that M is computed as  sequences using pointwise mutual information  the total number of coreference pairs  compris(PMI) between syntactic positions. We used a  siming an argument position (e.g., hp, argni) and any ilar PMI score, but defined it with respect to se-other argument position. Like Chambers and  Jumantic arguments instead of syntactic  dependenrafsky, we also used the discounting method  sugcies. Thus, the values for features 2 and 5 are  gested by Pantel and Ravichandran (2004) for  lowcomputed as follows (the notation is explained in  frequency observations. The PMI score is  somewhat noisy due to imperfect output, but it provides 5http://verbs.colorado.edu/semlink  information that is useful for classification.  Feature 10 does not depend on c0 and is specific annotated as filling the missing argument position.  to each predicate. Consider the following  examTo factor out errors from standard SRL analyses,  the model used gold-standard argument labels  provided by PropBank and NomBank. As shown in  (7) Statistics Canada reported that its [arg1  Figure 1 (Section 3.2), implicit arguments tend to industrial-product] [p price] index dropped  be located in close proximity to the predicate. We 2% in September.  found that using all candidate constituents c within The [p price] index collocation is rarely associ-the current and previous two sentences worked  ated with an arg  0 in NomBank or with an iarg0 in  We compared our supervised model with the  the seller). Feature 10 accounts for this type of be-simple baseline heuristic defined below:6  havior by encoding the syntactic head of p's right Fill iargn for predicate instance p  sibling. The value of feature 10 for Example 7 is  with the nearest constituent in the  twoprice:index. Contrast this with the following:  sentence candidate window that fills  argn for a different instance of p, where  0 The company] is trying to prevent  further [p price] drops.  their verbal forms.  The value of feature 10 for Example 8 is  price:drop. This feature captures an important dis-The normalization allows an existing arg0 for the  tinction between the two uses of price: the  forverb invested to fill an iarg0 for the noun  inWe also evaluated an oracle model  0, whereas the latter often  does. Features 12 and 15-17 account for  predicatethat made gold-standard predictions for candidates specific behaviors in a similar manner.  within the two-sentence prediction window.  Feature 14 identifies the discourse relation (if  We evaluated these models using the  methodolany) that holds between the candidate constituent  ogy proposed by Ruppenhofer et al. (2009). For  c and the filled predicate p. Consider the following each missing argument position of a predicate in-example:  stance, the models were required to either (1) identify a single constituent that fills the missing  argu(9) [iarg0 SFE Technologies] reported a net loss  ment position or (2) make no prediction and leave  of $889,000 on sales of $23.4 million.  the missing argument position unfilled. We scored  predictions using the Dice coefficient, which is  de(10) That compared with an operating [p loss] of  fined as follows:  [arg1 $1.9 million] on sales of $27.4 million  in the year-earlier period.  2 |P redicted T T rue|  |P redicted| + |T rue|  In this case, a comparison discourse relation (sig-naled by the underlined text) holds between the  P redicted is the set of tokens subsumed by the  first and sentence sentence. The coherence  proconstituent predicted by the model as filling a  vided by this relation encourages an inference that missing argument position.  T rue is the set of  identifies the marked iarg0 (the loser).  Throughtokens from a single annotated constituent that  out our study, we used gold-standard discourse  refills the missing argument position. The model's  lations provided by the Penn Discourse TreeBank  prediction receives a score equal to the  maximum Dice overlap across any one of the annotated  fillers. Precision is equal to the summed  prediction scores divided by the number of argument  poWe trained the feature-based logistic regression  sitions filled by the model. Recall is equal to the model over 816 annotated predicate instances as-summed prediction scores divided by the number  sociated with 650 implicitly filled argument  posiof argument positions filled in our annotated data.  tions (not all predicate instances had implicit ar-Predictions not covering the head of a true filler guments). During training, a candidate three-tuple were assigned a score of zero.  hp, iargn, c0i was given a positive label if the can-6This heuristic outperformed a more complicated heuris-didate implicit argument c (the primary filler) was tic that relied on the PMI score described in section 4.2.  investor  investment  Table 3: Evaluation results. The second column gives the number of predicate instances evaluated.  The third column gives the number of ground-truth implicitly filled argument positions for the predicate instances (not all instances had implicit arguments). P , R, and F1 indicate precision, recall, and F-measure ( = 1), respectively. p-values denote the bootstrapped significance of the difference in F1  between the baseline and discriminative models. Oracle precision (not shown) is 100% for all predicates.  stances associated with 246 implicitly filled  argument positions. Table 3 presents the results.  Predicates with the highest number of implicit  arguments sale and price showed F1 increases  Use 1,2,5 only  of 8 points and 18.8 points, respectively.  Overall, the discriminative model increased F1  performance 15.8 points (59.6%) over the baseline.  We measured human performance on this task  by running our undergraduate assistant's  annotaTable 4: Feature ablation results. The first column tions against the evaluation data. Our assistant  lists the feature configurations. All changes are  achieved an overall F  percentages relative to the full-featured discrimi-1 score of 58.4% using the  same candidate window as the baseline and  disnative model. p-values for the changes are  indicriminative models. The difference in F  cated in parentheses.  the discriminative and human results had an  exact p-value of less than 0.001. All significance  of the missing argument position (first  configuratesting was performed using a two-tailed bootstrap tion). The second configuration tested the effect of method similar to the one described by Efron and  using only the SRL-based features. This also  resulted in significant performance losses,  suggesting that the other features contribute useful information. Lastly, we tested the effect of removing  Feature ablation  discourse relations (feature 14), which are likely We conducted an ablation study to measure the  to be difficult to extract reliably in a practical set-contribution of specific feature sets.  ting. As shown, this feature did not have a statis-presents the ablation configurations and results.  tically significant effect on performance and could For each configuration, we retrained and retested  be excluded in future applications of the model.  the discriminative model using the features  deUnclassified true implicit arguments  scribed. As shown, we observed significant losses  when excluding features that relate the  semanOf all the errors made by the system,  approxitic roles of mentions in c0 to the semantic role  mately 19% were caused by the system's failure to  generate a candidate constituent c that was a cor-This is precisely what happened for the fund  predrect implicit argument. Without such a candidate,  icate, where the model incorrectly identified many the system stood no chance of identifying a cor-implicit arguments for stock [p fund] and  murect implicit argument. Two factors contributed to tual [p fund] . The left context of fund should help this type of error, the first being our assumption the model avoid this type of error; however, our  that implicit arguments are also core (i.e., argn) feature selection process did not identify any over-arguments to traditional SRL structures.  Approxiall gains from including this information.  mately 8% of the overall error was due to a failure of this assumption. In many cases, the true im-6.4  Improvements versus the baseline  plicit argument filled a non-core (i.e., adjunct) role The baseline heuristic covers the simple case  within PropBank or NomBank.  where identical predicates share arguments in the  More frequently, however, true implicit  argusame position. Thus, it is interesting to examine  ments were missed because the candidate window  cases where the baseline heuristic failed but the  was too narrow. This accounts for 12% of the  discriminative model succeeded. Consider the  following sentence:  umn in Table 3) indicates the nominals that  suf(12) Mr. Rogers recommends that [p investors]  fered most from windowing errors.  ple, the sale predicate was associated with the  highest number of true implicit arguments, but  Neither NomBank nor the baseline heuristic  assoonly 80% of those could be resolved within the  ciate the marked predicate in Example 12 with any  two-sentence candidate window. Empirically, we  arguments; however, the feature-based model was  found that extending the candidate window  uniable to correctly identify the marked iarg2 as the formly for all predicates did not increase perfor-entity being invested in. This inference captured a mance on the development data. The oracle re-tendency of investors to sell the things they have sults suggest that predicate-specific window set-invested in.  tings might offer some advantage.  We conclude our discussion with an example of  The investment and fund predicates  In Section 4.2, we discussed the price predicate,  (13) [iarg0 Olivetti] has denied that it violated  which frequently occurs in the [p price] index  the rules, asserting that the shipments were  We observed that this collocation  properly licensed. However, the legality of  is rarely associated with either an overt arg0 or  these [p sales] is still an open question.  an implicit iarg0. Similar observations can be  As shown in Example 13, the system was able to  made for the investment and fund predicates.  Alcorrectly identify Olivetti as the agent in the sell-though these two predicates are frequent, they are ing event of the second sentence. This inference  rarely associated with implicit arguments:  investinvolved two key steps. First, the system identified ment takes only eight implicit arguments across its coreferent mentions of Olivetti that participated in 21 instances, and fund takes only six implicit ar-exporting and supplying events (not shown).  Secguments across its 43 instances. This behavior is  ond, the system identified a tendency for exporters due in large part to collocations such as [p in-and suppliers to also be sellers. Using this knowl-vestment] banker , stock [p fund] , and mutual  edge, the system extracted information that could  [p fund] , which use predicate senses that are not not be extracted by the baseline heuristic or a tra-eventive. Such collocations also violate our  asditional SRL system.  sumption that differences between the PropBank  and NomBank argument structure for a predicate  Conclusions and future work  are indicative of implicit arguments (see Section  3.1 for this assumption).  Current SRL approaches limit the search for  arDespite their lack of implicit arguments, it is  guments to the sentence containing the predicate  important to account for predicates such as  inof interest. Many systems take this assumption  vestment and fund because incorrect prediction of  a step further and restrict the search to the predi-implicit arguments for them can lower precision.  cate's local syntactic environment; however,  predicates and the sentences that contain them rarely  exist in isolation. As shown throughout this paper, References  they are usually embedded in a coherent and  semantically rich discourse that must be taken into  Pinkal. 2005. Building text meaning  representaaccount. We have presented a preliminary study  tions from contextually related frames a case study.  of implicit arguments for nominal predicates that  In Proceedings of the Sixth International Workshop on Computational Semantics.  focused specifically on this problem.  Our contribution is three-fold. First, we have  Xavier Carreras and Llu s M rquez. 2005. Introduc-created gold-standard implicit argument  annotation to the CoNLL-2005 shared task: Semantic role  tions for a small set of pervasive nominal  predicates.7 Our analysis shows that these annotations  Nathanael Chambers and Dan Jurafsky. 2008.  Unsuadd 65% to the role coverage of NomBank.  Secpervised learning of narrative event chains. In Pro-ond, we have demonstrated the feasibility of  receedings of the Association for Computational  Linguistics, pages 789 797, Columbus, Ohio, June.  Ascovering implicit arguments for many of the  predsociation for Computational Linguistics.  icates, thus establishing a baseline for future work on this emerging task. Third, our study suggests  Jacob Cohen.  A coefficient of agreement  for nominal scales. Educational and Psychological  a few ways in which this research can be moved  forward. As shown in Section 6, many errors were  caused by the absence of true implicit arguments  Bradley Efron and Robert J. Tibshirani. 1993. An In-within the set of candidate constituents. More  introduction to the Bootstrap. Chapman & Hall, New York.  telligent windowing strategies in addition to  alternate candidate sources might offer some  improvement. Although we consistently observed  Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:  A Library for Large Linear Classification. Journal development gains from using automatic coref-of Machine Learning Research, 9:1871 1874.  erence resolution, this process creates errors that need to be studied more closely. It will also be  Christiane Fellbaum. 1998. WordNet: An Electronic  important to study implicit argument patterns of  Lexical Database (Language, Speech, and  Communication). The MIT Press, May.  non-verbal predicates such as the partitive percent.  These predicates are among the most frequent in  C.J. Fillmore and C.F. Baker. 2001. Frame semantics the TreeBank and are likely to require approaches  for text understanding. In Proceedings of WordNet  and Other Lexical Resources Workshop, NAACL.  that differ from the ones we pursued.  Finally, any extension of this work is likely to  Matthew Gerber, Joyce Y. Chai, and Adam Meyers.  encounter a significant knowledge acquisition  bot2009. The role of implicit argumentation in nominal tleneck. Implicit argument annotation is difficult SRL. In Proceedings of the North American Chap-ter of the Association for Computational Linguistics, because it requires both argument and coreference  identification (the data produced by Ruppenhofer  et al. (2009) is similar). Thus, it might be produc-David Graff. 2003. English Gigaword.  Linguistic  tive to focus future work on (1) the extraction of relevant knowledge from existing resources (e.g.,  our use of coreference patterns from Gigaword) or  son, Daisuke Kawahara, Maria Ant nia Mart , Llu s (2) semi-supervised learning of implicit argument  Pad , Jan t p nek, Pavel Stra k, Mihai Surdeanu, models from a combination of labeled and unla-Nianwen Xue, and Yi Zhang. 2009. The  CoNLL2009 shared task: Syntactic and semantic  dependencies in multiple languages. In Proceedings of the  Acknowledgments  Thirteenth Conference on Computational Natural  Language Learning (CoNLL 2009): Shared Task,  We would like to thank the anonymous  reviewpages 1 18, Boulder, Colorado, June. Association  ers for their helpful questions and comments. We  for Computational Linguistics.  would also like to thank Malcolm Doering for his  annotation effort. This work was supported in part Matsumoto. 2007. Annotating a Japanese text cor-by NSF grants IIS-0347548 and IIS-0840538.  pus with predicate-argument and coreference  relations. In Proceedings of the Linguistic Annotation 7Our annotation data can be freely downloaded at  Workshop in ACL-2007, page 132139.  hashi. 2004. Automatic construction of nominal  argument structure analysis with zero-anaphora res-case frames and its application to indirect anaphora olution. In Proceedings of the ACL-IJCNLP 2009  resolution. In Proceedings of Coling 2004, pages  Conference Short Papers, pages 85 88, Suntec,  Singapore, August. Association for Computational  LinMihai Surdeanu, Richard Johansson, Adam Meyers,  P. Kingsbury, M. Palmer, and M. Marcus.  The  Adding semantic annotation to the Penn TreeBank.  CoNLL 2008 shared task on joint parsing of  synIn Proceedings of the Human Language Technology  tactic and semantic dependencies. In CoNLL 2008:  Conference (HLT'02).  Proceedings of the Twelfth Conference on  Computational Natural Language Learning, pages 159 177,  Karin Kipper. 2005. VerbNet: A broad-coverage, com-Manchester, England, August. Coling 2008  Orgaprehensive verb lexicon. Ph.D. thesis, Department  of Computer and Information Science University of  Mitchell Marcus, Beatrice Santorini, and Mary Ann  Marcinkiewicz. 1993. Building a large annotated  corpus of English: the Penn TreeBank.  Computational Linguistics, 19:313 330.  Annotation guidelines for  NomBank noun argument structure for PropBank.  Technical report, New York University.  Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-man, Lynette Hirschman, Marcia Linebarger, and  John Dowding. 1986. Recovering implicit  information. In Proceedings of the 24th annual meeting  on Association for Computational Linguistics, pages 10 19, Morristown, NJ, USA. Association for Computational Linguistics.  Automatically labeling semantic classes.  itors, HLT-NAACL 2004: Main Proceedings, pages  321 328, Boston, Massachusetts, USA, May 2  May 7. Association for Computational Linguistics.  Linguistic Data Consortium, February.  P. Pudil, J. Novovicova, and J. Kittler. 1994. Floating search methods in feature selection. Pattern Recog-nition Letters, 15:1119 1125.  The importance of syntactic parsing and  inference in semantic role labeling. Comput. Linguist., 34(2):257 287.  Josef  Morante, Collin Baker, and Martha Palmer. 2009.  Semeval-2010 task 10: Linking events and their  In Proceedings of  the Workshop on Semantic Evaluations:  Achievements and Future Directions (SEW-2009),  tion for Computational Linguistics. 