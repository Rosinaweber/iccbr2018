 Joint Parsing and Named Entity Recognition  Jenny Rose Finkel and Christopher D. Manning  Computer Science Department  Stanford University  Stanford, CA 94305  fortunately, it is still common practice to cobble  together independent systems for the various types of  For many language technology applications,  annotation, and there is no guarantee that their  outsuch as question answering, the overall  sysputs will be consistent.  This paper begins to address this problem by  the data (such as a named entity recognizer, a  coreference system, and a parser). This  easbuilding a joint model of both parsing and named  ily results in inconsistent annotations, which  entity recognition. Vapnik has observed (Vapnik,  are harmful to the performance of the  aggre1998; Ng and Jordan, 2002) that one should solve  gate system. We begin to address this  probthe problem directly and never solve a more  genlem with a joint model of parsing and named  eral problem as an intermediate step, implying that  entity recognition, based on a discriminative  building a joint model of two phenomena is more  feature-based constituency parser. Our model  likely to harm performance on the individual tasks  produces a consistent output, where the named  than to help it. Indeed, it has proven very  diffientity spans do not conflict with the phrasal  cult to build a joint model of parsing and  semanspans of the parse tree. The joint  representic role labeling, either with PCFG trees (Sutton and  tation also allows the information from each  McCallum, 2005) or with dependency trees. The  type of annotation to improve performance  CoNLL 2008 shared task (Surdeanu et al., 2008)  on the other, and, in experiments with the  OntoNotes corpus, we found improvements of  was intended to be about joint dependency parsing  up to 1.36% absolute F1 for parsing, and up to  and semantic role labeling, but the top performing  9.0% F1 for named entity recognition.  systems decoupled the tasks and outperformed the  systems which attempted to learn them jointly.  Despite these earlier results, we found that combining  Introduction  parsing and named entity recognition modestly  imIn order to build high quality systems for complex  proved performance on both tasks. Our joint model  NLP tasks, such as question answering and textual  produces an output which has consistent parse  strucentailment, it is essential to first have high quality  ture and named entity spans, and does a better job at  systems for lower level tasks. A good (deep  analyboth tasks than separate models with the same  feasis) question answering system requires the data to  first be annotated with several types of information:  We first present the joint, discriminative model  that we use, which is a feature-based CRF-CFG  tion, etc. However, having high performing,  lowparser operating over tree structures augmented with  level systems is not enough; the assertions of the  NER information. We then discuss in detail how  various levels of annotation must be consistent with  we make use of the recently developed OntoNotes  one another. When a named entity span has crossing  corpus both for training and testing the model, and  brackets with the spans in the parse tree it is usually  then finally present the performance of the model  impossible to effectively combine these pieces of  inand some discussion of what causes its superior  performation, and system performance suffers. But,  unformance, and how the model relates to prior work.  Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 326 334, Boulder, Colorado, June 2009. c  2009 Association for Computational Linguistics  NamedEntity-GPE*  the  [District  of  the  District  of  Figure 1: An example of a (sub)tree which is modified for input to our learning algorithm. Starting from the normalized tree discussed in section 4.1, a new NamedEntity node is added, so that the named entity corresponds to a single phrasal node. That node, and its descendents, have their labels augmented with the type of named entity. The * on the NamedEntity node indicates that it is the root of the named entity.  The Joint Model  phrasal node in the entire tree. We then augment the  labels of the phrasal node and its descendents with  When constructing a joint model of parsing and  the type of named entity. We also distinguish  benamed entity recognition, it makes sense to think  tween the root node of an entity, and the descendent  about how the two distinct levels of annotation may  nodes. See Figure 1 for an illustration. This  reprehelp one another. Ideally, a named entity should  corsentation has several benefits, outlined below.  respond to a phrase in the constituency tree.  However, parse trees will occasionally lack some explicit  Nested Entities  structure, such as with right branching NPs. In these  The OntoNotes data does not contain any nested  encases, a named entity may correspond to a  contigutities. Consider the named entity portions of the  ous set of children within a subtree of the entire  rules seen in the training data. These will look, for  parse. The one thing that should never happen is for  instance, like none none person, and organization a named entity span to have crossing brackets with  organization organization. Because we only  alany spans in the parse tree.  low named entity derivations which we have seen in  For named entities, the joint model should help  the data, nested entities are impossible. However,  with boundaries. The internal structure of the named  there is clear benefit in a representation allowing  entity, and the structural context in which it  apnested entities. For example, it would be beneficial  pears, can also help with determining the type of  to recognize that the United States Supreme Court is  entity. Finding the best parse for a sentence can be  a an organization, but that it also contains a nested  helped by the named entity information in similar  GPE.1 Fortunately, if we encounter data which has  ways. Because named entities should correspond  been annotated with nested entities, this  representato phrases, information about them should lead to  tion will be able to handle them in a natural way.  better bracketing. Also, knowing that a phrase is a  In the given example, we would have a derivation  named entity, and the type of entity, may help in  getwhich includes organization GPE organization.  ting the structural context, and internal structure, of  This information will be helpful for correctly  lathat entity correct.  beling nested entities such as New Jersey Supreme  Joint Representation  Court, because the model will learn how nested  enAfter modifying the OntoNotes dataset to ensure  consistency, which we will discuss in Section 4, we  Feature Representation for Named  augment the parse tree with named entity  information, for input to our learning algorithm. In the cases  Currently, named entity recognizers are usually  conwhere a named entity corresponds to multiple  constructed using sequence models, with linear chain  tiguous children of a subtree, we add a new  NamedEntity node, which is the new parent to those  chil1As far as we know, GENIA (Kim et al., 2003) is the only  dren. Now, all named entities correspond to a single  corpus currently annotated with nested entities.  conditional random fields (CRFs) being the most  rules which only occur in the training data  augcommon. While it is possible for CRFs to have links  mented with named entity information, and because  that are longer distance than just between adjacent  of rules which only occur without the named entity  words, most of the benefit is from local features,  information. To combat this problem, we added  exover the words and labels themselves, and from  featra rules, unseen in the training data.  tures over adjacent pairs of words and labels. Our  joint representation allows us to port both types of  Augmenting the Grammar  features from such a named entity recognizer. The  For every rule encountered in the training data which  local features can be computed at the same time the  has been augmented with named entity information,  features over parts of speech are computed. These  we add extra copies of that rule to the grammar. We  are the leaves of the tree, when only the named  enadd one copy with all of the named entity  informatity for the current word is known.2 The pairwise  tion stripped away, and another copy for each other  features, over adjacent labels, are computed at the  entity type, where the named entity augmentation  same time as features over binary rules.  Binarizahas been changed to the other entity type.  tion of the tree is necessary for efficient  computaThese additions help, but they are not sufficient.  tion, so the trees consist solely of unary and  biMost entities correspond to noun phrases, so we took  nary productions. Because of this, for all pairs of  all rules which had an NP as a child, and made  adjacent words within an entity, there will be a  bicopies of that rule where the NP was augmented  nary rule applied where one word will be under the  with each possible entity type. These grammar  adleft child and the other word will be under the right  child. Therefore, we compute features over adjacent  Augmenting the Lexicon  words/labels when computing the features for the  binary rule which joins them.  The lexicon is augmented in a similar manner to  the rules. For every part of speech tag seen with a  Learning the Joint Model  named entity annotation, we also add that tag with  no named entity information, and a version which  We construct our joint model as an extension to the  has been augmented with each type of named entity.  discriminatively trained, feature-rich, conditional  It would be computationally infeasible to allow  random field-based, CRF-CFG parser of (Finkel and  any word to have any part of speech tag. We  thereManning, 2008). Their parser is similar to a  chartfore limit the allowed part of speech tags for  combased PCFG parser, except that instead of putting  mon words based on the tags they have been  obprobabilities over rules, it puts clique potentials over served with in the training data. We also augment  local subtrees. These unnormalized potentials know  each word with a distributional similarity tag, which  what span (and split) the rule is over, and arbitrary  we discuss in greater depth in Section 3, and  alfeatures can be defined over the local subtree, the  low tags seen with other words which belong to the  span/split and the words of the sentence. The  insidesame distributional similarity cluster. When  decidoutside algorithm is run over the clique potentials to  ing what tags are allowed for each word, we initially  produce the partial derivatives and normalizing  conignore named entity information. Once we  deterstant which are necessary for optimizing the log  likemine what base tags are allowed for a word, we also  allow that tag, augmented with any type of named  entity, if the augmented tag is present in the lexicon.  Grammar Smoothing  Because of the addition of named entity  annotaFeatures  tions to grammar rules, if we use the grammar  We defined features over both the parse rules and the  as read off the treebank, we will encounter  probnamed entities. Most of our features are over one or  lems with sparseness which severely degrade  perthe other aspects of the structure, but not both.  formance. This degradation occurs because of CFG  Both the named entity and parsing features utilize  the words of the sentence, as well as orthographic  Note that features can include information about other  words, because the entire sentence is observed. The features and distributional similarity information. For each  cannot include information about the labels of those words.  word we computed a word shape which encoded  information about capitalization, length, and inclu-not surprising that we found places where the data  sion of numbers and other non-alphabetic  characwas inconsistently annotated, namely with crossing  ters. For the distributional similarity information,  brackets between named entity and tree annotations.  we had to first train a distributional similarity model.  In the places where we found inconsistent  annoWe trained the model described in (Clark, 2000),  tation it was rarely the case that the different  levwith code downloaded from his website, on several  els of annotation were inherently inconsistent, but  hundred million words from the British national  corrather inconsistency results from somewhat arbitrary  pus, and the English Gigaword corpus. The model  choices made by the annotators. For example, when  we trained had 200 clusters, and we used it to assign  the last word in a sentence ends with a period, such  each word in the training and test data to one of the  as Corp. , one period functions both to mark the  abclusters.  breviation and the end of the sentence. The  convenFor the named entity features, we used a fairly  tion of the Penn Treebank is to separate the final  pestandard feature set, similar to those described in  riod and treat it as the end of sentence marker, but  (Finkel et al., 2005). For parse features, we used the  when the final word is also part of an entity, that  exact same features as described in (Finkel and  Manfinal period was frequently included in the named  ning, 2008). When computing those features, we  reentity annotation, resulting in the sentence  terminatmoved all of the named entity information from the  ing period being part of the entity, and the entity not  rules, so that these features were just over the parse  corresponding to a single phrase. See Figure 2 for an  information and not at all over the named entity  inillustration from the data. In this case, we removed  formation.  the terminating period from the entity, to produce a  Lastly, we have the joint features. We included as  features each augmented rule and each augmented  Overall, we found that 656 entities, out of 55,665  label. This allowed the model to learn that certain  total, could not be aligned to a phrase, or multiple  types of phrasal nodes, such as NP s are more likely  contiguous children of a node. We identified and  to be named entities, and that certain entities were  corrected the following sources of inconsistencies:  more likely to occur in certain contexts and have  particular types of internal structure.  Periods and abbreviations. This is the problem  described above with the Corp. example. We  corrected it by removing the sentence  terminatFor our experiments we used the LDC2008T04  ing final period from the entity annotation.  OntoNotes Release 2.0 corpus (Hovy et al., 2006).  Determiners and PPs. Noun phrases composed of  The OntoNotes project leaders describe it as a  a nested noun phrase and a prepositional phrase  large, multilingual richly-annotated corpus  conwere problematic when they also consisted of a  structed at 90% internanotator agreement. The  cordeterminer followed by an entity. We dealt with  pus has been annotated with multiple levels of  annothis by flattening the nested NP, as illustrated in  tation, including constituency trees, predicate  strucFigure 3. As we discussed in Section 2.1, this  ture, word senses, coreference, and named entities.  tree will then be augmented with an additional  For this work, we focus on the parse trees and named  node for the entity (see Figure 1).  entities. The corpus has English and Chinese  portions, and we used only the English portion, which  Adjectives and PPs. This problem is similar to the  itself has been split into seven sections: ABC, CNN,  previous problem, with the difference being  MNB, NBC, PRI, VOA, and WSJ. These sections  that there are also adjectives preceding the  enrepresent a mix of speech and newswire data.  tity. The solution is also similar to the solution  to the previous problem. We moved the  adjectives from the nested NP into the main NP.  While other work has utilized the OntoNotes corpus  (Pradhan et al., 2007; Yu et al., 2008), this is the  These three modifications to the data solved most,  first work to our knowledge to simultaneously model  but not all, of the inconsistencies. Another source  the multiple levels of annotation available. Because  of problems was conjunctions, such as North and  this is a new corpus, still under development, it is  South Korea, where North and South are a phrase, 329  of  Figure 2: An example from the data of inconsistently labeled named entity and parse structure. The inclusion of the final period in the named entity results in the named entity structure having crossing brackets with the parse structure.  the  [District  of  the  [District  of  Figure 3: (a) Another example from the data of inconsistently labeled named entity and parse structure. In this instance, we flatten the nested NP, resulting in (b), so that the named entity corresponds to a contiguous set of children of the top-level NP.  but South Korea is an entity. The rest of the  erNamed Entity Types  rors seemed to be due to annotation errors and other  random weirdnesses. We ended up unable to make  The data has been annotated with eighteen types of  0.4% of the entities consistent with the parses, so we  entities. Many of these entity types do not occur  omitted those entities from the training and test data.  very often, and coupled with the relatively small  amount of data, make it difficult to learn accurate  entity models. Examples are work of art, product, One more change we made to the data was with  and law. Early experiments showed that it was  difrespect to possessive NPs. When we encountered  ficult for even our baseline named entity recognizer,  noun phrases which ended with (POS 's) or (POS '), based on a state-of-the-art CRF, to learn these types  we modified the internal structure of the NP.  Origiof entities.3 As a result, we decided to merge all  nally, these NPs were flat, but we introduced a new  but the three most dominant entity types into into  nested NP which contained the entire contents of the  one general entity type called misc. The result was  original NP except for the POS. The original NP  lafour distinct entity types: person, organization, GPE  bel was then changed to PossNP. This change is  mo(geo-political entity, such as a city or a country), and  tivated by the status of 's as a phrasal affix or clitic: misc.  It is the NP preceding 's that is structurally equivalent to other NPs, not the larger unit that includes 's.  This change has the additional benefit in this context  The difficulties were compounded by somewhat  inconsistent and occasionally questionable annotations. For example,  that more named entities will correspond to a single  the word today was usually labeled as a date, but about 10% of phrase in the parse tree, rather than a contiguous set  the time it was not labeled as anything. We also found several of phrases.  strange work of art s, including Stanley Cup and the U.S.S. Cole.  and named entities jointly resulted in improved  performance on both. When looking at these numbers,  it is important to keep in mind that the sizes of the  training and test sets are significantly smaller than  the Penn Treebank. The largest of the six datasets,  CNN, has about one seventh the amount of training  data as the Penn Treebank, and the smallest, MNB,  has around 500 sentences from which to train. Parse  Table 1: Training and test set sizes for the six datasets in  performance was improved by the joint model for  sentences. The file ranges refer to the numbers within the  names of the original OntoNotes files.  five of the six datasets, by up to 1.36%. Looking  at the parsing improvements on a per-label basis,  the largest gains came from improved identication  of NML consituents, from an F-score of 45.9% to  We ran our model on six of the OntoNotes datasets  57.0% (on all the data combined, for a total of 420  described in Section 4,4 using sentences of length  NML constituents). This label was added in the new  40 and under (approximately 200,000 annotated  Entreebank annotation conventions, so as to identify  inglish words, considerably smaller than the Penn  ternal left-branching structure inside previously flat  Treebank (Marcus et al., 1993)). For each dataset,  NPs. To our surprise, performance on NPs only  inwe aimed for roughly a 75% train / 25% test split.  creased by 1%, though over 12, 949 constituents, for  See Table 1 for the the files used to train and test,  the largest improvement in absolute terms. The  secalong with the number of sentences in each.  ond largest gain was on PPs, where we improved by  For comparison, we also trained the parser  with1.7% over 3, 775 constituents. We tested the  signifout the named entity information (and omitted the  icance of our results (on all the data combined)  usNamedEntity nodes), and a linear chain CRF using  ing Dan Bikel's randomized parsing evaluation  comjust the named entity information. Both the  baseparator6 and found that both the precision and recall  line parser and CRF were trained using the exact  gains were significant at p 0.01.  same features as the joint model, and all were  opMuch greater improvements in performance were  timized using stochastic gradient descent. The full  seen on named entity recognition, where most of  results can be found in Table 2. Parse trees were  the domains saw improvements in the range of 3  scored using evalB (the extra NamedEntity nodes 4%, with performance on the VOA data improving  were ignored when computing evalB for the joint  by nearly 9%, which is a 45% reduction in error.  model), and named entities were scored using entity  There was no clear trend in terms of precision  verF-measure (as in the CoNLL 2003 conlleval).5  sus recall, or the different entity types. The first  While the main benefit of our joint model is the  place to look for improvements is with the  boundability to get a consistent output over both types of  aries for named entities. Once again looking at all of  annotations, we also found that modeling the parse  the data combined, in the baseline model there were  203 entities where part of the entity was found, but  These datasets all consistently use the new conventions for  treebank annotation, while the seventh WSJ portion is currently one or both boundaries were incorrectly identified.  still annotated in the original 1990s style, and so we left the The joint model corrected 72 of those entities, while  incorrectly identifying the boundaries of 37 entities  5Sometimes the parser would be unable to parse a sentence  which had previously been correctly identified. In  (less than 2% of sentences), due to restrictions in part of speech the baseline NER model, there were 243 entities for  tags. Because the underlying grammar (ignoring the additional  which the boundaries were correctly identified, but  named entity information) was the same for both the joint and  baseline parsers, it is the case that whenever a sentence is un-the type of entity was incorrect. The joint model  corparseable by either the baseline or joint parser it is in fact un-rected 80 of them, while changing the labels of 39  parsable by both of them, and would affect the parse scores of entities which had previously been correctly identi-both models equally. However, the CRF is able to named entity  fied. Additionally, 190 entities were found which  tag any sentence, so these unparsable sentences had an effect  the baseline model had missed entirely, and 68  ention the named entity score. To combat this, we fell back on  the baseline CRF model to get named entity tags for unparsable sentences.  Parse Labeled Bracketing  Named Entities  Precision  Precision  Table 2: Full parse and NER results for the six datasets. Parse trees were evaluated using evalB, and named entities were scored using macro-averaged F-measure (conlleval).  ties were lost. We tested the statistical significance  parser (Collins, 1997) over a syntactic structure  augof the gains (of all the data combined) using the  mented with the template entity and template rela-same sentence-level, stratified shuffling technique as  tions annotations for the MUC-7 shared task. Their  Bikel's parse comparator and found that both  precisentence augmentations were similar to ours, but  sion and recall gains were significant at p < 10 4.  they did not make use of features due to the  genAn example from the data where the joint model  erative nature of their model. This approach was not  helped improve both parse structure and named  enfollowed up on in other work, presumably because  tity recognition is shown in Figure 4. The output  around this time nearly all the activity in named  from the individual models is shown in part (a), with  entity and relation extraction moved to the use of  the output from the named entity recognizer shown  discriminative sequence models, which allowed the  in brackets on the words at leaves of the parse. The  flexible specification of feature templates that are  output from the joint model is shown in part (b),  very useful for these tasks. The present model is  with the named entity information encoded within  able to bring together both these lines of work, by  the parse. In this example, the named entity  Egypintegrating the strengths of both approaches.  tian Islamic Jihad helped the parser to get its  surrounding context correct, because it is improbable  There have been other attempts in NLP to jointly  to attach a PP headed by with to an organization.  model multiple levels of structure, with varying  deAt the same time, the surrounding context helped  grees of success. Most work on joint parsing and  sethe joint model correctly identify Egyptian Islamic  mantic role labeling (SRL) has been disappointing,  Jihad as an organization and not a person.  The  despite obvious connections between the two tasks.  baseline parser also incorrectly added an extra level  Sutton and McCallum (2005) attempted to jointly  of structure to the person name Osama Bin Laden,  model PCFG parsing and SRL for the CoNLL 2005  while the joint model found the correct structure.  shared task, but were unable to improve  performance on either task. The CoNLL 2008 shared task  Related Work  ing and SRL, but the top performing systems  deA pioneering antecedent for our work is (Miller et  coupled the tasks, rather than building joint models.  al., 2000), who trained a Collins-style generative  Zhang and Clark (2008) successfully built a joint  of  the [Egyptian Islamic Jihad]PER  with  NamedEntity-ORG*  of  the  Egyptian Islamic Jihad  with  to  Figure 4: An example for which the joint model helped with both parse structure and named entity recognition. The individual models (a) incorrectly attach the PP, label Egyptian Islamic Jihad as a person, and incorrectly add extra internal structure to Osama Bin Laden. The joint model (b) gets both the structure and the named entity correct.  model of Chinese word segmentation and parts of  is based on a discriminative constituency parser,  speech using a single perceptron.  with the data, grammar, and features carefully  conAn alternative approach to joint modeling is to  structed for the joint task. In the future, we would  take a pipelined approach. Previous work on  linguislike to add other levels of annotation available in  tic annotation pipelines (Finkel et al., 2006;  Hollingthe OntoNotes corpus to our model, including word  shead and Roark, 2007) has enforced consistency  sense disambiguation and semantic role labeling.  from one stage to the next. However, these models  are only used at test time; training of the  compoAcknowledgements  nents is still independent. These models also have  The first author is supported by a Stanford  Graduthe potential to suffer from search errors and are not  ate Fellowship. This paper is based on work funded  guaranteed to find the optimal output.  in part by the Defense Advanced Research Projects  Agency through IBM. The content does not  necesConclusion  sarily reflect the views of the U.S. Government, and  We presented a discriminatively trained joint model  no official endorsement should be inferred. We also  of parsing and named entity recognition, which  imwish to thank the creators of OntoNotes, without  proved performance on both tasks.  which this project would not have been possible.  V. N. Vapnik. 1998. Statistical Learning Theory. John  Wiley & Sons.  Alexander Clark. 2000. Inducing syntactic categories by  context distribution clustering. In Proc. of Conference  2008. OntoNotes: Corpus cleanup of mistaken  agreeon Computational Natural Language Learning, pages  ment using word sense disambiguation. In  Proceedings of the 22nd International Conference on  CompuMichael Collins.  1997. Three generative, lexicalised  tational Linguistics (Coling 2008), pages 1057 1064.  models for statistical parsing. In ACL 1997.  Yue Zhang and Stephen Clark. 2008. Joint word  segmenJenny Rose Finkel and Christopher D. Manning. 2008.  tation and POS tagging using a single perceptron. In  Efficient, feature-based conditional random field  parsManning. 2005. Incorporating non-local information  into information extraction systems by gibbs sampling.  In ACL 2005.  Jenny Rose Finkel, Christopher D. Manning, and  Andrew Y. Ng. 2006. Solving the problem of cascading  errors: Approximate bayesian inference for linguistic  Kristy Hollingshead and Brian Roark. 2007. Pipeline  iteration. In ACL 2007.  Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance  Ramshaw, and Ralph Weischedel. 2006. Ontonotes:  The 90% solution. In HLT-NAACL 2006.  Tsujii. 2003. Genia corpus a semantically annotated  corpus for bio-textmining. Bioinformatics, 19(suppl.  Mitchell P. Marcus, Beatrice Santorini, and Mary Ann  Marcinkiewicz. 1993. Building a large annotated  corpus of English: The Penn Treebank. Computational  Linguistics, 19(2):313 330.  Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph  Weischedel. 2000. A novel use of statistical parsing to  extract information from text. In In 6th Applied  Natural Language Processing Conference, pages 226 233.  Andrew Ng and Michael Jordan. 2002. On  discriminative vs. generative classifiers: A comparison of logistic  regression and naive bayes. In Advances in Neural  Information Processing Systems (NIPS).  restricted coreference: Identifying entities and events  in ontonotes. International Conference on Semantic  Mihai Surdeanu, Richard Johansson, Adam Meyers,  Llu s M rquez, and Joakim Nivre. 2008. The  CoNLL2008 shared task on joint parsing of syntactic and  semantic dependencies. In Proceedings of the 12th  Conference on Computational Natural Language Learning  (CoNLL), Manchester, UK.  Charles Sutton and Andrew McCallum. 2005. Joint  parsing and semantic role labeling. In Conference on 