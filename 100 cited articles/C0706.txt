 Extracting Semantic Orientations of Phrases from Dictionary Hiroya Takamura  Precision and Intelligence Laboratory  Integrated Research Institute  Tokyo Institute of Technology  Tokyo Institute of Technology  Precision and Intelligence Laboratory  Tokyo Institute of Technology  The most fundamental step for sentiment  analysis is to acquire the semantic orientations of words:  We propose a method for extracting  semantic orientations of phrases (pairs of an  example, the word beautiful is positive, while the  adjective and a noun): positive, negative,  word dirty is negative. Many researchers have  deor neutral. Given an adjective, the  semanveloped several methods for this purpose and  obtic orientation classification of phrases can  tained good results. One of the next problems to be  be reduced to the classification of words.  solved is to acquire semantic orientations of phrases, We construct a lexical network by con-or multi-term expressions, such as high+risk and  necting similar/related words. In the  net light+laptop-computer . Indeed the semantic  oriwork, each node has one of the three  orientations of phrases depend on context just as the  seentation values and the neighboring nodes  mantic orientations of words do, but we would like  tend to have the same value. We adopt  to obtain the orientations of phrases as basic units  the Potts model for the probability model  for sentiment analysis. We believe that we can use  of the lexical network. For each  adjecthe obtained basic orientations of phrases for affect  tive, we estimate the states of the nodes,  analysis of higher linguistic units such as sentences  which indicate the semantic orientations  of the adjective-noun pairs. Unlike  exA computational model for the semantic  orientaisting methods for phrase classification,  tions of phrases has been proposed by Takamura et  the proposed method can classify phrases  al. (2006). However, their method cannot deal with  consisting of unseen words. We also  prothe words that did not appear in the training data.  pose to use unlabeled data for a seed set of  The purpose of this paper is to propose a method for  extracting semantic orientations of phrases, which is  ation shows the effectiveness of the  proapplicable also to expressions consisting of unseen  words. In our method, we regard this task as the  noun classification problem for each adjective; the  Introduction  nouns that become respectively positive (negative,  or neutral) when combined with a given adjective  Technology for affect analysis of texts has recently  are distinguished from the other nouns. We create  gained attention in both academic and industrial  ara lexical network with words being nodes, by  coneas. It can be applied to, for example, a survey of  necting two words if one of the two appears in the  new products or a questionnaire analysis. Automatic  gloss of the other. In the network, each node has one  sentiment analysis enables a fast and comprehensive  of the three orientation values and the neighboring  investigation.  nodes expectedly tend to have the same value. For  Proceedings of NAACL HLT 2007, pages 292 299,  2007 Association for Computational Linguistics  example, the gloss of cost is a sacrifice, loss, or Takamura et al. (2006) proposed to use based on  penalty and these words (cost, sacrifice, loss, and  latent variable models for sentiment classification of penalty) have the same orientation. To capture this  noun-adjective pairs. Their model consists of  varitendency of the network, we adopt the Potts model  ables respectively representing nouns, adjectives,  sefor the probability distribution of the lexical  netmantic orientations, and latent clusters, as well as  work. For each adjective, we estimate the states of  the edges between the nodes. The words that are  the nodes, which indicate the semantic orientations  similar in terms of semantic orientations, such as  of the adjective-noun pairs. Information from seed  risk and mortality (i.e., the positive orientation words is diffused to unseen nouns on the network.  emerges when they are low ), make a cluster in  We also propose a method for enlarging the seed  their model, which can be an automated version of  set by using the output of an existing method for the  Inui's or Wilson et al.'s idea above. However, their  seed words of the probability computation.  method cannot do anything for the words that did not  Empirical evaluation shows that our method  appear in the labeled training data. In this paper, we works well both for seen and unseen nouns, and that  call their method the latent variable method (LVM).  the enlarged seed set significantly improves the  classification performance of the proposed model.  If a variable can have more than two values and  Related Work  there is no ordering relation between the values,  the network comprised of such variables is called  The semantic orientation classification of words has  Potts model (Wu, 1982). In this section, we ex-been pursued by several researchers.  Some of  plain the simplified mathematical model of Potts  them used corpora (Hatzivassiloglou and McKeown,  model, which is used for our task in Section 4.  1997; Turney and Littman, 2003), while others used  The Potts system has been used as a mathematical  dictionaries (Kobayashi et al., 2001; Kamps et al.,  model in several applications such as image  restoration (Tanaka and Morita, 1996) and rumor  transmisTurney (2002) applied an internet-based  technique to the semantic orientation classification of  Introduction to the Potts Model  phrases, which had originally been developed for  Suppose a network consisting of nodes and weighted  word sentiment classification. In their method, the  edges is given. States of nodes are represented by c.  number of hits returned by a search-engine, with a  The weight between i and j is represented by wij.  query consisting of a phrase and a seed word (e.g.,  Let H(c) denote an energy function, which  indi phrase NEAR good ) is used to determine the ori-cates a state of the whole network:  entation. Baron and Hirst (2004) extracted  collocations with Xtract (Smadja, 1993) and classified the  collocations using the orientations of the words in  the neighboring sentences. Their method is similar  where is a constant called the inverse-temperature, to Turney's in the sense that cooccurrence with seed  L is the set of the indices for the observed variables, words is used. In addition to individual seed words,  ai is the state of each observed variable indexed by i, Kanayama and Nasukawa (2006) used more compli-and is a positive constant representing a weight on cated syntactic patterns that were manually created.  labeled data. Function returns 1 if two arguments The four methods above are based on context infor-are equal to each other, 0 otherwise. The state is  mation. In contrast, our method exploits the internal  penalized if ci ( i L) is different from ai. Using structure of the semantic orientations of phrases.  H(c), the probability distribution of the network is Wilson et al. (2005) worked on phrase-level se-represented as P (c) = exp { H(c) }/Z, where Z is mantic orientations.  They introduced a polarity  a normalization factor.  shifter. They manually created the list of polarity  However, it is computationally difficult to exactly  shifters. Inui (2004) also proposed a similar idea.  estimate the state of this network. We resort to a  mean-field approximation method that is described We can find a similarity also to the PageRank al-by Nishimori (2001). In the method, P (c) is re-gorithm (Brin and Page, 1998), which has been  applaced by factorized function (c) =  plied also to natural language processing tasks  (MiThen we can obtain the function with the smallest  halcea, 2004; Mihalcea, 2005). In the PageRank  alvalue of the variational free energy:  gorithm, the pagerank score ri is updated as  where d is a constant (0 d 1). This update i  equation consists of the first term corresponding to  i( ci) j ( cj ) wij ( ci, cj ) random jump from an arbitrary node and the sec-ij  ond term corresponding to the random walk from the  Let us derive the first order Taylor expansion of  By minimizing F (c) under the condition that i, Equation (3). We use the equation for i /  L and  denote the denominator by Z  c i( ci) = 1, we obtain the following fixed point , for simplicity. Since  The fixed point equation for i /  L can be obtained  by removing ( c, a  i) from above.  This fixed point equation is solved by an  iterative computation. In the actual implementation, we  represent i with a linear combination of the dis-Equation (5) clearly has a quite similar form as  crete Tchebycheff polynomials (Tanaka and Morita,  Equation (4). Thus, the PageRank algorithm can be  1996). Details on the Potts model and its  computaregarded as an approximation of our model. Let us  tion can be found in the literature (Nishimori, 2001).  clarify the difference between the two algorithms.  After the computation, we obtain the function  The PageRank is designed for two-class  classificai i( ci). When the number of classes is 2, the Potts tion, while the Potts model can be used for an arbi-model in this formulation is equivalent to the  meantrary number of classes. In this sense, the PageRank  field Ising model (Nishimori, 2001).  is an approximated Ising model. The PageRank is  Relation to Other Models  applicable to asymmetric graphs, while the theory  used in this paper is based on symmetric graphs.  This Potts model with the mean-field approximation  has relation to several other models.  As is often discussed (Mackay, 2003), the  minimization of the variational free energy  (Equation (2)) is equivalent to the obtaining the factorized In this section, we explain our classification method, model that is most similar to the maximum likeli-which is applicable also to the pairs consisting of an hood model in terms of the Kullback-Leibler diver-adjective and an unseen noun.  The second term of Equation (2) is the entropy  Construction of Lexical Networks  of the factorized function. Hence the optimization  We construct a lexical network, which Takamura et  problem to be solved here is a kind of the  maxial. (2005) call the gloss network, by linking two  mum entropy model with a penalty term, which  corwords if one word appears in the gloss of the other  responds to the first term of Equation (2).  Each link belongs to one of two groups:  the same-orientation links SL and the different-not explicitly incorporated, we can manually  deterorientation links DL.  mine two thresholds that define respectively the  posIf a negation word (e.g., nai, for Japanese) follows  itive/neutral and negative/neutral boundaries. For  a word in the gloss of the other word, the link is a  the semantic orientations of phrasal expressions,  different-orientation link. Otherwise the links is a  however, it is impractical to manually determine  the thresholds for each of the numerous adjectives.  We next set weights W = ( wij) to links : Therefore, we have to incorporate the neutral class  using the Potts model.  For some adjectives, the semantic orientation is  constant regardless of the nouns. We need not use  otherwise  the Potts model for those unambiguous adjectives.  We thus propose the following two-step  classificawhere lij denotes the link between word i and word tion procedure for a given noun-adjective pair < j, and d( i) denotes the degree of word i, which n, a > .  means the number of words linked with word i. Two words without connections are regarded as being  1. if the semantic orientation of all the instances  connected by a link of weight 0.  with a in L is c, then classify < n, a > into c.  2. otherwise, use the Potts model.  Classification of Phrases  Takamura et al. (2005) used the Ising model to  exWe can also construct a probability model for  tract semantic orientations of words (not phrases).  each noun to deal with unseen adjectives. However,  We extend their idea and use the Potts model to  exwe focus on the unseen nouns in this paper, because  tract semantic orientations of phrasal expressions.  our dataset has many more nouns than adjectives.  Given an adjective, the decision remaining to be  Hyper-parameter Prediction  made in classification of phrasal expressions  concerns nouns. We therefore estimate the state of the  The performance of the proposed method largely  denodes on the lexical network for each adjective. The  pends on the value of hyper-parameter . In order to nouns paring with the given adjective in the train-make the method more practical, we propose a  criing data are regarded as seed words, which we call  seen words, while the words that did not appear in  Takamura et al. (2005) proposed two kinds of  crithe training data are referred to as unseen words.  teria. One of the two criteria is an approximated  We use the mean-field method to estimate the  leave-one-out error rate and can be used only when a  state of the system. If the probability  large labeled dataset is available. The other is a  noable being positive (negative, neutral) is the highest tion from statistical physics, that is, magnetization: of the three classes, then the word corresponding to  the variable is classified as a positive (negative, neu-m  We explain the reason why we use the Potts model  At a high temperature, variables are randomly  oriinstead of the Ising model. While only two classes  ented ( paramagnetic phase, m 0). At a low (i.e., positive and negative) can be modeled by the  temperature, most of the variables have the same  direction ( ferromagnetic phase, m 6= 0).  It is  and neutral) can be modelled by the Potts model.  known that at some intermediate temperature,  ferroFor the semantic orientations of words, all the words  magnetic phase suddenly changes to paramagnetic  are sorted in the order of the average orientation  phase. This phenomenon is called phase transition.  value, equivalently the probability of the word  beSlightly before the phase transition, variables are lo-ing positive. Therefore, even if the neutral class is  cally polarized; strongly connected nodes have the  same polarity, but not in a global way. Intuitively,  For English data, a negation should precede a word, in order for the corresponding link to be a different-orientation link.  the state of the lexical network is locally polarized.  Therefore, they calculate values of m with several consisting of 12066 pair instances (7416 different  different values of and select the value just before pairs). The dataset contains 4459 negative instances,  the phase transition.  4252 neutral instances, and 3355 positive instances.  Since we cannot expect a large labeled dataset  The number of distinct nouns is 4770 and the  numto be available for each adjective, we use not  ber of distinct adjectives is 384. To check the  interthe approximated leave-one-out error rate, but the  annotator agreement between two annotators, we  magnetization-like criterion. However, the  magnecalculated statistics, which was 0.6402. This value tization above is defined for the Ising model. We  is allowable, but not quite high. However,  positivetherefore consider that the phase transition has  ocnegative disagreement is observed for only 0.7% of  curred, if a certain class c begins to be favored all the data. In other words, this statistics means that  over the system. In practice, when the maximum of  the task of extracting neutral examples, which has  the spatial averages of the approximated  probabilhardly been explored, is intrinsically difficult.  i i( c) /N exceeds a threshold during We should note that the judgment in annotation  increasing , we consider that the phase transition depends on which perspective the annotator takes;  has occurred. We select the value of slightly  be high+salary is positive from employee's  perspecfore the phase transition.  tive, but negative from employer's perspective. The  annotators are supposed to take a perspective  subjecEnlarging Seed Word Set  tively. Our attempt is to imitate annotator's decision.  We usually have only a few seed words for a given  To construct a classifier that matches the decision of adjective. Enlarging the set of seed words will in-the average person, we also have to address how to  crease the classification performance. Therefore, we  create an average corpus. We do not pursue this  isautomatically classify unlabeled pairs by means of  sue because it is out of the scope of the paper.  an existing method and use the classified instances  As unlabeled data, we extracted approximately  65,000 pairs for each iteration of the 10-fold  crossAs an existing classifier, we use LVM. Their  validation, from the same news source.  model can classify instances that consist of a seen  The average number of seed nouns for each  amnoun and a seen adjective, but are unseen as a pair.  biguous adjective was respectively 104 in the  laAlthough we could classify and use all the nouns  beled seed set and 264 in the labeled+unlabeled seed  that appeared in the training data (with an adjective  set. Please note that these figures are counted for  which is different from the given one), we do not  only ambiguous adjectives. Usually ambiguous  adadopt such an alternative, because it will incorporate jectives are more frequent than unambiguous adjec-even non-collocating pairs such as green+idea into  seeds, resulting in possible degradation of  classification performance. Therefore, we sample unseen  pairs consisting of a seen noun and a seen adjective  We employ 10-fold cross-validation to obtain the  from a corpus, classify the pairs with the latent vari-averaged classification accuracy. We split the data  able model, and add them to the seed set. The  ensuch that there is no overlapping pair (i.e., any pair larged seed set consists of pairs used in newspaper  in the training data does not appear in the test data).  articles and does not include non-collocating pairs.  Hyperparameter was set to 1000, which is very large since we regard the labels in the seed set is  reliable. For the seed words added by the classifier,  lower can be better. Determining a good value for is regarded as future work.  We extracted pairs of a noun (subject) and an  adHyperparameter is automatically selected from jective (predicate), from Mainichi newspaper arti-2  cles (1995) written in Japanese, and annotated the  Although Kanayama and Nasukawa (2006) that for their dataset similar to ours was 0 . 83, this value cannot be directly pairs with semantic orientation tags : positive, neu-compared with our value because their dataset includes both intral or negative. We thus obtained the labeled dataset dividual words and pairs of words.  { 0.1, 0.2, , 2.5 } for each adjective and each fold tion, since many compound nouns and exceedingly-of the cross-validation using the prediction method  subdivided morphemes are not in dictionaries. An  described in Section 4.3.  appropriate mapping from the words found in  corpus to entries of a dictionary will solve this problem.  We found a number of proper nouns, many of which  The results of the classification experiments are  are not in the dictionary. By estimating a class of a  summarized in Table 1.  proper noun and finding the words that matches the  The proposed method succeeded in classifying,  class in the dictionary, we can predict the semantic  with approximately 65% in accuracy, those phrases  orientations of the proper noun based on the  orientaconsisting of an ambiguous adjective and an unseen  tions of the found words.  noun, which could not be classified with existing  In order to see the overall tendency of errors, we  computational models such as LVM.  calculated the confusion matrices both for pairs of  Incorporation of unlabeled data improves  accuan ambiguous adjective and a seen noun, and for  racy by 15.5 points for pairs consisting of a seen  pairs of an ambiguous adjective and an unseen noun  noun and an ambiguous adjective, and by 3.5 points  (Table 2). The proposed method works quite well for  for pairs consisting of an unseen noun and an  ampositive/negative classification, though it finds still biguous adjective, approximately. The reason why  some difficulty in correctly classifying neutral  inthe former obtained high increase is that pairs with  stances even after enhanced with the unlabeled data.  an ambiguous adjective3 are usually frequent and  In order to qualitatively evaluate the method,  likely to be found in the added unlabeled dataset.  we list several word pairs below.  These word  If we regard this classification task as binary  claspairs are classified by the Potts model with the  lasification problems where we are to classify  instances into one class or not, we obtain three  accuthey did not appear in the original training dataset.  racies: 90.76% for positive, 81.75% for neutral, and  Please note again that the actual data is Japanese.  86.85% for negative. This results suggests the  identification of neutral instances is relatively difficult.  Next we compare the proposed method with  adjective  LVM. The latent variable method is applicable only  basic price  to instance pairs consisting of an adjective and a  seen noun. Therefore, we computed the accuracy  high  for 6586 instances using the latent variable method  educational background  high  and obtained 80.76 %. The corresponding accuracy  by our method was 80.93%. This comparison shows  that our method is better than or at least comparable  high  to the latent variable method. However, we have to  note that this accuracy of the proposed method was  computed using the unlabeled data classified by the  There are still 3320 (=12066-8746) word pairs  capacity  which could not be classified, because there are no  entries for those words in the dictionary. However,  the main cause of this problem is word  segmentaknowledge  3Seen nouns are observed in both the training and the test datasets because they are frequent. Ambiguous adjectives are For example, although both salary and com-often-used adjectives such as large , small , high , and  mission are kinds of money, our method captures  Table 1: Classification accuracies (%) for various seed sets and test datasets. Labeled' seed set corresponds to the set of manually labeled pairs. Labeled+unlabeled' seed set corresponds to the union of labeled' seed set and the set of pairs labeled by LVM. Seen nouns' for test are the nouns that appeared in the training data, while unseen nouns' are the nouns that did not appear in the training dataset'. Please note that seen pairs are excluded from the test data. Unambiguous' adjectives corresponds to the pairs with an adjective which has a unique orientation in the original training dataset, while ambiguous' adjectives corresponds to the pairs with an adjective which has more than one orientation in the original training dataset.  Table 2: Confusion matrices of classification result with labeled+unlabeled seed set Potts model  the difference between them; high salary is  posiity have strong interaction (Wiebe and  Mihaltive, while low (cheap) commission is also  posi The value of must be properly set, because 6  Conclusion  lower can be better for the seed words added by the classifier,  We proposed a method for extracting semantic  orientations of phrases (pairs of an adjective and a  To address word-segmentation problem  disnoun). For each adjective, we constructed a Potts  cussed in Section 5.3, we can utilize the fact  system, which is actually a lexical network extracted  that the heads of compound nouns often inherit  from glosses in a dictionary. We empirically showed  the property determining the semantic  orientathat the proposed method works well in terms of  tion when combined with an adjective.  classification accuracy.  Future work includes the following:  The semantic orientations of pairs consisting of a proper noun will be estimated from the named  We assumed that each word has a semantic orientity classes of the proper nouns such as  perentation. However, word senses and  subjectivson name and organization.  Rada Mihalcea. 2005. Unsupervised large-vocabulary  word sense disambiguation with graph-based  algoFaye Baron and Graeme Hirst. 2004. Collocations as  rithms for sequence data labeling. In Proceedings of cues to semantic orientation. In AAAI Spring Sympo-the Joint Conference on Human Language Technology  sium on Exploring Attitude and Affect in Text:  Theo/ Empirical Methods in Natural Language Processing ries and Applications.  Sergey Brin and Lawrence Page. 1998. The anatomy of  Hidetoshi Nishimori. 2001. Statistical Physics of Spin a large-scale hypertextual Web search engine. Com-Glasses and Information Processing. Oxford Univer-puter Networks and ISDN Systems, 30(1 7):107 117.  sity Press.  Frank Z. Smadja. 1993. Retrieving collocations from  ing the semantic orientation of terms through gloss  text: Xtract. Computational Linguistics, 19(1):143  In Proceedings of the 14th ACM  International Conference on Information and Knowledge  2005. Extracting semantic orientations of words using  spin model. In Proceedings of the 43rd Annual Meet-1997. Predicting the semantic orientation of  adjecing of the Association for Computational Linguistics tives. In Proceedings of the 35th Annual Meeting of (ACL'05), pages 133 140.  the Association for Computational Linguistics and the Hiroya Takamura, Takashi Inui, and Manabu Okumura.  8th Conference of the European Chapter of the Asso-2006. Latent variable models for semantic orientations ciation for Computational Linguistics, pages 174 181.  of phrases. In Proceedings of the 11th Conference of the European Chapter of the Association for Compu-Takashi Inui. 2004. Acquiring Causal Knowledge from tational Linguistics (EACL'06).  Text Using Connective Markers. Ph.D. thesis, Grad-Kazuyuki Tanaka and Tohru Morita. 1996. Application  uate School of Information Science, Nara Institute of  of cluster variation method to image restoration  probScience and Technology.  lem. In Theory and Applications of the Cluster Vari-Jaap Kamps, Maarten Marx, Robert J. Mokken, and  ation and Path Probability Methods, pages 353 373.  Plenum Press, New York.  semantic orientation of adjectives.  In Proceedings  of the 4th International Conference on Language Re-Peter D. Turney and Michael L. Littman. 2003.  Measursources and Evaluation (LREC'04), volume IV, pages ing praise and criticism: Inference of semantic orien-1115 1118.  tation from association. ACM Transactions on Information Systems, 21(4):315 346.  Peter D. Turney. 2002. Thumbs up or thumbs down?  timent analysis. In Proceedings of the Conference on semantic orientation applied to unsupervised classifi-Empirical Methods in Natural Language Processing  cation of reviews. In Proceedings 40th Annual  Meeting of the Association for Computational Linguistics (ACL'02), pages 417 424.  Dictionary-based acquisition of the lexical  Janyce M. Wiebe and Rada Mihalcea. 2006. Word sense  knowledge for p/n analysis (in Japanese).  and subjectivity. In Proceedings of the 21st Interna-ceedings of Japanese Society for Artificial Intelligence, tional Conference on Computational Linguistics and SLUD-33, pages 45 50.  the 44th annual meeting of the Association for Computational Linguistics (COLING-ACL'06), pages 1065  Potts model for exaggeration of a simple rumor  transmitted by recreant rumormongers. Physical Review E, Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.  David J. C. Mackay. 2003. Information Theory, Infer-sentiment analysis.  In Proceedings of joint  conference and Learning Algorithms. Cambridge University ence on Human Language Technology / Conference on  Press.  Empirical Methods in Natural Language Processing  Fa-Yueh Wu. 1982. The potts model. Reviews of  ModRada Mihalcea. 2004. Graph-based ranking algorithms  ern Physics, 54(1):235 268.  for sentence extraction, applied to text summarization.  In The Companion Volume to the Proceedings of the  42nd Annual Meeting of the Association for Computational Linguistics, (ACL'04), pages 170 173. 