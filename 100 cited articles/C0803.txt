 An Application to Opinion Mining  cipline that deals with the quantitative and  qualitative analysis of text for the purpose of determining  This paper presents an application of  PageRits opinion-related properties (ORPs). An important  ank, a random-walk model originally  depart of this research has been the work on the  autovised for ranking Web search results, to  matic determination of the ORPs of terms, as e.g.,  ranking WordNet synsets in terms of how  in determining whether an adjective tends to give a  strongly they possess a given semantic  proppositive, a negative, or a neutral nature to the noun  erty. The semantic properties we use for  exphrase it appears in. While many works (Esuli and  emplifying the approach are positivity and  Sebastiani, 2005; Hatzivassiloglou and McKeown,  negativity, two properties of central  importance in sentiment analysis. The idea derives  Turney and Littman, 2003) view the properties of  from the observation that WordNet may be  positivity and negativity as categorical (i.e., a term is seen as a graph in which synsets are con-either positive or it is not), others (Andreevskaia and nected through the binary relation a term  belonging to synset sk occurs in the gloss  Hovy, 2004; Subasic and Huettner, 2001) view them  of synset si , and on the hypothesis that  as graded (i.e., a term may be positive to a certain  this relation may be viewed as a  transmitdegree), with the underlying interpretation varying  ter of such semantic properties. The data  from fuzzy to probabilistic.  for this relation can be obtained from  eXSome authors go a step further and attach these  tended WordNet, a publicly available  senseproperties not to terms but to term senses  (typdisambiguated version of WordNet. We  arically: WordNet synsets), on the assumption that  gue that this relation is structurally akin to  different senses of the same term may have  difthe relation between hyperlinked Web pages,  ferent opinion-related properties (Andreevskaia and  and thus lends itself to PageRank analysis.  We report experimental results supporting  2006; Wiebe and Mihalcea, 2006).  In this paper we contribute to this latter literature  with a novel method for ranking the entire set of  Introduction  WordNet synsets, irrespectively of POS, according  to their ORPs. Two rankings are produced, one  acRecent years have witnessed an explosion of work  cording to positivity and one according to negativity.  on opinion mining (aka sentiment analysis), the  disThe two rankings are independent, i.e., it is not the  This work was partially supported by Project ONTOTEXT  case that one is the inverse of the other, since e.g.,  From Text to Knowledge for the Semantic Web , funded by the least positive synsets may be negative or neutral  Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 424 431, Prague, Czech Republic, June 2007. c  2007 Association for Computational Linguistics  The main hypothesis underlying our method is draulics between synsets. We thus see positivity  that the positivity and negativity of WordNet synsets  and negativity only as proofs-of-concept for the  pocan be determined by mining their glosses.  It  tential of the method.  crucially relies on the observation that the gloss  The rest of the paper is organized as follows.  Secof a WordNet synset contains terms that  themtion 2 reports on related work on the ORPs of  lexselves belong to synsets, and on the hypothesis that  ical items, highlighting the similarities and  differthe glosses of positive (resp. negative) synsets will  ences between the discussed methods and our own.  mostly contain terms belonging to positive  (negaIn Section 3 we turn to discussing our method; in  ortive) synsets. This means that the binary relation  der to make the paper self-contained, we start with  si I sk ( the gloss of synset si contains a term  a brief introduction of PageRank (Section 3.1) and  belonging to synset sk ), which induces a directed  of the structure of eXtended WordNet (Section 3.2).  graph on the set of WordNet synsets, may be thought  Section 4 describes the structure of our experiments,  of as a channel through which positivity and  negawhile Section 5 discusses the results we have  obtivity flow, from the definiendum (the synset si  betained, comparing them with other results from the  ing defined) to the definiens (a synset sk that  contributes to the definition of si by virtue of its member terms occurring in the gloss of si). In other words,  Related work  if a synset si is known to be positive (negative), this can be viewed as an indication that the synsets sk to  Several works have recently tackled the automated  which the terms occurring in the gloss of si belong,  determination of term polarity. Hatzivassiloglou and  are themselves positive (negative).  McKeown (1997) determine the polarity of  adjecWe obtain the data of the I relation from  eXtives by mining pairs of conjoined adjectives from  tended WordNet (Harabagiu et al., 1999), an  autotext, and observing that conjunctions such as and  matically sense-disambiguated version of WordNet  tend to conjoin adjectives of the same polarity while  in which every term occurrence in every gloss is  conjunctions such as but tend to conjoin adjectives  linked to the synset it is deemed to belong to.  of opposite polarity. Turney and Littman (2003)  deIn order to compute how polarity flows in the  termine the polarity of generic terms by computing  graph of WordNet synsets we use the well known  the pointwise mutual information (PMI) between the  PageRank algorithm (Brin and Page, 1998).  PageRtarget term and each of a set of seed terms of  ank, a random-walk model for ranking Web search  known positivity or negativity, where the marginal  results which lies at the basis of the Google search  and joint probabilities needed for PMI computation  engine, is probably the most important single  contriare equated to the fractions of documents from a  bution to the fields of information retrieval and Web  given corpus that contain the terms, individually or  search of the last ten years, and was originally  dejointly. Kamps et al. (2004) determine the polarity  vised in order to detect how authoritativeness flows  of adjectives by checking whether the target  adjecin the Web graph and how it is conferred onto Web  tive is closer to the term good or to the term bad  sites. The advantages of PageRank are its strong  in the graph induced on WordNet by the synonymy  theoretical foundations, its fast convergence  properrelation. Kim and Hovy (2004) determine the  poties, and the effectiveness of its results. The reason larity of generic terms by means of two alternative  why PageRank, among all random-walk algorithms,  learning-free methods that use two sets of seed terms  is particularly suited to our application will be  disof known positivity and negativity, and are based  cussed in the rest of the paper.  on the frequency with which synonyms of the target  Note however that our method is not limited to  term also appear in the respective seed sets. Among  ranking synsets by positivity and negativity, and  these works, (Turney and Littman, 2003) has proven  could in principle be applied to the determination of  by far the most effective, but it is also by far the most other semantic properties of synsets, such as mem-computationally intensive.  bership in a domain, since for many other properties  Some recent works have employed, as in the  we may hypothesize the existence of a similar  hypresent paper, the glosses from online  dictionaries for term polarity detection. Andreevskaia and the |N | |N | adjacency matrix of G, i.e., the ma-Berger (2006a) extend a set of terms of known  postrix such that W0[i, j] = 1 iff there is a link from  itivity/negativity by adding to them all the terms  whose glosses contain them; this algorithm does not  {nj | W0[j, i] = 1} the set of the backward  neighview glosses as a source for a graph of terms, and  bours of ni, and by F (i) = {nj | W0[i, j] = 1}  is based on a different intuition than ours. Esuli  the set of the forward neighbours of ni. Let W be  and Sebastiani (2005; 2006a) determine the ORPs of  the row-normalized adjacency matrix of G, i.e., the  generic terms by learning, in a semi-supervised way,  matrix such that W[i, j] =  a binary term classifier from a set of training terms  and W[i, j] = 0 otherwise.  that have been given vectorial representations by  inThe input to PageRank is the row-normalized  addexing their WordNet glosses. The same authors  jacency matrix W, and its output is a vector a =  later extend their work to determining the ORPs  ha1, . . . , a|N|i, where ai represents the score of of WordNet synsets (Esuli and Sebastiani, 2006b).  node ni. When using PageRank for search results  However, there is a substantial difference between  ranking, ni is a Web site and ai measures its  comthese works and the present one, in that the former  puted authoritativeness; in our application ni is  insimply view the glosses as sources of textual  represtead a synset and ai measures the degree to which  sentations for the terms/synsets, and not as inducing  ni has the semantic property of interest. PageRank  a graph of synsets as we instead view them here.  iteratively computes vector a based on the formula  The work closest in spirit to the present one is  probably that by Takamura et al. (2005), who  determine the polarity of terms by applying intuitions  from the theory of electron spins: two terms that  appear one in the gloss of the other are viewed as akin  where a  denotes the value of the i-th entry of  vecto two neighbouring electrons, which tend to acquire  tor a at the k-th iteration, ei is a constant such that the same spin (a notion viewed as akin to polarity)  = 1, and 0 1 is a control parameter.  due to their being neighbours. This work is  simiIn vectorial form, Equation 1 can be written as  lar to ours since a graph between terms is generated  from dictionary glosses, and since an iterative  algorithm that converges to a stable state is used, but the algorithm is very different, and based on intuitions  The underlying intuition is that a node ni has a high  from very different walks of life.  score when (recursively) it has many high-scoring  Some recent works have tackled the attribution  backward neighbours with few forward neighbours  of opinion-related properties to word senses or  each; a node nj thus passes its score aj along to  synsets (Ide, 2006; Wiebe and Mihalcea, 2006)1;  its forward neighbours F (j), but this score is  subhowever, they do not use glosses in any significant  divided equally among the members of F (j). This  way, and are thus very different from our method.  mechanism (that is represented by the summation in  The interested reader may also consult (Mihalcea,  Equation 1) is then smoothed by the ei constants,  2006) for other applications of random-walk models  whose role is (see (Bianchini et al., 2005) for  deto computational linguistics.  tails) to avoid that scores flow and get trapped into  so-called rank sinks (i.e., cliques with backward  neighbours but no forward neighbours).  The PageRank algorithm  The computational properties of the PageRank  algorithm, and how to compute it efficiently, have  Let G = hN, Li be a directed graph, with N its set  been widely studied; the interested reader may  conof nodes and L its set of directed links; let W0 be  1Andreevskaia and Berger (2006a) also work on term  In the original application of PageRank for  ranksenses, rather than terms, but they evaluate their work on terms ing Web search results the elements of e are usually  only. This is the reason why they are listed in the preceding paragraph and not here.  taken to be all equal to 1 . However, it is possible  to give different values to different elements in e. In contain a link from synset si to synset sk iff the  fact, the value of ei amounts to an internal source  gloss of si contains at least a term belonging  of score for ni that is constant across the iterations to sk (terms occurring in the examples phrases  and independent from its backward neighbours. For  and terms occurring after a term that expresses  instance, attributing a null ei value to all but a few negation are not considered). Numbers, articles  Web pages that are about a given topic can be used  and prepositions occurring in the glosses are  in order to bias the ranking of Web pages in favour  discarded, since they can be assumed to carry  of this topic (Haveliwala, 2003).  no positivity and negativity, and since they do  In this work we use the ei values as internal  not belong to a synset of their own. This leaves  sources of a given ORP (positivity or negativity),  only nouns, adjectives, verbs, and adverbs.  by attributing a null ei value to all but a few seed  2. The graph G = hN, Li is pruned by  removsynsets known to possess that ORP. PageRank will  thus make the ORP flow from the seed synsets, at  s  a rate constant throughout the iterations, into other  i into itself (since we assume that there is no  flow of semantics from a concept unto itself).  synsets along the I relation, until a stable state is  The row-normalized adjacency matrix W of G  reached; the final ai values can be used to rank the  synsets in terms of that ORP. Our method thus  requires two runs of PageRank; in the first e has  non3. The ei values are loaded into the e vector; all  null scores for the positive seed synsets, while in the synsets other than the seed synsets of renowned  second the same happens for the negative ones.  positivity (negativity) are given a value of 0.  The control parameter is set to a fixed value.  We experiment with several different versions  The transformation of WordNet into a graph based  of the e vector and several different values of  on the I relation would of course be  non ; see Section 4.3 for details.  trivial, but is luckily provided by eXtended  Word4. PageRank is executed using W and e,  iterversion of WordNet in which (among other things)  ating until a predefined termination condition  each term s  is reached. The termination condition we use  k occurring in a WordNet gloss  (except those in example phrases) is lemmatized and  in this work consists in the fact that the  comapped to the synset in which it belongs2.  sine of the angle between a(k) and a(k+1) is  use eXtended WordNet version 2.0-1.1, which refers  above a predefined threshold (here we have  to WordNet version 2.0. The eXtended WordNet  resource has been automatically generated, which  5. We rank all the synsets of WordNet in  descendmeans that the associations between terms and  ing order of their ai score.  synsets are likely to be sometimes incorrect, and this of course introduces noise in our method.  The process is run twice, once for positivity and  once for negativity.  The last question to be answered is: why  PageRflow  ank? Are the characteristics of PageRank more  We now discuss the application of PageRank to  suitable to the problem of ranking synsets than other  ranking WordNet synsets by positivity and  negativrandom-walk algorithms? The answer is yes, since  ity. Our algorithm consists in the following steps:  it seems reasonable that:  1. If terms contained in synset sk occur in the  1. The graph G = hN, Li on which PageRank  glosses of many positive synsets, and if the  poswill be applied is generated. We define N to  itivity scores of these synsets are high, then it  be the set of all WordNet synsets; in WordNet  is likely that sk is itself positive (the same  hap2.0 there are 115,424 of them. We define L to  pens for negativity). This justifies the  summation of Equation 1.  2. If the gloss of a positive synset that contains at least one such term, without paying attention to  a term in synset sk also contains many other  terms, then this is a weaker indication that sk is  The corpus is divided into three parts:  itself positive (this justifies dividing by |F (j)|  in Equation 1).  Common: 110 synsets which all the evaluators  evaluated by working together, so as to align  3. The ranking resulting from the algorithm needs  their evaluation criteria.  to be biased in favour of a specific ORP; this  Group1: 496 synsets which were each  indejustifies the presence of the (1 )ei factor in  pendently evaluated by three evaluators.  Group2: 499 synsets which were each  indeThe fact that PageRank is the right random-walk  pendently evaluated by the other two  evaluaalgorithm for our application is also confirmed by  tors.  some experiments (not reported here for reasons of  space) we have run with slightly different variants of Each of these three parts has the same balance, in  the model (e.g., one in which we challenge intuition  terms of both parts of speech and ORPs, of  Micro2 above and thus avoid dividing by |F (j)| in  EquaWNOp as a whole. We obtain the positivity  (negation 1). These experiments have always returned  tivity) ranking from Micro-WNOp by averaging the  inferior results with respect to standard PageRank,  positivity (negativity) scores assigned by the  evaluathereby confirming the correctness of our intuitions.  tors of each group into a single score, and by sorting the synsets according to the resulting score. We use  Group1 as a validation set, i.e., in order to fine-tune our method, and Group2 as a test set, i.e., in order  The benchmark  to evaluate our method once all the parameters have  been optimized on the validation set.  To evaluate the quality of the rankings produced  The result of applying PageRank to the graph G  by our experiments we have used the Micro-WNOp  induced by the I relation, given a vector e of  incorpus (Cerini et al., 2007) as a benchmark3.  Microternal sources of positivity (negativity) score and a  WNOp consists in a set of 1,105 WordNet synsets,  value for the parameter, is a ranking of all the  each of which was manually assigned a triplet of  WordNet synsets in terms of positivity (negativity).  scores, one of positivity, one of negativity, one  By using different e vectors and different values of  of neutrality.  The evaluation was performed by  we obtain different rankings, whose quality we  five MSc students of linguistics, proficient  secondevaluate by comparing them against the ranking  oblanguage speakers of English. Micro-WNOp is  reptained from Micro-WNOp.  resentative of WordNet with respect to the different  parts of speech, in the sense that it contains synsets 4.2  The effectiveness measure  of the different parts of speech in the same  proporA ranking is a partial order on a set of objects  tions as in the entire WordNet. However, it is not  N = {o1 . . . o|N|}. Given a pair (oi, oj) of objects, representative of WordNet with respect to ORPs,  oi may precede oj (oi oj), it may follow oi (oi  since this would have brought about a corpus largely  oj), or it may be tied with oj (oi oj).  composed of neutral synsets, which would be pretty  To evaluate the rankings produced by PageRank  useless as a benchmark for testing automatically  dewe have used the p-normalized Kendall distance  rived lexical resources for opinion mining. It was  thus generated by randomly selecting 100 positive +  the Micro-WNOp rankings and those predicted by  100 negative + 100 neutral terms from the General  PageRank. A standard function for the evaluation of  Inquirer lexicon (see (Turney and Littman, 2003) for  rankings with ties, p is defined as  details) and including all the synsets that contained  where nd is the number of discordant pairs, i.e., null scores for all other synsets.  pairs of objects ordered one way in the gold  stanWe have also tested a more complex version of  dard and the other way in the prediction; nu is the  e, with ei scores obtained from release 1.0 of  Sentinumber of pairs ordered (i.e., not tied) in the gold  WordNet (Esuli and Sebastiani, 2006b)5. This latter  standard and tied in the prediction, and p is a  penalis a lexical resource in which each WordNet synset  ization to be attributed to each such pair; and Z is  is given a positivity score, a negativity score, and a a normalization factor (equal to the number of pairs  neutrality score. We produced an e vector (dubbed  that are ordered in the gold standard) whose aim is  e4) in which the score assigned to a synset is  proporto make the range of p coincide with the [0, 1]  intional to the positivity (negativity) score assigned to terval. Note that pairs tied in the gold standard are  it by SentiWordNet, and in which all entries sum up  not considered in the evaluation.  to 1. In a similar way we also produced a further e  The penalization factor is set to p = 1 , which  vector (dubbed e5) through the scores of a newer  reis equal to the probability that a ranking algorithm  lease of SentiWordNet (release 1.1), resulting from a  correctly orders the pair by random guessing; there  slight modification of the approach that had brought  is thus no advantage to be gained from either  ranabout release 1.0 (Esuli and Sebastiani, 2007b).  dom guessing or assigning ties between objects. For  PageRank is parametric on , which determines  a prediction which perfectly coincides with the gold  the balance between the contributions of the a(k 1)  standard p equals 0; for a prediction which is  exvector and the e vector. A value of = 0 makes  actly the inverse of the gold standard p equals 1.  the a(k) vector coincide with e, and corresponds to  discarding the contribution of the random-walk  algorithm. Conversely, setting = 1 corresponds  In order to produce a ranking by positivity  (negato discarding the contribution of e, and makes a(k)  tivity) we need to provide an e vector as input to  uniquely depend on the topology of the graph; the  PageRank. We have experimented with several  difresult is an unbiased ranking. The desirable cases  ferent definitions of e, each for both positivity and  are, of course, in between. As first hinted in  Secnegativity. For reasons of space, we only report  retion 4.1, we thus optimize the parameter on the  sults from the five most significant ones.  synsets in Group1, and then test the algorithm with  We have first tested a vector (hereafter dubbed  the optimal value of on the synsets in Group2.  e1) with all values uniformly set to 1 . This is the  All the 101 values of from 0.0 to 1.0 with a step of  e vector originally used in (Brin and Page, 1998)  .01 have been tested in the optimization phase.  Opfor Web page ranking, and brings about an unbiased  timization is performed anew for each experiment,  (that is, with respect to particular properties)  rankwhich means that different values of may be  evening of WordNet. Of course, it is not meant to be  tually selected for different e vectors.  used for ranking by positivity or negativity; we have  used it as a baseline in order to evaluate the impact  of property-biased vectors.  The results show that the use of PageRank in  comThe first sensible, albeit minimalistic, definition  bination with suitable vectors e almost always  imof e we have used (dubbed e2) is that of a  vecproves the ranking, sometimes significantly so, with  tor with uniform non-null ei scores assigned to the  respect to the original ranking embodied by the e  synsets that contain the adjective good (bad), and  null scores for all other synsets. A further, still fairly For positivity, the rankings produced using  minimalistic definition we have used (dubbed e3) is  PageRank and any of the vectors from e2 to e5 all  that of a vector with uniform non-null ei scores  asimprove on the original rankings, with a relative  imsigned to the synsets that contain at least one of the provement, measured as the relative decrease in p,  seven paradigmatic positive (negative) adjectives  positive, fortunate, correct, superior, and the seven negative used as seeds in (Turney and Littman, 2003)4, and  ones are bad, nasty, poor, negative, unfortunate, wrong, inferior.  4The seven positive adjectives are good, nice, excellent, 5http://sentiwordnet.isti.cnr.it/  ranging from 4.88% (e5) to 6.75% (e4). These Positivity  Negativity  rankings are also all better than the rankings  produced by using PageRank and the uniform-valued  vector e1, with a minimum relative improvement  of 5.04% (e3) and a maximum of 34.47% (e4).  This suggests that the key to good performance is  indeed a combination of positivity flow and internal  source of score.  For the negativity rankings, the performance of  both SentiWordNet-based vectors is still good,  producing a 4.31% (e4) and a 3.45% (e5)  improveTable 1: Values of p between predicted rankings  ment with respect to the original rankings.  The  minimalistic vectors (i.e., e2 and e3) are not as  each experiment the first line indicates the ranking  good as their positive counterparts.  The reason  obtained from the original e vector (before the  apseems to be that the generation of a ranking by  negplication of PageRank), while the second line  indiativity seems a somehow harder task than the  gencates the ranking obtained after the application of  eration of a ranking by positivity; this is also shown PageRank, with the relative improvement (a nega-by the results obtained with the uniform-valued  vective percentage indicates improvement).  tor e1, in which the application of PageRank  improves with respect to e1 for positivity but  deteriorates for negativity. However, against the baseline  constituted by the results obtained with the  uniformWe have investigated the applicability of a  randomvalued vector e1 for negativity, our rankings show  walk model to the problem of ranking synsets  aca relevant improvement, ranging from 8.56% (e2)  cording to positivity and negativity. However, we  conjecture that this model can be of more general  Our results are particularly significant for the e4  use, i.e., for the determination of other properties of vectors, derived by SentiWordNet 1.0, for a num-term senses, such as membership in a domain. This  ber of reasons. First, e4 brings about the best value  paper thus presents a proof-of-concept of the model,  of p obtained in all our experiments (.325 for  posand the results of experiments support our intuitions.  itivity, .284 for negativity). Second, the relative im-Also, we see this work as a proof of concept  provement with respect to e4 is the most marked  for the applicability of general random-walk  algoamong the various choices for e (6.75% for  positivrithms (and not just PageRank) to the determination  ity, 4.31% for negativity). Third, the improvement  of the semantic properties of synsets. In a more  reis obtained with respect to an already high-quality  cent paper (Esuli and Sebastiani, 2007a) we have  resource, obtained by the same techniques that, at  investigated a related random-walk model, one in  the term level, are still the best performers for  powhich, symmetrically to the intuitions of the model  larity detection on the widely used General Inquirer  presented in this paper, semantics flows from the  definiens to the definiendum; a metaphor that proves  Finally, observe that the fact that e4 outperforms  no less powerful than the one we have championed  all other choices for e (and e2 in particular) was not in this paper.  necessarily to be expected. In fact, SentiWordNet  1.0 was built by a semi-supervised learning method  that uses vectors e2 as its only initial training data.  Alina Andreevskaia and Sabine Bergler. 2006a. Mining Word-This paper thus shows that, starting from e2 as the  Net for fuzzy sentiment: Sentiment tag extraction from WordNet glosses. In Proceedings of the 11th Conference of only manually annotated data, the best results are  the European Chapter of the Association for Computational obtained neither by the semi-supervised method that  Linguistics (EACL'06), pages 209 216, Trento, IT.  generated SentiWordNet 1.0, nor by PageRank, but  by the concatenation of the former with the latter.  tag extraction from WordNet glosses.  In Proceedings of  the 5th Conference on Language Resources and Evaluation Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997.  Predicting the semantic orientation of adjectives. In Proceedings of the 35th Annual Meeting of the Association Monica Bianchini, Marco Gori, and Franco Scarselli. 2005. Infor Computational Linguistics (ACL'97), pages 174 181, side PageRank. ACM Transactions on Internet Technology, Madrid, ES.  Topic-sensitive PageRank:  Sergey Brin and Lawrence Page. 1998. The anatomy of a large-A context-sensitive ranking algorithm for Web search.  scale hypertextual Web search engine. Computer Networks IEEE Transactions on Knowledge and Data Engineering,  and ISDN Systems, 30(1-7):107 117.  Nancy Ide. 2006. Making senses: Bootstrapping sense-tagged WNOp: A gold standard for the evaluation of automati-lists of semantically-related words. In Proceedings of the cally compiled lexical resources for opinion mining. In An-7th International Conference on Computational Linguistics drea Sans , editor, Language resources and linguistic the-and Intelligent Text Processing (CICLING'06), pages 13 27, ory: Typology, second language acquisition, English linguis-Mexico City, MX.  tics. Franco Angeli Editore, Milano, IT. Forthcoming.  Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten Andrea Esuli and Fabrizio Sebastiani. 2005. Determining the De Rijke. 2004. Using WordNet to measure semantic ori-semantic orientation of terms through gloss analysis. In Pro-entation of adjectives. In Proceedings of the 4th Interna-ceedings of the 14th ACM International Conference on In-tional Conference on Language Resources and Evaluation formation and Knowledge Management (CIKM'05), pages  Soo-Min Kim and Eduard Hovy.  Determining the  Andrea Esuli and Fabrizio Sebastiani. 2006a. Determining sentiment of opinions.  In Proceedings of the 20th  Interterm subjectivity and term orientation for opinion mining. In national Conference on Computational Linguistics (COL-Proceedings of the 11th Conference of the European Chapter ING'04), pages 1367 1373, Geneva, CH.  of the Association for Computational Linguistics (EACL'06), pages 193 200, Trento, IT.  Rada Mihalcea. 2006. Random walks on text structures. In Proceedings of the 7th International Conference on Com-Andrea Esuli and Fabrizio Sebastiani. 2006b. SENTIWORD-putational Linguistics and Intelligent Text Processing (CI-NET: A publicly available lexical resource for opinion min-CLING'06), pages 249 262, Mexico City, MX.  ing. In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC'06), pages 417 422,  GenPero Subasic and Alison Huettner. 2001. Affect analysis of text ova, IT.  using fuzzy semantic typing. IEEE Transactions on Fuzzy Systems, 9(4):483 496.  walk models of term semantics: An application to opinion-Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005.  related properties.  Technical Report ISTI-009/2007,  IstiExtracting emotional polarity of words using spin model.  ation for Computational Linguistics (ACL'05), pages 133  Andrea Esuli and Fabrizio Sebastiani. 2007b. SENTIWORDNET: A high-coverage lexical resource for opinion mining.  Peter D. Turney and Michael L. Littman.   MeasurRonald Fagin, Ravi Kumar, Mohammad Mahdiany, D. Sivakumar, and Erik Veez. 2004. Comparing and aggregating rank-Janyce Wiebe and Rada Mihalcea. 2006. Word sense and sub-ings with ties. In  jectivity. In Proceedings of the 44th Annual Meeting of the Proceedings of ACM International Confer-Association for Computational Linguistics (ACL'06), pages ence on Principles of Database Systems (PODS'04), pages 47 58, Paris, FR.  Gregory Grefenstette, Yan Qu, David A. Evans, and James G.  Validating the coverage of lexical  resources for affect analysis and automatically classifying new words along semantic axes. In James G. Shanahan, Yan Qu, and Janyce Wiebe, editors, Computing Attitude and Affect in Text: Theories and Applications, pages 93 107. Springer, Heidelberg, DE.  Sanda H. Harabagiu, George A. Miller, and Dan I. Moldovan.  1999. WordNet 2: A morphologically and semantically en-hanced resource. In Proceedings of the ACL SIGLEX Work-shop on Standardizing Lexical Resources, pages 1 8, Col-lege Park, US. 