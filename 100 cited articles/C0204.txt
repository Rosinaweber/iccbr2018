 Early Results for  Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons  Department of Computer Science  University of Massachusetts Amherst  {mccallum,weili}@cs.umass.edu  Introduction  Feature induction methods still require the user to cre-Models for many natural language tasks benefit from the ate the building-block atomic features. Lexicon member-flexibility to use overlapping, non-independent features.  ship tests are particularly powerful features in natural lan-For example, the need for labeled data can be drastically guage tasks. The question is where to get lexicons that are reduced by taking advantage of domain knowledge in  relevant for the particular task at hand?  the form of word lists, part-of-speech tags, character n-This paper describes WebListing, a method that obtains grams, and capitalization patterns. While it is difficult to seeds for the lexicons from the labeled data, then uses the capture such inter-dependent features with a generative Web, HTML formatting regularities and a search engine probabilistic model, conditionally-trained models, such service to significantly augment those lexicons. For ex-as conditional maximum entropy models, handle them  ample, based on the appearance of Arnold Palmer in the well. There has been significant work with such mod-labeled data, we gather from the Web a large list of other els for greedy sequence modeling in NLP (Ratnaparkhi, golf players, including Tiger Woods (a phrase that is dif-1996; Borthwick et al., 1998).  ficult to detect as a name without a good lexicon).  Conditional Random Fields (CRFs) (Lafferty et al.,  We present results on the CoNLL-2003 named entity  2001) are undirected graphical models, a special case of recognition (NER) shared task, consisting of news arti-which correspond to conditionally-trained finite state ma-cles with tagged entities PERSON, LOCATION,  ORGANIchines. While based on the same exponential form as  ZATION and MISC. The data is quite complex; for exam-maximum entropy models, they have efficient procedures ple the English data includes foreign person names (such for complete, non-greedy finite-state inference and train-as Yayuk Basuki and Innocent Butare), a wide diversity of ing. CRFs have shown empirical successes recently in locations (including sports venues such as The Oval, and POS tagging (Lafferty et al., 2001), noun phrase segmen-rare location names such as Nirmal Hriday), many types tation (Sha and Pereira, 2003) and Chinese word segmen-of organizations (from company names such as 3M, to tation (McCallum and Feng, 2003).  acronyms for political parties such as KDP, to location Given these models' great flexibility to include a wide names used to refer to sports teams such as Cleveland), array of features, an important question that remains is and a wide variety of miscellaneous named entities (from what features should be used? For example, in some  software such as Java, to nationalities such as Basque, to cases capturing a word tri-gram is important, however, sporting competitions such as 1,000 Lakes Rally).  there is not sufficient memory or computation to include On this, our first attempt at a NER task, with just a few all word tri-grams. As the number of overlapping atomic person-weeks of effort and little work on development-features increases, the difficulty and importance of con-set error analysis, our method currently obtains overall structing only certain feature combinations grows.  English F1 of 84.04% on the test set by using CRFs, fea-This paper presents a feature induction method for  ture induction and Web-augmented lexicons. German F1  CRFs. Founded on the principle of constructing only  using very limited lexicons is 68.11%.  those feature conjunctions that significantly increase log-likelihood, the approach builds on that of Della Pietra et 2  Conditional Random Fields  al (1997), but is altered to work with conditional rather than joint probabilities, and with a mean-field approxi-Conditional Random Fields (CRFs) (Lafferty et al., 2001) mation and other additional modifications that improve are undirected graphical models used to calculate the con-efficiency specifically for a sequence model. In compari-ditional probability of values on designated output nodes son with traditional approaches, automated feature induc-given values assigned to other designated input nodes.  tion offers both improved accuracy and significant reducIn the special case in which the output nodes of the tion in feature count; it enables the use of richer, higher-graphical model are linked by edges in a linear chain, order Markov models, and offers more freedom to liber-CRFs make a first-order Markov independence  assumpally guess about which atomic features may be relevant tion, and thus can be understood as conditionally-trained  finite state machines (FSMs). In the remainder of this The Viterbi algorithm for finding the most likely state section we introduce the likelihood model, inference and sequence given the observation sequence can be corre-estimation procedures for CRFs.  spondingly modified from its HMM form.  Let o = ho1, o2, ...oT i be some observed input data sequence, such as a sequence of words in text in a doc-2.1  ument, (the values on n input nodes of the graphical The weights of a CRF, ={ , ...}, are set to maximize the model). Let S be a set of FSM states, each of which  conditional log-likelihood of labeled sequences in some is associated with a label, l L, (such as ORG). Let training set, D = {ho, li(1), ...ho, li(j), ...ho, li(N)}: s = hs1, s2, ...sT i be some sequence of states, (the val-N  ues on T output nodes). By the Hammersley-Clifford theX  orem, CRFs define the conditional probability of a state =  sequence given an input sequence to be  where the second sum is a Gaussian prior over parameters T  (with variance ) that provides smoothing to help cope (s|o) =  with sparsity in the training data.  When the training labels make the state sequence  unwhere Zo is a normalization factor over all state se-ambiguous (as they often do in practice), the likelihood quences, fk(st 1, st, o, t) is an arbitrary feature func-function in exponential models such as CRFs is  contion over its arguments, and k is a learned weight for vex, so there are no local maxima, and thus finding the each feature function. A feature function may, for exam-global optimum is guaranteed. It has recently been shown ple, be defined to have value 0 in most cases, and have that quasi-Newton methods, such as L-BFGS, are signifi-value 1 if and only if st 1 is state #1 (which may have cantly more efficient than traditional iterative scaling and label OTHER), and st is state #2 (which may have la-even conjugate gradient (Malouf, 2002; Sha and Pereira, bel LOCATION), and the observation at position t in o 2003). This method approximates the second-derivative is a word appearing in a list of country names. Higher  of the likelihood by keeping a running, finite-sized win-weights make their corresponding FSM transitions more dow of previous first-derivatives.  likely, so the weight k in this example should be pos-L-BFGS can simply be treated as a black-box  optiitive. More generally, feature functions can ask pow-mization procedure, requiring only that one provide the erfully arbitrary questions about the input sequence, in-first-derivative of the function to be optimized. Assum-cluding queries about previous words, next words, and ing that the training labels on instance j make its state conjunctions of all these, and fk( ) can range ... .  path unambiguous, let s(j) denote that path, and then the CRFs define the conditional probability of a label  first-derivative of the log-likelihood is  sequence based on total probability over the state sequences, P  (s|o), where l(s) is  the sequence of labels corresponding to the labels of the  Note that the normalization factor, Zo, is the sum  of the scores of all possible state sequences, Zo =  , and that  s ST  s  the number of state sequences is exponential in the input sequence length, T . In arbitrarily-structured CRFs, where Ck(s, o) is the count for feature k given s  calculating the normalization factor in closed form is and o, equal to PT  the sum of  intractable, but in linear-chain-structured CRFs, as in fk(st 1, st, o, t) values for all positions, t, in the se-forward-backward for hidden Markov models (HMMs), quence s. The first two terms correspond to the differ-the probability that a particular transition was taken be-ence between the empirical expected value of feature fk tween two CRF states at a particular position in the input and the model's expected value: (  E[fk] E [fk])N . The  sequence can be calculated efficiently by dynamic pro-last term is the derivative of the Gaussian prior.  gramming. We define slightly modified forward values,  Efficient Feature Induction for CRFs  t(si), to be the unnormalized probability of arriving in state si given the observations ho1, ...oti. We set 0(s) Typically the features, fk, are based on some number of equal to the probability of starting in each state s, and hand-crafted atomic observational tests (such as word is recurse:  capitalized or word is said , or word appears in  lexicon of country names), and a large collection of features X  t+1(s) =  t(s0) exp  is formed by making conjunctions of the atomic tests in s0  certain user-defined patterns; (for example, the conjunc-The backward procedure and the remaining details of  tions consisting of all tests at the current sequence po-Baum-Welch are defined similarly. Zo is then P  sition conjoined with all tests at the position one step s  T (s).  ahead specifically, for instance, current word is capi-and expand them after they are selected. (2) In many talized and next word is Inc ). There can easily be sequence problems, the great majority of the tokens are over 100,000 atomic tests (mostly based on tests for the correctly labeled even in the early stages of training. We identity of words in the vocabulary), and ten or more significantly gain efficiency by including in the gain cal-shifted-conjunction patterns resulting in several million culation only those tokens that are mislabeled by the cur-features (Sha and Pereira, 2003). This large number of rent model. Let {o(i) : i = 1...M } be those tokens, and features can be prohibitively expensive in memory and o(i) be the input sequence in which the ith error token computation; furthermore many of these features are ir-occurs at position t(i). Then algebraic simplification us-relevant, and others that are relevant are excluded.  ing these approximations and previous definitions gives In response, we wish to use just those time-shifted  conjunctions that will significantly improve performance.  We start with no features, and over several rounds of fea-M  exp g(s  ture induction: (1) consider a set of proposed new fea-log  tures, (2) select for inclusion those candidate features that o(i)( , g, )  will most increase the log-likelihood of the correct state M  path s(j), and (3) train weights for all features. The  proposed new features are based on the hand-crafted obser-i=1  vational tests consisting of singleton tests, and binary conjunctions of tests with each other and with features where Zo(i)( , g, ) (with non-bold o) is simply  currently in the model. The later allows arbitrary-length P  s  The optimal  valconjunctions to be built. The fact that not all singleton ues of the 's cannot be solved in closed form, but New-tests are included in the model gives the designer great ton's method finds them all in about 12 quick iterations.  freedom to use a very large variety of observational tests, There are two additional important modeling choices: and a large window of time shifts.  (1) Because we expect our models to still require sev-To consider the effect of adding a new feature, define eral thousands of features, we save time by adding many the new sequence model with additional feature, g, hav-of the features with highest gain each round of induction ing weight , to be  rather than just one; (including a few redundant features is not harmful). (2) Because even models with a small se-P  lect number of features can still severely overfit, we train (s|o) exp  t 1, st, o, t)  the model with just a few BFGS iterations (not to  convergence) before performing the next round of feature induction. Details are in (McCallum, 2003).  in the denominator is simply the additional portion of 4  Web-augmented Lexicons  normalization required to make the new function sum to 1 over all state sequences.  Some general-purpose lexicons, such a surnames and lo-Following (Della Pietra et al., 1997), we efficiently as-cation names, are widely available, however, many nat-sess many candidate features in parallel by assuming that ural language tasks will benefit from more task-specific the parameters on all included features remain fixed lexicons, such as lists of soccer teams, political parties, while estimating the gain, G(g), of a candidate feature, g, NGOs and English counties. Creating new lexicons en-based on the improvement in log-likelihood it provides, tirely by hand is tedious and time consuming.  Using a technique we call WebListing, we build lexi-G (g) = max G (g, ) = max L +g L .  cons automatically from HTML data on the Web.  Previous work has built lexicons from fixed corpora by deter-where L +g includes 2/2 2.  mining linguistic patterns for the context in which rele-In addition, we make this approach tractable for CRFs vant words appear (Collins and Singer, 1999; Jones et al., with two further reasonable and mutually-supporting ap-1999). Rather than mining a small corpus, we gather data proximations specific to CRFs. (1) We avoid dynamic  from nearly the entire Web; rather than relying on fragile programming for inference in the gain calculation with linguistic context patterns, we leverage robust formatting a mean-field approximation, removing the dependence  regularities on the Web. WebListing finds co-occurrences among states. (Thus we transform the gain from a se-of seed terms that appear in an identical HTML format-quence problem to a token classification problem. How-ting pattern, and augments a lexicon with other terms on ever, the original posterior distribution over states given the page that share the same formatting. Our current im-each token, P (s|o) = t(s|o) t+1(s|o)/Zo, is still  plementation uses GoogleSets, which we understand to calculated by dynamic programming without approxima-be a simple implementation of this approach based on us-tion.) Furthermore, we can calculate the gain of aggre-ing HTML list items as the formatting regularity. We are gate features irrespective of transition source, g(st, o, t), currently building a more sophisticated replacement.  English devel.  Precision  To perform named entity extraction on the news articles MISC  in the CoNLL-2003 English shared task, several families ORG  of features are used, all time-shifted by -2, -1, 0, 1, 2: (a) PER  the word itself, (b) 16 character-level regular expressions, Overall  mostly concerning capitalization and digit patterns, such as A, A+, Aa+, Aa+Aa*, A., D+, where A, a and D indi-English test  Precision  cate the regular expressions [A-Z], [a-z] and [0-9], LOC  (c) 8 lexicons entered by hand, such as honorifics, days MISC  and months, (d) 15 lexicons obtained from specific web ORG  sites, such as countries, publicly-traded companies, sur-PER  names, stopwords, and universities, (e) 25 lexicons ob-Overall  tained by WebListing (including people names, organizations, NGOs and nationalities), (f) all the above tests German devel.  Precision  with prefix firstmention from any previous duplicate of LOC  the current word, (if capitalized). A small amount of MISC  hand-filtering was performed on some of the  WebListing lexicons. Since GoogleSets' support for non-English PER  is severely limited, only 5 small lexicons were used for Overall  German; but character and tri-grams were added.  A Java-implemented, first-order CRF was trained for  German test  Precision  about 12 hours on a 1GHz Pentium with a Gaussian prior LOC  variance of 0.5, inducing 1000 or fewer features (down MISC  to a gain threshold of 5.0) each round of 10 iterations of ORG  L-BFGS. Candidate conjunctions are limited to the 1000  atomic and existing features with highest gain. Perfor-Overall  mance results for each of the entity classes can be found in Figure 1. The model achieved an overall F1 of 84.04%  Table 1: English and German named entity extraction.  on the English test set using 6423 features. (Using a set of fixed conjunction patterns instead of feature induction results in F1 73.34%, with about 1 million features; trial-Transactions on Pattern Analysis and Machine Intelligence, and-error tuning the fixed patterns would likely improve 19(4):380 393.  this.) Accuracy gains are expected from experimentation Rosie Jones, Andrew McCallum, Kamal Nigam, and Ellen with the induction parameters and improved WebListing.  Riloff. 1999. Bootstrapping for Text Learning Tasks. In IJCAI-99 Workshop on Text Mining: Foundations, Tech-Acknowledgments  We thank John Lafferty, Fernando Pereira, Andres Corrada-John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.  Conditional Random Fields: Probabilistic Models for Seg-Emmanuel, Drew Bagnell and Guy Lebanon, for helpful  menting and Labeling Sequence Data. In Proc. ICML.  input.  This work was supported in part by the Center  Robert Malouf. 2002. A comparison of algorithms for max-for Intelligent Information Retrieval, SPAWARSYSCEN-SD  imum entropy parameter estimation. In Sixth Workshop on grant numbers N66001-99-1-8912 and N66001-02-1-8903, Ad-Computational Language Learning (CoNLL-2002).  vanced Research and Development Activity under contract Andrew McCallum and Fang-Fang Feng.  Chinese  number MDA904-01-C-0984, and DARPA contract  F30602Word Segmentation with Conditional Random Fields and In-01-2-0566.  tegrated Domain Knowledge. In Unpublished Manuscript.  Andrew McCallum. 2003. Efficiently Inducing Features of References  Conditional Random Fields. In Nineteenth Conference on Uncertainty in Artificial Intelligence (UAI03). (Submitted).  Exploiting diverse knowledge sources via maximum entropy Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for in named entity recognition.  In Proceedings of the Sixth  Part-of-Speech Tagging. In Eric Brill and Kenneth Church, Workshop on Very Large Corpora, Association for Compu-editors, Proceedings of the Conference on Empirical Meth-tational Linguistics.  ods in Natural Language Processing, pages 133 142. Association for Computational Linguistics.  M. Collins and Y. Singer.  named entity classification. In Proceedings of the Joint SIG-Fei Sha and Fernando Pereira. 2003. Shallow Parsing with DAT Conference on Empirical Methods in Natural Language Conditional Random Fields. In Proceedings of Human Lan-Processing and Very Large Corpora.  guage Technology, NAACL.  Stephen Della Pietra, Vincent J. Della Pietra, and John D. Lafferty. 1997. Inducing Features of Random Fields. IEEE 