 Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 417-424.  Thumbs Up or Thumbs Down? Semantic Orientation Applied to  Unsupervised Classification of Reviews  Institute for Information Technology  National Research Council of Canada  case, Google1 reports about 5,000 matches. It  would be useful to know what fraction of these  matches recommend Akumal as a travel  destinaThis paper presents a simple unsupervised  tion. With an algorithm for automatically  classifylearning algorithm for classifying reviews  ing a review as thumbs up or thumbs down , it  as recommended (thumbs up) or not  recwould be possible for a search engine to report  ommended (thumbs down). The  classifisuch summary statistics. This is the motivation for  cation of a review is predicted by the  the research described here. Other potential  appliaverage semantic orientation of the  cations include recognizing flames (abusive  phrases in the review that contain  adjecnewsgroup messages) (Spertus, 1997) and  developtives or adverbs. A phrase has a positive  ing new kinds of search tools (Hearst, 1992).  semantic orientation when it has good  asIn this paper, I present a simple unsupervised  sociations (e.g., subtle nuances ) and a  learning algorithm for classifying a review as  recnegative semantic orientation when it has  ommended or not recommended. The algorithm  bad associations (e.g., very cavalier ). In  takes a written review as input and produces a  this paper, the semantic orientation of a  classification as output. The first step is to use a  phrase is calculated as the mutual  inforpart-of-speech tagger to identify phrases in the  inmation between the given phrase and the  put text that contain adjectives or adverbs (Brill,  word excellent minus the mutual  1994). The second step is to estimate the semantic  information between the given phrase and  orientation of each extracted phrase  (Hatzivassithe word poor . A review is classified as  loglou & McKeown, 1997). A phrase has a  posirecommended if the average semantic  oritive semantic orientation when it has good  entation of its phrases is positive. The  alassociations (e.g., romantic ambience ) and a  gorithm achieves an average accuracy of  negative semantic orientation when it has bad  as74% when evaluated on 410 reviews from  sociations (e.g., horrific events ). The third step is  Epinions, sampled from four different  to assign the given review to a class, recommended  domains (reviews of automobiles, banks,  or not recommended, based on the average  semanmovies, and travel destinations). The  actic orientation of the phrases extracted from the  recuracy ranges from 84% for automobile  view. If the average is positive, the prediction is  reviews to 66% for movie reviews.  that the review recommends the item it discusses.  Otherwise, the prediction is that the item is not  recommended.  Introduction  The PMI-IR algorithm is employed to estimate  the semantic orientation of a phrase (Turney,  If you are considering a vacation in Akumal,  Mex2001). PMI-IR uses Pointwise Mutual Information  ico, you might go to a search engine and enter the  (PMI) and Information Retrieval (IR) to measure  query Akumal travel review . However, in this  the similarity of pairs of words or phrases. The  semantic orientation of a given phrase is calculated  by comparing its similarity to a positive reference  However, although an isolated adjective may  indiword ( excellent ) with its similarity to a negative  cate subjectivity, there may be insufficient context  reference word ( poor ). More specifically, a  to determine semantic orientation. For example,  phrase is assigned a numerical rating by taking the  the adjective unpredictable may have a negative  mutual information between the given phrase and  orientation in an automotive review, in a phrase  the word excellent and subtracting the mutual  such as unpredictable steering , but it could have  information between the given phrase and the word  a positive orientation in a movie review, in a  poor . In addition to determining the direction of  phrase such as unpredictable plot . Therefore the  the phrase's semantic orientation (positive or  negaalgorithm extracts two consecutive words, where  tive, based on the sign of the rating), this numerical  one member of the pair is an adjective or an adverb  rating also indicates the strength of the semantic  and the second provides context.  orientation (based on the magnitude of the  numFirst a part-of-speech tagger is applied to the  ber). The algorithm is presented in Section 2.  review (Brill, 1994).3 Two consecutive words are  Hatzivassiloglou and McKeown (1997) have  extracted from the review if their tags conform to  also developed an algorithm for predicting  semanany of the patterns in Table 1. The JJ tags indicate  tic orientation. Their algorithm performs well, but  adjectives, the NN tags are nouns, the RB tags are  it is designed for isolated adjectives, rather than  adverbs, and the VB tags are verbs.4 The second  phrases containing adjectives or adverbs. This is  pattern, for example, means that two consecutive  discussed in more detail in Section 3, along with  words are extracted if the first word is an adverb  other related work.  and the second word is an adjective, but the third  The classification algorithm is evaluated on 410  word (which is not extracted) cannot be a noun.  reviews from Epinions2, randomly sampled from  NNP and NNPS (singular and plural proper nouns)  are avoided, so that the names of the items in the  banks, movies, and travel destinations. Reviews at  review cannot influence the classification.  Epinions are not written by professional writers;  Table 1. Patterns of tags for extracting two-word  any person with a Web browser can become a  phrases from reviews.  member of Epinions and contribute a review. Each  of these 410 reviews was written by a different  auFirst Word  Second Word  Third Word  thor. Of these reviews, 170 are not recommended  (Not Extracted)  NN or NNS  and the remaining 240 are recommended (these  2. RB, RBR, or JJ  classifications are given by the authors). Always  guessing the majority class would yield an  accuracy of 59%. The algorithm achieves an average  4. NN or NNS JJ  accuracy of 74%, ranging from 84% for  automo5. RB, RBR, or VB, VBD,  bile reviews to 66% for movie reviews. The  experimental results are given in Section 4.  The second step is to estimate the semantic  oriThe interpretation of the experimental results,  entation of the extracted phrases, using the PMI-IR  the limitations of this work, and future work are  algorithm. This algorithm uses mutual information  discussed in Section 5. Potential applications are  as a measure of the strength of semantic  associaoutlined in Section 6. Finally, conclusions are  pretion between two words (Church & Hanks, 1989).  sented in Section 7.  PMI-IR has been empirically evaluated using 80  synonym test questions from the Test of English as  Classifying Reviews  a Foreign Language (TOEFL), obtaining a score of  The first step of the algorithm is to extract phrases  74% (Turney, 2001). For comparison, Latent  Secontaining adjectives or adverbs. Past work has  mantic Analysis (LSA), another statistical measure  demonstrated that adjectives are good indicators of  of word association, attains a score of 64% on the  4 See Santorini (1995) for a complete description of the tags.  some minor algebraic manipulation, if  cooccurrence is interpreted as NEAR:  The Pointwise Mutual Information (PMI)  beSO( phrase) =  1 and word 2, is defined as  follows (Church & Hanks, 1989):  hits( phrase NEAR excellent ) hits( poor )  hits( phrase NEAR poor ) hits( excellent )  Equation (3) is a log-odds ratio (Agresti, 1996).  To avoid division by zero, I added 0.01 to the hits.  Here, p( word 1 & word 2) is the probability that I also skipped phrase when both hits( phrase  word 1 and word 2 co-occur. If the words are  statistiNEAR excellent ) and hits( phrase NEAR  cally independent, then the probability that they  poor ) were (simultaneously) less than four.  co-occur is given by the product p( word 1)  These numbers (0.01 and 4) were arbitrarily  chop( word 2). The ratio between p( word 1 & word 2) and sen. To eliminate any possible influence from the  p( word 1) p( word 2) is thus a measure of the degree  testing data, I added AND (NOT host:epinions)  of statistical dependence between the words. The  to every query, which tells AltaVista not to include  log of this ratio is the amount of information that  the Epinions Web site in its searches.  we acquire about the presence of one of the words  The third step is to calculate the average  semanwhen we observe the other.  tic orientation of the phrases in the given review  The Semantic Orientation (SO) of a phrase,  and classify the review as recommended if the  avphrase, is calculated here as follows:  erage is positive and otherwise not recommended.  Table 2 shows an example for a recommended  PMI( phrase, poor )  review and Table 3 shows an example for a not  The reference words excellent and poor were  recommended review. Both are reviews of the  chosen because, in the five star review rating  sysBank of America. Both are in the collection of 410  tem, it is common to define one star as poor and  reviews from Epinions that are used in the  experifive stars as excellent . SO is positive when  ments in Section 4.  phrase is more strongly associated with excellent  Table 2. An example of the processing of a review that  and negative when phrase is more strongly  associthe author has classified as recommended. 6  ated with poor .  Extracted Phrase  Part-of-Speech  PMI-IR estimates PMI by issuing queries to a  search engine (hence the IR in PMI-IR) and noting  the number of hits (matching documents). The  follow fees  lowing experiments use the AltaVista Advanced  local branch  Search engine5, which indexes approximately 350  million web pages (counting only those pages that  online service  are in English). I chose AltaVista because it has a  NEAR operator. The AltaVista NEAR operator  constrains the search to documents that contain the  well other  words within ten words of one another, in either  inconveniently  order. Previous work has shown that NEAR  perlocated  other bank  forms better than AND when measuring the  strength of semantic association between words  Average Semantic Orientation  (Turney, 2001).  Let hits( query) be the number of hits returned,  given the query query. The following estimate of  SO can be derived from equations (1) and (2) with  6 The semantic orientation in the following tables is calculated  using the natural logarithm (base e), rather than base 2. The  natural log is more common in the literature on log-odds ratio.  Since all logs are equivalent up to a constant factor, it makes  no difference for the algorithm.  Table 3. An example of the processing of a review that  All conjunctions of adjectives are extracted  the author has classified as not recommended.  from the given corpus.  Extracted Phrase  Part-of-Speech  A supervised learning algorithm combines  multiple sources of evidence to label pairs of  adjectives as having the same semantic  orientaclever tricks  tion or different semantic orientations. The  reprograms such  sult is a graph where the nodes are adjectives  and links indicate sameness or difference of  unethical practices  low funds  A clustering algorithm processes the graph  structure to produce two subsets of adjectives,  other problems  such that links across the two subsets are  probably wondering RB VBG  mainly different-orientation links, and links  inside a subset are mainly same-orientation links.  other bank  Since it is known that positive adjectives  tend to be used more frequently than negative  adjectives, the cluster with the higher average  cool thing  frequency is classified as having positive  severy handy  This algorithm classifies adjectives with accuracies  Average Semantic Orientation  ranging from 78% to 92%, depending on the  amount of training data that is available. The  algo3 Related Work  rithm can go beyond a binary positive-negative  distinction, because the clustering algorithm (step 3  This work is most closely related to  Hatzivassiabove) can produce a goodness-of-fit measure  loglou and McKeown's (1997) work on predicting  that indicates how well an adjective fits in its  asthe semantic orientation of adjectives. They note  signed cluster.  that there are linguistic constraints on the semantic  Although they do not consider the task of  clasorientations of adjectives in conjunctions. As an  sifying reviews, it seems their algorithm could be  example, they present the following three  senplugged into the classification algorithm presented  tences (Hatzivassiloglou & McKeown, 1997):  in Section 2, where it would replace PMI-IR and  The tax proposal was simple and  wellequation (3) in the second step. However, PMI-IR  received by the public.  is conceptually simpler, easier to implement, and it  can handle phrases and adverbs, in addition to  isoThe tax proposal was simplistic but  welllated adjectives.  received by the public.  As far as I know, the only prior published work  (*) The tax proposal was simplistic and  on the task of classifying reviews as thumbs up or  well-received by the public.  down is Tong's (2001) system for generating  senThe third sentence is incorrect, because we use  timent timelines. This system tracks online  discus and with adjectives that have the same semantic  sions about movies and displays a plot of the  orientation ( simple and well-received are both  number of positive sentiment and negative  sentipositive), but we use but with adjectives that  ment messages over time. Messages are classified  have different semantic orientations ( simplistic  by looking for specific phrases that indicate the  is negative).  sentiment of the author towards the movie (e.g.,  Hatzivassiloglou and McKeown (1997) use a  great acting , wonderful visuals , terrible  four-step supervised learning algorithm to infer the  score , uneven editing ). Each phrase must be  semantic orientation of adjectives from constraints  manually added to a special lexicon and manually  on conjunctions:  tagged as indicating positive or negative sentiment.  The lexicon is specific to the domain (e.g., movies)  and must be built anew for each new domain. The  tion to recommended and not recommended,  Epincompany Mindfuleye7 offers a technology called  ions reviews are classified using the five star rating  Lexant that appears similar to Tong's (2001)  system. The third column shows the correlation  between the average semantic orientation and the  Other related work is concerned with  determinnumber of stars assigned by the author of the  reing subjectivity (Hatzivassiloglou & Wiebe, 2000;  view. The results show a strong positive  correlaWiebe, 2000; Wiebe et al., 2001). The task is to  tion between the average semantic orientation and  distinguish sentences that present opinions and  the author's rating out of five stars.  evaluations from sentences that objectively present  Table 4. A summary of the corpus of reviews.  factual information (Wiebe, 2000). Wiebe et al.  (2001) list a variety of potential applications for  Domain of Review  Number of  automated subjectivity tagging, such as  recognizPhrases per  ing flames (Spertus, 1997), classifying email,  recognizing speaker role in radio broadcasts, and  Honda Accord  mining reviews. In several of these applications,  the first step is to recognize that the text is  subjective and then the natural second step is to  deterBank of America  mine the semantic orientation of the subjective  Washington Mutual  text. For example, a flame detector cannot merely  detect that a newsgroup message is subjective, it  The Matrix  must further detect that the message has a negative  Pearl Harbor  semantic orientation; otherwise a message of praise  Travel Destinations  could be classified as a flame.  Hearst (1992) observes that most search  engines focus on finding documents on a given topic,  but do not allow the user to specify the  directionalTable 5. The accuracy of the classification and the  cority of the documents (e.g., is the author in favor of,  relation of the semantic orientation with the star rating.  neutral, or opposed to the event or item discussed  Domain of Review  Accuracy  Correlation  in the document?). The directionality of a  document is determined by its deep argumentative  Honda Accord  structure, rather than a shallow analysis of its  adjectives. Sentences are interpreted metaphorically  in terms of agents exerting force, resisting force,  Bank of America  and overcoming resistance. It seems likely that  Washington Mutual 81.67 %  there could be some benefit to combining shallow  and deep analysis of the text.  The Matrix  Pearl Harbor  Travel Destinations  Table 4 describes the 410 reviews from Epinions  that were used in the experiments. 170 (41%) of  the reviews are not recommended and the  remainDiscussion of Results  ing 240 (59%) are recommended. Always guessing  the majority class would yield an accuracy of 59%.  A natural question, given the preceding results, is  The third column shows the average number of  what makes movie reviews hard to classify? Table  phrases that were extracted from the reviews.  6 shows that classification by the average SO tends  Table 5 shows the experimental results. Except  to err on the side of guessing that a review is not  for the travel reviews, there is surprisingly little  recommended, when it is actually recommended.  variation in the accuracy within a domain. In  addiThis suggests the hypothesis that a good movie  will often contain unpleasant scenes (e.g., violence,  death, mayhem), and a recommended movie  review may thus have its average semantic  orientaBut it is possible that a better SO estimator could  tion reduced if it contains descriptions of these  unproduce significantly better classifications.  pleasant scenes. However, if we add a constant  Table 7. Sample phrases from misclassified reviews.  value to the average SO of the movie reviews, to  compensate for this bias, the accuracy does not  The Matrix  improve. This suggests that, just as positive  reAuthor's Rating:  recommended (5 stars)  views mention unpleasant things, so negative  re-0.219 ( not recommended)  Sample Phrase:  more evil [RBR JJ]  views often mention pleasant scenes.  SO of Sample  Table 6. The confusion matrix for movie classifications.  Phrase:  Context of Sample  The slow, methodical way he  Author's Classification  Phrase:  spoke. I loved it! It made him  Thumbs  Thumbs  Down  Pearl Harbor  Author's Rating:  recommended (5 stars)  Negative  -0.378 ( not recommended)  Sample Phrase:  sick feeling [JJ NN]  Table 7 shows some examples that lend support  SO of Sample  Phrase:  to this hypothesis. For example, the phrase more  Context of Sample  During this period I had a sick  evil does have negative connotations, thus an SO  Phrase:  feeling, knowing what was  of -4.384 is appropriate, but an evil character does  coming, knowing what was  not make a bad movie. The difficulty with movie  part of our history.  reviews is that there are two aspects to a movie, the  The Matrix  events and actors in the movie (the elements of the  Author's Rating:  not recommended (2 stars)  movie), and the style and art of the movie (the  0.177 ( recommended)  movie as a gestalt; a unified whole). This is likely  Sample Phrase:  very talented [RB JJ]  also the explanation for the lower accuracy of the  SO of Sample  Cancun reviews: good beaches do not necessarily  Phrase:  add up to a good vacation. On the other hand, good  Context of Sample  Well as usual Keanu Reeves is  Phrase:  nothing special, but  surprisautomotive parts usually do add up to a good  ingly, the very talented  Laurautomobile and good banking services add up to a  ence Fishbourne is not so good  good bank. It is not clear how to address this issue.  either, I was surprised.  Future work might look at whether it is possible to  Pearl Harbor  tag sentences as discussing elements or wholes.  Author's Rating:  not recommended (3 stars)  Another area for future work is to empirically  0.015 ( recommended)  compare PMI-IR and the algorithm of  HatzivassiSample Phrase:  loglou and McKeown (1997). Although their  algoSO of Sample  rithm does not readily extend to two-word phrases,  Phrase:  I have not yet demonstrated that two-word phrases  Context of Sample  Anyone who saw the trailer in  Phrase:  the theater over the course of  are necessary for accurate classification of reviews.  the last year will never forget  On the other hand, it would be interesting to  evaluthe images of Japanese war  ate PMI-IR on the collection of 1,336 hand-labeled  planes swooping out of the  adjectives that were used in the experiments of  blue skies, flying past the  Hatzivassiloglou and McKeown (1997). A related  children playing baseball, or  question for future work is the relationship of  acthe truly remarkable shot of a  curacy of the estimation of semantic orientation at  bomb falling from an enemy  the level of individual phrases to accuracy of  replane into the deck of the USS  view classification. Since the review classification  is based on an average, it might be quite resistant  Equation (3) is a very simple estimator of  seto noise in the SO estimate for individual phrases.  mantic orientation. It might benefit from more  sophisticated statistical analysis (Agresti, 1996). One  possibility is to apply a statistical significance test  troduction, one application is to provide summary  to each estimated SO. There is a large statistical  statistics for search engines. Given the query  literature on the log-odds ratio, which might lead  Akumal travel review , a search engine could  reto improved results on this task.  port, There are 5,000 hits, of which 80% are  This paper has focused on unsupervised  classithumbs up and 20% are thumbs down. The search  fication, but average semantic orientation could be  results could be sorted by average semantic  oriensupplemented by other features, in a supervised  tation, so that the user could easily sample the most  classification system. The other features could be  extreme reviews. Similarly, a search engine could  based on the presence or absence of specific  allow the user to specify the topic and the rating of  words, as is common in most text classification  the desired reviews (Hearst, 1992).  work. This could yield higher accuracies, but the  Preliminary experiments indicate that semantic  intent here was to study this one feature in  isolaorientation is also useful for summarization of  retion, to simplify the analysis, before combining it  views. A positive review could be summarized by  with other features.  picking out the sentence with the highest positive  Table 5 shows a high correlation between the  semantic orientation and a negative review could  average semantic orientation and the star rating of  be summarized by extracting the sentence with the  a review. I plan to experiment with ordinal  classilowest negative semantic orientation.  fication of reviews in the five star rating system,  Epinions asks its reviewers to provide a short  using the algorithm of Frank and Hall (2001). For  description of pros and cons for the reviewed item.  ordinal classification, the average semantic  orientaA pro/con summarizer could be evaluated by  tion would be supplemented with other features in  measuring the overlap between the reviewer's pros  a supervised classification system.  and cons and the phrases in the review that have  A limitation of PMI-IR is the time required to  the most extreme semantic orientation.  send queries to AltaVista. Inspection of Equation  Another potential application is filtering  (3) shows that it takes four queries to calculate the  flames for newsgroups (Spertus, 1997). There  semantic orientation of a phrase. However, I  could be a threshold, such that a newsgroup  mescached all query results, and since there is no need  sage is held for verification by the human  moderato recalculate hits( poor ) and hits( excellent ) for  tor when the semantic orientation of a phrase drops  every phrase, each phrase requires an average of  below the threshold. A related use might be a tool  slightly less than two queries. As a courtesy to  for helping academic referees when reviewing  AltaVista, I used a five second delay between  quejournal and conference papers. Ideally, referees are  ries.8 The 410 reviews yielded 10,658 phrases, so  unbiased and objective, but sometimes their  critithe total time required to process the corpus was  cism can be unintentionally harsh. It might be  posroughly 106,580 seconds, or about 30 hours.  sible to highlight passages in a draft referee's  This might appear to be a significant limitation,  report, where the choice of words should be  modibut extrapolation of current trends in computer  fied towards a more neutral tone.  memory capacity suggests that, in about ten years,  Tong's (2001) system for detecting and  trackthe average desktop computer will be able to easily  ing opinions in on-line discussions could benefit  store and search AltaVista's 350 million Web  from the use of a learning algorithm, instead of (or  pages. This will reduce the processing time to less  in addition to) a hand-built lexicon. With  autothan one second per review.  mated review rating (opinion rating), advertisers  could track advertising campaigns, politicians  6 Applications  could track public opinion, reporters could track  public response to current events, stock traders  There are a variety of potential applications for  could track financial opinions, and trend analyzers  automated review rating. As mentioned in the  incould track entertainment and technology trends.  8 This line of research depends on the good will of the major  search engines. For a discussion of the ethics of Web robots,  This paper introduces a simple unsupervised  learnthe proposed extended standard for robot exclusion would be  ing algorithm for rating a review as thumbs up or  down. The algorithm has three steps: (1) extract  pean Conference on Machine Learning (pp.  145phrases containing adjectives or adverbs, (2)  estimate the semantic orientation of each phrase, and  Hatzivassiloglou, V., & McKeown, K.R. 1997.  Predict(3) classify the review based on the average  seing the semantic orientation of adjectives.  Proceedmantic orientation of the phrases. The core of the  ings of the 35th Annual Meeting of the ACL and the  algorithm is the second step, which uses PMI-IR to  8th Conference of the European Chapter of the ACL  calculate semantic orientation (Turney, 2001).  (pp. 174-181). New Brunswick, NJ: ACL.  In experiments with 410 reviews from  EpinHatzivassiloglou, V., & Wiebe, J.M. 2000. Effects of  ions, the algorithm attains an average accuracy of  adjective orientation and gradability on sentence  sub74%. It appears that movie reviews are difficult to  jectivity. Proceedings of 18th International  Conferclassify, because the whole is not necessarily the  ence on Computational Linguistics. New Brunswick,  sum of the parts; thus the accuracy on movie  reviews is about 66%. On the other hand, for banks  Hearst, M.A. 1992. Direction-based text interpretation  and automobiles, it seems that the whole is the sum  as an information access refinement. In P. Jacobs  of the parts, and the accuracy is 80% to 84%.  (Ed.), Text-Based Intelligent Systems: Current  ReTravel reviews are an intermediate case.  search and Practice in Information Extraction and  Previous work on determining the semantic  orientation of adjectives has used a complex  algoates.  rithm that does not readily extend beyond isolated  Landauer, T.K., & Dumais, S.T. 1997. A solution to  adjectives to adverbs or longer phrases  (HatzivassiPlato's problem: The latent semantic analysis theory  loglou and McKeown, 1997). The simplicity of  of the acquisition, induction, and representation of  PMI-IR may encourage further work with semantic  knowledge. Psychological Review, 104, 211-240.  Santorini, B. 1995. Part-of-Speech Tagging Guidelines  The limitations of this work include the time  for the Penn Treebank Project (3rd revision, 2nd  required for queries and, for some applications, the  printing). Technical Report, Department of Computer  level of accuracy that was achieved. The former  and Information Science, University of Pennsylvania.  difficulty will be eliminated by progress in  hardSpertus, E. 1997. Smokey: Automatic recognition of  ware. The latter difficulty might be addressed by  hostile messages. Proceedings of the Conference on  using semantic orientation combined with other  Innovative Applications of Artificial Intelligence (pp.  features in a supervised classification algorithm.  1058-1065). Menlo Park, CA: AAAI Press.  Tong, R.M. 2001. An operational system for detecting  Acknowledgements  and tracking opinions in on-line discussions. Working  Thanks to Joel Martin and Michael Littman for  Notes of the ACM SIGIR 2001 Workshop on  Operahelpful comments.  tional Text Classification (pp. 1-6). New York, NY:  ACM.  Turney, P.D. 2001. Mining the Web for synonyms:  PMI-IR versus LSA on TOEFL. Proceedings of the  Twelfth European Conference on Machine Learning  analysis. New York: Wiley.  Brill, E. 1994. Some advances in transformation-based  Wiebe, J.M. 2000. Learning subjective adjectives from  part of speech tagging. Proceedings of the Twelfth  corpora. Proceedings of the 17th National  ConferNational Conference on Artificial Intelligence (pp.  ence on Artificial Intelligence. Menlo Park, CA:  722-727). Menlo Park, CA: AAAI Press.  AAAI Press.  Church, K.W., & Hanks, P. 1989. Word association  norms, mutual information and lexicography.  ProT. 2001. A corpus study of evaluative and  speculaceedings of the 27th Annual Conference of the ACL  tive language. Proceedings of the Second ACL SIG  on Dialogue Workshop on Discourse and Dialogue.  Frank, E., & Hall, M. 2001. A simple approach to  ordinal classification. Proceedings of the Twelfth 