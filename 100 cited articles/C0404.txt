 Proceedings of the Conference on Empirical Methods in Natural  Language Processing (EMNLP), Philadelphia, July 2002, pp. 1-8.  Association for Computational Linguistics.  riminativeTrainingMethodsforHiddenMarkovModels: TheoryandExperimentswithPer  eptronAlgorithms  haelCollins  AT&TLabs-Resear  h,FlorhamPark,NewJersey.  ollinsresear  ribenewalgorithmsfortrain­ingtaggingmodels,asanalternative tomaximum­entropymodelsor  ondi­tionalrandomfelds(CRFs).Theal­gorithmsrelyonViterbide  odingof trainingexamples,  ombinedwithsim­pleadditiveupdates.Wedes  ribethe­oryjustifyingthealgorithmsthrough amodif  ationoftheproofof  eoftheper  eptronalgorithmfor  ationproblems.Wegiveexper­imentalresultsonpart­of­spee  hunking,in both  asesshowingimprovementsover resultsforamaximum­entropytagger.  tion  efortaggingproblemsin NaturalLanguagePro  essing:forexamplesee (Ratnaparkhi96)fortheiruseonpart­of­spee  Callumetal.2000)fortheir useonaFAQsegmentationtask.MEmodels havetheadvantageofbeingquitefexibleinthe featuresthat  orporatedinthemodel. However,re  alandexperimentalre­sultsin(Lafertyetal.2001)havehighlighted problemswiththeparameterestimationmethod forMEmodels.Inresponsetotheseproblems, theydes  ribealternativeparameterestimation methodsbasedonConditionalMarkovRandom Fields(CRFs).(Lafertyetal.2001)giveexper­imentalresultssuggestingthatCRFs  antlybetterthanMEmodels.  Inthispaperwedes  ribeparameterestima­tionalgorithmswhi  harenaturalalternativesto CRFs.Thealgorithmsarebasedontheper  hapire99).Thesealgorithms havebeenshownby(Freund&S  hapire99)to be  ompetitivewithmodernlearningalgorithms su  hassupportve  hines;however,they havepreviouslybeenappliedmainlyto  ationtasks,anditisnotentirely  learhowthe algorithms  anbe  Thispaperdes  ribesvariantsoftheper  ep­tronalgorithmfortaggingproblems.Theal­gorithmsrelyonViterbide  odingoftraining examples,  ombinedwithsimpleadditiveup­dates.Wedes  ribetheoryjustifyingthealgo­rithmthroughamodif  ationoftheproofof  eoftheper  eptronalgorithmfor  ationproblems.Wegiveexperimentalresults onpart­of­spee  hunking,inboth  asesshowingimprovements overresultsforamaximum­entropytagger(a 11.9%relativeredu  tioninerrorforPOStag­ging,a5.1%relativeredu  hunking).Althoughwe  entrateontagging problemsinthispaper,thetheoreti  tion3of thispapershouldbeappli  abletoawideva­rietyofmodelswhereViterbi­stylealgorithms  Context­FreeGrammars,orMEmodels forparsing.See(CollinsandDufy2001;Collins andDufy2002;Collins2002)forotherappli  a­tionsofthevotedper  Inthisse  tion,asamotivatingexample,wede­s  aseofthealgorithminthis paper:thealgorithmappliedtoatrigramtag­ger.InatrigramHMMtagger,ea  tronalgorithm(Rosenblatt58),andthevoted  1Thetheoremsinse  tion3,andtheproofsinse  ­oraveragedversionsoftheper  ribed tion5,applydire  tlytotheworkintheseotherpapers.  htag/wordpairhaveasso  iated parameters.Wewritetheparameterasso  iated withatrigram(x,y,Z)asaxNyNz,andtheparam­eterasso  iatedwithatag/wordpair(t,w)as atNw.A  histotaketheparam­eterstobeestimatesof  onditionalprobabilities: axNyNz=logP(ZIx,y),atNw=logP(wIt).  ewewillusew[1:n]asshort­handforasequen  eofwords[w1,w2wn], andt[1:n]asshorthandforataqsequen  e [t1,t2tn].Inatrigramtaggerthes  ew[1:n]is2i=1ati 2Nti 1Nti+i=1atiNwi. Whentheparametersare  onditionalprobabil­itiesasabovethis"s  ore"isanestimateofthe logofthejointprobabilityP(w[1:n],t[1:n]).The Viterbialgorithm  anbeusedtofndthehighest s  ore.  Asanalternativetomaximum likelihoodpa­rameterestimates,thispaperwillproposethe followingestimationalgorithm.Saythetrain­ingset  es,thei'th senten  ebeingoflengthni.Wewillwritethese examplesas(wi,tfori=1n.Then  thetrainingalgorithmisasfollows:  •ChooseaparameterTdefningthenumber  ofiterationsoverthetrainingset.  InitiallysetallparametersaxNyNzandatNw tobezero.  Fort=1T,i=1n:UsetheViterbi algorithmtofndthebesttaggedsequen  allthistaggedsequen  2timesinZ[1:ni]where  2.Foreverytag/word pair(t,w)seen  1timesin(wi,tand  2timesin(wi,Zwhere  2. Asanexample,saythei'thtaggedsenten  the/Dman/Nsaw/Vthe/Ddog/N  andunderthe  urrentparametersettingsthe highests  2WetakeL 1andL 2tobespe  ialNULLtagsymbols. 3Tisusually  the/Dman/Nsaw/Nthe/Ddog/N  Thentheparameterupdatewilladd1tothe parametersaNNNv,aNNvN ,avN NN,avNsawand subtra  t1fromtheparametersaNNNN,aNNNN , aNN NN,aNNsaw.Intuitivelythishastheef­fe  tofin  reasingtheparametervaluesforfea­tureswhi  hwere"missing"fromtheproposed sequen  eZ[1:ni],anddownweightingparameter valuesfor"in  orre  t"featuresinthesequen  hanges aremadetotheparametervalues.  tors  ribehowtogeneralizethealgorithm tomoregeneralrepresentationsoftaggedse­quen  es.Inthisse  tionwedes  ribethefeature­ve  torrepresentationswhi  ommonlyused inmaximum­entropymodelsfortagging,and whi  harealsousedinthispaper.  isionsin taggingtheprobleminleft­to­rightfashion.At ea  hpointthereisa"history" the  ontextin whi  isionismade andthetask istopredi  tthetaggiventhehistory.Formally, ahistoryisa4­tuple(tL1,tL2,w[1:n],i)where tL1,tL2aretheprevioustwotags,w[1:n]isanar­rayspe  e, andiistheindexofthewordbeingtagged.We use1todenotethesetofallpossiblehistories.  Maximum­entropymodelsrepresentthetag­gingtaskthroughafeature-ve  torrepresentation ofhistory­tagpairs.Afeatureve  tion¢thatmapsa history tagpairtoad­dimensionalfeatureve  ­tor.Ea  omponent¢s(h,t)fors=1d  tionof(h,t).Itis  ommon(e.g.,see(Ratnaparkhi96))forea  h feature¢stobeanindi  tion.Forex­ample,onesu  hfeaturemightbe  urrentwordwiisthe  0otherwise  Similarfeaturesmightbedefnedforevery word/tagpairseenintrainingdata.Another  featuretypemighttra  ktrigramsoftags,forex­ample¢1oo1(h,t)=1if(tL2,tL1,t)=(D,N,V) and0otherwise.Similarfeatureswouldbede­fnedforalltrigramsoftagsseenintraining.A realadvantageofthesemodels  omesfromthe freedomindefningthesefeatures:forexample, (Ratnaparkhi96;M  Callumetal.2000)both des  ribefeaturesetswhi  Inadditiontofeatureve  torrepresentations ofhistory/tagpairs,wewillfndit  onvenient todefnefeatureve  torsof(w[1:n],t[1:n])pairs wherew[1:n]isasequen  tionfrom(w[1:n],t[1:n])pairstod­dimensionalfeatureve  tors.Wewilloftenrefer to<asa"global"representation,in  al"representation.Theparti  onsideredinthispaper aresimplefun  tionsoflo  wherehi =(tiL1,tiL2,w[1:n],i).Ea  hglobal feature<s(w[1:n],t[1:n])issimplythevaluefor thelo  alrepresentation¢ssummedoverallhis­tory/tagpairsin(w[1:n],t[1:n]).Ifthelo  tions,thentheglobalfea­tureswilltypi  ounts".Forexample, with¢1ooodefnedasabove,<1ooo(w[1:n],t[1:n]) isthenumberoftimestheisseentaggedasDT inthepairofsequen  Inmaximum­entropytaggersthefeatureve  tors ¢togetherwithaparameterve  onditionalprobabilitydistri­butionovertagsgivenahistoryas  as4s(hNt)  as4s(hNl)  whereZ(h,a  )=s.Thelogof  lETethisprobabilityhastheformlogp(tIh,a  s=1as¢s(h,t)-logZ(h,a  wherehi =(tiL1,tiL2,w,i).Givenparame­  ethe  w[1:n],highestprobabilitytaggedsequen  eunderthe formulainEq.2  anbefoundeÆ  ientlyusing theViterbialgorithm. Theparameterve  isestimatedfroma trainingsetofsenten  anbe estimatedusingGeneralizedIterativeS  asesitmaybepreferabletoapplya bayesianapproa  hin  2..ANewEstimationMethod  ribeanalternativemethodfores­timatingparametersofthemodel.Givenase­quen  eofwordsw[1:n]andasequen  eofpartof spee  htags,t[1:n],wewilltakethe"s  as¢s(hi,ti)=as<s(w[1:n],t[1:n]) i=1s=1 s=1  wherehiisagain(tiL1,tiL2,w[1:n],i).Notethat thisisalmostidenti  altoEq.2,butwithoutthe lo  ).Under thismethodforassignings  es,thehighests  anbefoundusingtheViterbi algorithm.(We  odingalgorithmtothatformaximum­entropy taggers,thediferen  ulated.)  Wethenproposethetrainingalgorithminfg­ure1.ThealgorithmtakesTpassesoverthe trainingsample.Allparametersareinitiallyset tobezero.Ea  odedus­ingthe  urrentparametersettings.Ifthehigh­ests  orre  t,theparametersasareupdatedina simpleadditivefashion.  Notethatifthelo  tions,thentheglobalfeatures<swillbe  ounts.Inthis  s -ds toea  hparameteras,where  sisthenumber oftimesthes'thfeatureo  urredinthe  e,anddsisthenumberoftimes  es, (wiTLifori=ln.AparameterT  ifyingnumberofiterationsoverthetrainingset.A "lo  alrepresentation"<whi  tionthatmaps history/tagpairstod­dimensionalfeatureve  tors.The globalrepresentation<isdefnedthrough<asinEq.l. Initialization:Setparameterve  tor a=. Algorithm:  •UsetheViterbialgorithmtofndtheoutputofthe modelonthei'thtrainingsenten  ewiththe  rn.  whereisthesetofalltagsequen  •IfZ[1:n.] [1:n.]thenupdatetheparameters  s= s<s(wiTLi(wiTZ[1:n.])  Output:Parameterve  tor a.  Figure1:Thetrainingalgorithmfortagging.  ursinhighests  oringsequen  urrentmodel.Forexample,ifthefeatures¢s areindi  kingalltrigramsand word/tagpairs,thenthetrainingalgorithmis identi  altothatgiveninse  tion2.1.  Thereisasimplerefnementtothealgorithm infgure1,  alledthe"averagedparameters"  method.Defneastobethevalueforthes'th parameterafterthei'thtrainingexamplehas beenpro  essedinpasstoverthetrainingdata. Thenthe"averagedparameters"aredefnedas Is =atNi/nTforalls=1d.  Itissimpletomodifythealgorithmtostore thisadditionalsetofparameters.Experiments inse  tion4showthattheaveragedparameters performsignif  antlybetterthanthefnalpa­  rametersas.Thetheoryinthenextse  ationfortheaveragingmethod.  3TheoryJustifyingtheAlgorithm  Inthisse  tionwegiveageneralalgorithmfor problemssu  hastaggingandparsing,andgive theoremsjustifyingthealgorithm.Wealsoshow howthetaggingalgorithminfgure1isaspe­  aseofthisalgorithm.Convergen  etheo­remsfortheper  ation problemsappearin(Freund&S  hapire99) theresultsinthisse  ­tion5,showhowthe  Inputs:Trainingexamples(xiTYi) Initialization:Set a= Algorithm:  If(Zi then =a<(TYi)-<(xiTZ  Figure2:Avariantoftheper  eptronalgorithm.  Thetaskistolearnamappingfrominputs xEXtooutputsyEY.Forexample,Xmight beasetofsenten  es,withYbeingasetofpos­sibletagsequen  Trainingexamples(xi,yi)fori=1n.  tionGENwhi  henumeratesasetof  andidatesGEN(x)foraninputx.  Arepresentation<mappingea  h(x,y)E XYtoafeatureve  •Aparameterve  The  defneamap­pingfromaninputxtoanoutputF(x)through  where<(x,y)·a  t (x,y).Thelearningtaskistosetthe  usingthetrainingexamples aseviden  Thetaggingprobleminse  tion2  anbe mappedtothissettingasfollows:  •Thetrainingexamplesaresenten  Givenasetofpossibletags,wedefne GEN(w[1:n])=n ,i.e.,thefun  etothesetof  Therepresentation<(x,y)= <(w[1:n],t[1:n])isdefnedthroughlo  tors¢(h,t)where(h,t)isa history/tagpair.(SeeEq.1.)  Figure2showsanalgorithmforsettingthe weightsa  .It  anbeverifedthatthetraining  algorithmfortaggersinfgure1isaspe  ase ofthisalgorithm,ifwedefne(xi,yi),GENand <asjustdes  Wewillnowgiveafrsttheoremregarding the  eofthisalgorithm.Thistheorem thereforealsodes  ribes  onditionsunderwhi  h thealgorithminfgure1  onverges.First,we needthefollowingdefnition:  Defnition1LetGEN(xi)=GEN(xi)-{Yi}.In otherwordsGEN(xi)isthesetofin  orre  andidates foranexamplexi.Wewillsaythatatrainingsequen  e (xiTYi)fori=lnisseparablewithmarginÆ ifthereexistssomeve  torUwithllUll=lsu  hthat  .llUllisthe2-normofU,i.e.,llUll=sUs2.J  anthenstatethefollowingtheorem(see se  tion5foraproof): Theorem1Foranytrainingsequen  his separablewithmarginÆ,thenfortheper  eptronalgorithm infgure2  Æ2 whereRisa  Thistheoremimpliesthatifthereisaparam­eterve  torUwhi  hmakeszeroerrorsonthe trainingset,thenafterafnitenumberofitera­tionsthetrainingalgorithmwillhave  onverged toparametervalueswithzerotrainingerror.A  ialpointisthatthenumberofmistakesisin­dependentofthenumberof  h example(i.e.thesizeofGEN(xi)forea  hi), dependingonlyontheseparationofthetraining data,whereseparationisdefnedabove.This isimportantbe  anbeexponentialinthesizeofthe inputs.Allofthe  eandgeneraliza­tionresultsinthispaperdependonnotionsof separabilityratherthanthesizeofGEN.  ometomind.First,arethere guaranteesforthealgorithmifthetrainingdata isnotseparable?Se  ond,performan  eona trainingsampleisallverywell,butwhatdoes thisguaranteeabouthowwellthealgorithm generalizestonewlydrawntestexamples?(Fre­und&S  usshowthetheory  an beextendedtodealwithbothofthesequestions. Thenextse  ribehowtheseresults  an beappliedtothealgorithmsinthispaper.  3.1Theoryforinseparabledata  Inthisse  tionwegiveboundswhi  happlywhen thedataisnotseparable.First,weneedthe followingdefnition:  ThevalueDuNÆisameasureofhow  loseU istoseparatingthetrainingdatawithmarginÆ. DuNÆis0iftheve  torUseparatesthedatawith atleastmarginÆ.IfUseparatesalmostallof theexampleswithmarginÆ,butafewexamples arein  orre  tlytaggedorhavemarginlessthan Æ,thenDuNÆwilltakearelativelysmallvalue.  Thefollowingtheoremthenapplies(seese  ­tion5foraproof):  Theorem2Foranytrainingsequen  e(xiTYi),forthe frstpassoverthetrainingsetoftheper  eptronalgorithm infgure2,  whereRisa  hthatViTVZE GEN(xi)ll<(xiTYi)-<(xiTZ)ll:R,andthe ministakenoverÆ,llUll=l.  Thistheoremimpliesthatifthetrainingdata is"  lose"tobeingseparablewithmarginÆ i.e.,thereexistssomeUsu  hthatDuNÆisrela­tivelysmallthenthealgorithmwillagainmake asmallnumberofmistakes.Thustheorem2 showsthattheper  eptronalgorithm  anbero­busttosometrainingdataexamplesbeingdif­f  orre  tly.  3.2Generalizationresults  Theorems1and2giveresultsboundingthe numberoferrorsontrainingsamples,butthe questionwearereallyinterestedin  erns guaranteesofhowwellthemethodgeneralizes tonewtestexamples.Fortunately,thereare severaltheoreti  alresultssuggestingthatifthe per  eptronalgorithmmakesarelativelysmall numberofmistakesonatrainingsamplethenit islikelytogeneralizewelltonewexamples.This se  ribessomeoftheseresults,whi  h originallyappearedin(Freund&S  tlyfromresultsin(Helm­boldandWarmuth95).  ationoftheper  ep­tronalgorithm,thevotedper  onsiderthefrstpassoftheper  hofthesewilldefneanoutput Vi =argmaxzEGEN(x)a  1Ni·<(x,Z).Thevoted per  eptrontakesthemostfrequentlyo  urring outputintheset{V1Vn}.Thusthevoted per  eptronisamethodwhereea  1Nifori=1ngetasin­glevotefortheoutput,andthemajoritywins. Theaveragedalgorithminse  tion2.5  anbe  onsideredtobeanapproximationofthevoted method,withtheadvantagethatasinglede  od­ingwiththeaveragedparameters  anbeper­formed,ratherthannde  odingswithea  hof thenparametersettings.  Inanalyzingthevotedper  eptrontheoneas­sumptionwewillmakeisthatthereissome unknowndistributionP(x,y)overthesetX Y,andthatbothtrainingandtestexamples aredrawnindependently,identi  allydistributed (i.i.d.)fromthisdistribution.Corollary1of (Freund&S  hapire99)thenstates:  eoftrainingexamples andlet(xn+1TYn+1)beatestexample.Thentheprob­ability.overthe  eofallnlexamplesJthatthe voted-per  whereEn+1[]isanexpe  tedvaluetakenovernlex­amples,RandDu,Æareasdefnedabove,andtheminis takenoverÆ,llUll=l.  Weranexperimentsontwodatasets:part­of­spee  htaggingonthePennWallStreetJournal treebank(Mar  usetal.93),andbasenoun­phrasere  ognitiononthedatasetsoriginallyin­trodu  edby(RamshawandMar  us95).Inea  asewehadatraining,developmentandtestset. Forpart­of­spee  htaggingthetrainingsetwas se  tions0 18ofthetreebank,thedevelopment setwasse  tions19 21andthefnaltestsetwas se  tions22­24.InNP  CurrentwordW. &t.  PreviouswordW..l &t.  NextwordW.+l &t.  WordtwoaheadW.+2 &t.  CurrenttagP. &t.  PrevioustagP..l &t.  TagtwoaheadP.+2 &t.  Figure3:FeaturetemplatesusedintheNP  hunking experiments.wiisthe  e.PiisPOStagforthe  urrentword,and P1PnisthePOSsequen  eforthesenten  e.Liisthe  hunkingtagassignedtothei'thword.  tion15 18,thedevelopment setwasse  tion21,andthetestsetwasse  tion 20.ForPOStaggingwereporttheper  entage of  orre  or­respondingtobaseNP  ..2Features  alfeaturesto thoseof(Ratnaparkhi96),theonlydiferen  e beingthatwedidnotmaketherareworddis­tin  tionintable1of(Ratnaparkhi96)(i.e., spellingfeatureswerein  ludedforallwordsin trainingdata,andtheworditselfwasusedasa featureregardlessofwhetherthewordwasrare). Thefeaturesettakesintoa  ounttheprevious tagandpreviouspairsoftagsinthehistory,as wellasthewordbeingtagged,spellingfeatures ofthewordsbeingtagged,andvariousfeatures ofthewordssurroundingthewordbeingtagged.  Inthe  h forthosewordsfromthetaggerin(Brill95).Ta­ble3showsthefeaturesusedintheexperiments. The  hunkingproblemisrepresentedasathree­tagtask,wherethetagsareB,I,0forwords beginninga  tively.All  hunksbe­ginwithaBsymbol,regardlessofwhetherthe previouswordistagged0orI.  Method F­Measure Numits  Method Errorrate/% Numits  Figure4:Resultsforvariousmethodsonthepart­of­spee  entages.Numitsisthenumber oftrainingiterationsatwhi  hthebests  hieved. Per  istheper  eptronalgorithm,MEisthemaximum entropymethod.Avg/noavgistheper  eptronwithor withoutaveragedparameterve  tors.  =5meansonly featureso  urring5timesormoreintrainingarein­  =meansallfeaturesintrainingarein  Weappliedbothmaximum­entropymodelsand theper  eptronalgorithmtothetwotagging problems.Wetestedseveralvariantsforea  h algorithmonthedevelopmentset,togainsome understandingofhowthealgorithms'perfor­man  evariedwithvariousparametersettings, andtoallowoptimizationoffreeparametersso thatthe  omparisononthefnaltestsetisafair one.Forbothmethods,wetriedthealgorithms withfeature  ount  ut­ofssetat0and5(i.e., weranexperimentswithallfeaturesintraining datain  luded,orwithallfeatureso  urring5 timesormorein  ount  ut­ofof5).Intheper  eptronalgo­rithm,thenumberofiterationsToverthetrain­ingsetwasvaried,andthemethodwastested withbothaveragedandunaveragedparameter  tors(i.e.,withasandIs,asdefnedin se  tion2.5,foravarietyofvaluesforT).In themaximumentropymodelthenumberofit­erationsoftrainingusingGeneralizedIterative S  Figure4showsresultsondevelopmentdata onthetwotasks.Thetrendsarefairly  antlyforthe per  eptronmethod,asdoesin  ludingallfea­turesratherthanimposinga  ount  ut­ofof5. In  ontrast,theMEmodels'performan  luded.Thebestper  onfgurationgivesimprovementsoverthe maximum­entropymodelsinboth  ases:anim­provementinF­measurefrom9265%to9353% in  tionfrom328%to 293%errorrateinPOStagging.Inlooking attheresultsfordiferentnumbersofiterations ondevelopmentdatawefoundthataveraging notonlyimprovesthebestresult,butalsogives mu  hgreaterstabilityofthetagger(thenon­averagedvarianthasmu  hgreatervarian  ores).  Asafnaltest,theper  eptronandMEtag­gerswereappliedtothetestsets,withtheop­timalparametersettingsondevelopmentdata. OnPOStaggingtheper  omparedto3.28%errorforthe maximum­entropymodel(a11.9%relativere­du  tioninerror).InNP  hunkingtheper  hievesanF­measureof93.63%, in  ontrasttoanF­measureof93.29%forthe MEmodel(a5.1%relativeredu  tioninerror).  5ProofsoftheTheorems  Thisse  tiongivesproofsoftheorems1and2. Theproofsareadaptedfromproofsforthe  ation  ProofofTheorem1:Leta  betheweights beforethek'thmistakeismade.Itfollowsthat a  1=0.Supposethek'thmistakeismadeat thei'thexample.TakeZtotheoutputproposed atthisexample,Z=argmaxyEGEN(xi)<(xi,y)·  .Itfollowsfromthealgorithmupdatesthat  tsofbothsideswiththeve  torU:  wheretheinequalityfollowsbe  auseoftheprop­ertyofUassumedinEq.3.Be  1=0, andthereforeU·a  1=0,itfollowsbyindu  tiononkthatforallk,U·a  itfollowsthat IIa  WealsoderiveanupperboundforIIa  wheretheinequalityfollowsbe  ause II<(xi,yi)-<(xi,Z)II2:R2byassump­tion,anda  ause Zisthehighests  andidateforxiunder theparametersa  k .Itfollowsbyindu  tionthat IIa  CombiningtheboundsIIa  :kRgivestheresultforallkthat  ProofofTheorem2:Wetransformtherep­resentation<(x,y)E.dtoanewrepresentation a.d+n  <d+j(x,y)=6if(x,y)=(xj,yj),0otherwise, where6isaparameterwhi  hisgreaterthan0. Similary,saywearegivenaU,Æpair,and  or­respondingvaluesforEiasdefnedabove.We D.d+n  torUE  withUi =Uifori=1dandUd+j=Ej/6 forj=1n.Underthesedefnitionsit  It  anbeseenthattheve  torU/IIDseparates  thedatawithmarginÆ/1+D2/62 .Bythe­  orem1,thismeansthatthefrstpassoftheper­a  eptronalgorithmwithrepresentation<makes  atmostkmax(6)= Æ1 2(R2+62)(1+}u 2 ,Æ)mis­takes.Butthefrstpassoftheoriginalalgo­rithmwithrepresentation<isidenti  altothe frstpassofthealgorithmwithrepresentation  ausetheparameterweightsfortheaddi­a  tionalfeatures<d+jforj=1nea  ta singleexampleoftrainingdata,anddonotafe  t the  ationoftestdataexamples.Thus theoriginalper  eptronalgorithmalsomakesat mostkmax(6)mistakesonitsfrstpassoverthe trainingdata.Finally,we  withrespe  tto6,giving6=RDuNÆ,and  kmax(RDuNÆ)=(R2+D2)/Æ2 ,implyingthe  boundinthetheorem.  6Con  ribednewalgorithmsfortagging, whoseperforman  eguaranteesdependonano­tionof"separability"oftrainingdataexam­ples.Thegeneri  algorithminfgure2,and  thetheoremsdes  ouldbeappliedtoseveralothermodels intheNLPliterature.Forexample,aweighted  analsobe  ,sothe weightsforgenerativemodelssu  hasPCFGs  ouldbetrainedusingthismethod.  hapireandYoram Singerformanyusefuldis  ussionsregarding thealgorithmsinthispaper,andtoFernando PereiraforpointerstotheNP  hunkingdata set,andforsuggestionsregardingthefeatures usedintheexperiments.  Brill,E.(l995).Transformation­BasedError­Driven LearningandNaturalLanguagePro  essing:ACase StudyinPartofSpee  s.  Collins,M.,andDufy,N.(2l).ConvolutionKernels forNaturalLanguage.InPro  eedingsofNeuralInfor­mationPro  Collins,M.,andDufy,N.(22).NewRankingAlgo­rithmsforParsingandTagging:KernelsoverDis  tures,andtheVotedPer  Collins,M.(22).RankingAlgorithmsforNamed. EntityExtra  tion:BoostingandtheVotedPer  ationusingthePer  eptronAlgorithm.InMa  hine Learning,37(3):277.296.  Helmbold,D.,andWarmuth,M.Onweaklearning.Jour­nalofComputerandSystemS  Laferty,J.,M  Callum,A.,Freitag,D.,andPereira,F.(2)Max­imumentropymarkovmodelsforinformationextra  ­tionandsegmentation.InPro  z,M.(l993). Buildingalargeannotated  orpusofenglish:The Penntreebank.ComputationalLinguisti  Ramshaw,L.,andMar  us,M.P.(l995).TextChunking UsingTransformation­BasedLearning.InPro  eedings oftheThirdACLWorkshoponVeryLargeCorpora, Asso  ModelforInformationStorageandOrganizationinthe Brain.Psy  alReview,65,386.4 8.(Reprinted inNeuro  omputing(MITPress,l998).) 