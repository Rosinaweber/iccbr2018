 Emotions from text: machine learning for text-based emotion prediction Cecilia Ovesdotter Alm  Dan Roth  Richard Sproat  Dept. of Linguistics  Dept. of Computer Science  Dept. of Linguistics  UIUC  UIUC  Dept. of Electrical Eng.  UIUC  study, including the machine learning model, the  corpus, the feature set, parameter tuning, etc.  SecIn addition to information, text  contion 5 presents experimental results from two  classitains attitudinal, and more specifically,  fication tasks and feature set modifications. Section  emotional content. This paper explores  6 describes the agenda for refining the model, before  the text-based emotion prediction  probpresenting concluding remarks in 7.  lem empirically, using supervised machine  learning with the SNoW learning  archiApplication area: Text-to-speech  tecture. The goal is to classify the  emotional affinity of sentences in the  narraNarrative text is often especially prone to having  tive domain of children's fairy tales, for  emotional contents. In the literary genre of fairy  tales, emotions such as HAPPINESS and ANGER and  sive rendering of text-to-speech  syntherelated cognitive states, e.g. LOVE or HATE, become  sis. Initial experiments on a preliminary  integral parts of the story plot, and thus are of  pardata set of 22 fairy tales show  encouragticular importance. Moreover, the story teller  reading results over a na ve baseline and BOW  ing the story interprets emotions in order to orally  approach for classification of emotional  convey the story in a fashion which makes the story  versus non-emotional contents, with some  come alive and catches the listeners' attention.  In speech, speakers effectively express emotions  by modifying prosody, including pitch, intensity,  which covers emotional valence, as well  and durational cues in the speech signal. Thus, in  as feature set alternations. In addition, we  order to make text-to-speech synthesis sound as  natpresent plans for a more cognitively sound  ural and engaging as possible, it is important to  convey the emotional stance in the text. However, this  tion a larger set of basic emotions.  implies first having identified the appropriate  emotional meaning of the corresponding text passage.  Thus, an application for emotional text-to-speech  Introduction  synthesis has to solve two basic problems. First,  Text does not only communicate informative  conwhat emotion or emotions most appropriately  detents, but also attitudinal information, including  scribe a certain text passage, and second, given a text  emotional states. The following reports on an  empassage and a specified emotional mark-up, how to  pirical study of text-based emotion prediction.  render the prosodic contour in order to convey the  Section 2 gives a brief overview of the intended  emotional content, (Cahn, 1990). The text-based  application area, whereas section 3 summarizes  reemotion prediction task (TEP) addresses the first of lated work. Next, section 4 explains the empirical  these two problems.  Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 579 586, Vancouver, October 2005. c 2005 Association for Computational Linguistics  Previous work  such as happy or sad, matter, emotion classification probably needs to consider additional inference  For a complete general overview of the field of  afmechanisms. Moreover, a na ve compositional  apfective computing, see (Picard, 1997). (Liu, Lieber-proach to emotion recognition is risky due to simple  man and Selker, 2003) is a rare study in  textlinguistic facts, such as context-dependent  semanbased inference of sentence-level emotional  affintics, domination of words with multiple meanings,  ity. The authors adopt the notion of basic emotions, and emotional negation.  cf. (Ekman, 1993), and use six emotion categories:  Many NLP problems address attitudinal  meaning distinctions in text, e.g.  detecting subjective  SURPRISE. They critique statistical NLP for being  unsuccessful at the small sentence level, and instead  al, 2004), measuring strength of subjective clauses use a database of common-sense knowledge and  create affect models which are combined to form a  reppolarity (Hatzivassiloglou and McKeown, 1997) or  resentation of the emotional affinity of a sentence.  texts' attitudinal valence, e.g. (Turney, 2002), (Bai,  At its core, the approach remains dependent on an  emotion lexicon and hand-crafted rules for  concepVaithyanathan, 2003), (Mullen and Collier, 2003),  tual polarity. In order to be effective, emotion  recog(Pang and Lee, 2003). Here, it suffices to say that  nition must go beyond such resources; the authors  the targets, the domain, and the intended application  note themselves that lexical affinity is fragile. The  differ; our goal is to classify emotional text passages  method was tested on 20 users' preferences for an  in children's stories, and eventually use this  inforemail-client, based on user-composed text emails  mation for rendering expressive child-directed  stodescribing short but colorful events. While the users  rytelling in a text-to-speech application. This can be  preferred the emotional client, this evaluation does  useful, e.g. in therapeutic education of children with  not reveal emotion classification accuracy, nor how  well the model generalizes on a large data set.  Whereas work on emotion classification from  Empirical study  the point of view of natural speech and  humancomputer dialogues is fairly extensive, e.g. (Scherer,  This part covers the experimental study with a  for2003), (Litman and Forbes-Riley, 2004), this  appears not to be the case for text-to-speech  synthetion, data, features, and a note on parameter tuning.  sis (TTS). A short study by (Sugimoto et al., 2004)  Machine learning model  Japanese TTS. Their model uses a composition  asDetermining emotion of a linguistic unit can be  sumption: the emotion of a sentence is a function of  cast as a multi-class classification problem.  the emotional affinity of the words in the sentence.  the flat case, let T denote the text, and s an em-They obtain emotional judgements of 73 adjectives  bedded linguistic unit, such as a sentence, where  and a set of sentences from 15 human subjects and  s T . Let k be the number of emotion classes E =  compute words' emotional strength based on the  ra{em 1 , em 2 , .., emk}, where em 1 denotes the special tio of times a word or a sentence was judged to fall  case of neutrality, or absence of emotion. The goal into a particular emotion bucket, given the number  is to determine a mapping function f : s emi, of human subjects. Additionally, they conducted an  such that we obtain an ordered labeled pair ( s, emi).  interactive experiment concerning the acoustic  renThe mapping is based on F = {f 1 , f 2 , .., fn}, where dering of emotion, using manual tuning of prosodic  F contains the features derived from the text.  parameters for Japanese sentences. While the  auFurthermore, if multiple emotion classes can  thors actually address the two fundamental problems  characterize s, then given E' E, the target of the of emotional TTS, their approach is impractical and  mapping function becomes the ordered pair ( s, E0).  most likely cannot scale up for a real corpus. Again,  Finally, as further discussed in section 6, the  hierwhile lexical items with clear emotional meaning,  archical case of label assignment requires a  sequential model that further defines levels of coarse ver-marked for other affective contents, i.e. background  sus fine-grained classifiers, as done by (Li and Roth,  mood, secondary emotions via intensity, feeler, and 2002) for the question classification problem.  textual cues. Disagreements in annotations are resolved by a second pass of tie-breaking by the first  author, who chooses one of the competing labels.  Whereas our goal is to predict finer emotional  meanEventually, the completed annotations will be made  ing distinctions according to emotional categories in  speech; in this study, we focus on the basic task of  recognizing emotional passages and on determining  Table 1: Basic emotions used in annotation  their valence (i.e. positive versus negative) because  Abbreviation  Emotion class  we currently do not have enough training data to  explore finer-grained distinctions. The goal here is to  get a good understanding of the nature of the TEP  problem and explore features which may be useful.  We explore two cases of flat classification,  using a variation of the Winnow update rule  implemented in the SNoW learning architecture  (Carlson et al., 1999),1 which learns a linear classifier  Emotion annotation is hard; interannotator  agreein feature space, and has been successful in  sevment currently range at = . 24 . 51, with the ra-eral NLP applications, e.g. semantic role labeling  tio of observed annotation overlap ranging between  45-64%, depending on annotator pair and stories  asthe first case, the set of emotion classes E consists  signed. This is expected, given the subjective nature  of  of the annotation task. The lack of a clear  definiEMOTIONAL versus non-emotional or NEUTRAL,  i.e. E = {N, E}. In the second case, E has been tion for emotion vs. non-emotion is acknowledged  incremented with emotional distinctions according  across the emotion literature, and contributes to  dyto the valence, i.e. E = {N, P E, N E}. Experi-namic and shifting annotation targets.  Indeed, a  ments used 10-fold cross-validation, with 90% train  ciding whether or not a sentence is emotional or  non-emotional. Emotion perception also depends on  which character's point-of-view the annotator takes,  The goal of our current data annotation project is  and on extratextual factors such as annotator's  perto annotate a corpus of approximately 185 children  sonality or mood. It is possible that by focusing  stories, including Grimms', H.C. Andersen's and B.  more on the training of annotator pairs, particularly  Potter's stories. So far, the annotation process  proon joint training, agreement might improve.  Howceeds as follows: annotators work in pairs on the  ever, that would also result in a bias, which is  probsame stories. They have been trained separately and  ably not preferable to actual perception. Moreover,  work independently in order to avoid any  annotawhat agreement levels are needed for successful  extion bias and get a true understanding of the task  difficulty. Each annotator marks the sentence level  The current data set consisted of a preliminary  anwith one of eight primary emotions, see table 1, re-notated and tie-broken data set of 1580 sentence, or  flecting an extended set of basic emotions (Ekman, 22 Grimms' tales. The label distribution is in table  1993). In order to make the annotation process more  2. NEUTRAL was most frequent with 59.94%.  focused, emotion is annotated from the point of view  of the text, i.e. the feeler in the sentence. While the primary emotions are targets, the sentences are also  Table 2: Percent of annotated labels  1Available from http://l2r.cs.uiuc.edu/ cogcomp/  2Experiments were also run for Perceptron, however the  results are not included. Overall, Perceptron performed worse.  Feature conjunctions covered pairings of counts of Table 3: % EMOTIONAL vs. NEUTRAL examples  positive and negative words with range of story  progress or interjections, respectively.  Feature groups 1, 3, 5, 6, 7, 8, 9, 10 and 14 are  extracted automatically from the sentences in the  stories; with the SNoW POS-tagger used for features  9, 10, and 14. Group 10 reflects how many verbs  are active in a sentence. Together with the quotation  Next, for the purpose of this study, all emotional  and punctuation, verb domination intends to capture  classes, i.e. A, D, F, H, SA, SU+, SU-, were  comthe assumption that emotion is often accompanied  bined into one emotional superclass E for the first by increased action and interaction. Feature group  experiment, as shown in table 3. For the second  ex4 is based on Finish scholar Antti Aarne's classes  periment, we used two emotional classes, i.e.  posof folk-tale types according to their informative  theitive versus negative emotions; P E= { H, SU+ } and matic contents (Aarne, 1964).  The current tales  N E= { A, D, F, SA, }, as seen in table 4.  have 3 top story types (ANIMAL TALES, ORDINARY  Feature set  FOLK-TALES, and JOKES AND ANECDOTES), and  15 subtypes (e.g. supernatural helpers is a subtype The feature extraction was written in python. SNoW  of the ORDINARY FOLK-TALE). This feature intends  only requires active features as input, which resulted  to provide an idea about the story's general affective  in a typical feature vector size of around 30 features.  personality (Picard, 1997), whereas the feature reThe features are listed below. They were  impleflecting the story progress is hoped to capture that  mented as boolean values, with continuous values  some emotions may be more prevalent in certain  represented by ranges. The ranges generally  oversections of the story (e.g. the happy end).  lapped, in order to get more generalization coverage.  For semantic tasks, words are obviously  impor1. First sentence in story  tant. In addition to considering content words', we  also explored specific word lists. Group 11 uses  2. Conjunctions of selected features (see below)  2 lists of 1636 positive and 2008 negative words,  3. Direct speech (i.e. whole quote) in sentence  obtained from (Di Cicco et al., online). Group 12  4. Thematic story type (3 top and 15 sub-types)  uses lexical lists extracted from WordNet (Fellbaum,  1998), on the basis of the primary emotion words  5. Special punctuation (! and ?)  in their adjectival and nominal forms. For the  ad6. Complete upper-case word  jectives, Py-WordNet's (Steele et al., 2004)  SIMILAR feature was used to retrieve similar items of  7. Sentence length in words (0-1, 2-3, 4-8, 9-15,  the primary emotion adjectives, exploring one  additional level in the hierarchy (i.e. similar items of all  8. Ranges of story progress (5-100%, 15-100%,  senses of all words in the synset). For the nouns and  any identical verbal homonyms, synonyms and  hy9. Percent of JJ, N, V, RB (0%, 1-100%,  50ponyms were extracted manually.3 Feature group 13  used a short list of 22 interjections collected  manually by browsing educational ESL sites, whereas the  10. V count in sentence, excluding participles (0-1,  affective word list of 771 words consisted of a  combination of the non-neutral words from  (Johnson11. Positive and negative word counts ( 1, 2, Laird and Oatley, 1989) and (Siegle, online). Only a  subset of these lexical lists actually occurred.4  12. WordNet emotion words  3Multi-words were transformed to hyphenated form.  13. Interjections and affective words  4At this point, neither stems and bigrams nor a list of ono-matopoeic words contribute to accuracy. Intermediate resource 14. Content BOW: N, V, JJ, RB words by POS  processing inserted some feature noise.  The above feature set is henceforth referred to as in pejorative parameter settings. The random selec-all features, whereas content BOW is just group 14.  tion could make a difference, but was not explored.  The content BOW is a more interesting baseline than the na ve one, P(Neutral), i.e. always assigning the 5  Results and discussion  most likely NEUTRAL category. Lastly, emotions  This section first presents the results from  experiments with the two different confusion sets  de2003). Thus, emotion and background mood of  imscribed above, as well as feature experimentation.  mediately adjacent sentences, i.e. the sequencing, seems important. At this point, it is not implemented  Classification results  automatically. Instead, it was extracted from the  Average accuracy from 10-fold cross validation for  manual emotion and mood annotations. If  sequencthe first experiment, i.e. classifying sentences as  eiing seemed important, an automatic method using  ther NEUTRAL or EMOTIONAL, are included in  tasequential target activation could be added next.  ble 5 and figure 1 for the two tuning conditions on  the main feature sets and baselines. As expected,  The Winnow parameters that were tuned included  promotional , demotional , activation threshold Table 5: Mean classification accuracy: N vs. E, 2 conditions , initial weights , and the regularization parame-same-tune-eval  ter, S, which implements a margin between positive P(Neutral)  and negative examples. Given the currently fairly  limited data, results from 2 alternative tuning  methods, applied to all features, are reported.  For the condition called sep-tune-eval, 50%  of the sentences were randomly selected and  degree of success reflects parameter settings, both  set aside to be used for the parameter tuning  for content BOW and all features. Nevertheless, un-process only. Of this subset, 10% were  subseder these circumstances, performance above a na ve  quently randomly chosen as test set with the  rebaseline and a BOW approach is obtained.  Moremaining 90% used for training during the  autoover, sequencing shows potential for contributing matic tuning process, which covered 4356 dif-in one case. However, observations also point to  ferent parameter combinations. Resulting  pathree issues: first, the current data set appears to  rameters were: = 1 . 1, = 0 . 5, = 5, be too small. Second, the data is not easily  separa = 1 . 0, S = 0 . 5. The remaining half of ble. This comes as no surprise, given the subjective  the data was used for training and testing in the  nature of the task, and the rather low  interannota10-fold cross-validation evaluation. (Also, note  tor agreement, reported above. Moreover, despite  the slight change for P(Neutral) in table 5, due  the schematic narrative plots of children's stories,  to randomly splitting the data.)  tales still differ in their overall affective orientation,  Given that the data set is currently small, for the which increases data complexity. Third and finally,  condition named same-tune-eval, tuning was  the EMOTION class is combined by basic emotion  performed automatically on all data using a  labels, rather than an original annotated label.  slightly smaller set of combinations, and then  manually adjusted against the 10-fold  crosscross-validation are included in table 6 using all  validation process. Resulting parameters were:  features and the separated tuning and evaluation  = 1 . 2, = 0 . 9, = 4, = 1, S = 0 . 5. All data condition sep-tune-eval. With these parame-data was used for evaluation.  ters, approximately 3% improvement in accuracy  over the na ve baseline P(Neutral) was recorded, Emotion classification was sensitive to the selected  and 5% over the content BOW, which obviously did  tuning data. Generally, a smaller tuning set resulted  poorly with these parameters. Moreover, precision is  Table 7: N, PE, and NE ( all features, sep-tune-eval) All features except BOW  All features  Averaged precision  Averaged recall  Averaged F-score  Table 8: Feature group members  Word lists  interj., WordNet, affective lists, pos/neg  Syntactic  length ranges, % POS, V-count ranges  Story-related  % story-progress, 1st sent., story type  Orthographic  punctuation, upper-case words, quote  % Accuracy  Conjunctions with pos/neg  Figure 1: Accuracy under different conditions (in %)  with different feature configurations. Starting with  Table 6: Classifying N vs. E ( all features, sep-tune-eval) all features, again using 10-fold cross-validation for  the separated tuning-evaluation condition  sep-tuneAveraged accuracy  eval, one additional feature group was removed  until none remained. The feature groups are listed in  Averaged precision  table 8. Figure 2 on the next page shows the  accuAveraged recall  racy at each step of the cumulative subtraction  proAveraged F-score  cess. While some feature groups, e.g. syntactic,  appeared less important, the removal order mattered;  higher than recall for the combined EMOTION class.  e.g. if syntactic features were removed first,  accuIn comparison, with the same-tune-eval procedure, racy decreased. This fact also illustrated that fea-the accuracy improved by approximately 9% over  tures work together; removing any group degraded  P(Neutral) and by 8% over content BOW.  performance because features interact and there is  In the second experiment, the emotion category  It was observed that  feawas split into two classes: emotions with positive  tures' contributions were sensitive to parameter  tunversus negative valence. The results in terms of  preing. Clearly, further work on developing features  cision, recall, and F-score are included in table 7,  uswhich fit the TEP problem is needed.  ing all features and the sep-tune-eval condition. The 6  Refining the model  decrease in performance for the emotion classes  mirrors the smaller amounts of data available for each  This was a first pass of addressing TEP for TTS.  class. As noted in section 4.3, only 9.87% of the  At this point, the annotation project is still on-going,  sentences were annotated with a positive emotion,  and we only had a fairly small data set to draw on.  and the results for this class are worse. Thus,  perforNevertheless, results indicate that our learning  apmance seems likely to improve as more annotated  proach benefits emotion recognition. For example,  story data becomes available; at this point, we are  the following instances, also labeled with the same  experimenting with merely around 12% of the total  valence by both annotators, were correctly classified  texts targeted by the data annotation project.  both in the binary (N vs. E) and the tripartite  polarity task (N, NE, PE), given the separated tuning and  evaluation data condition, and using all features: Emotions are poorly understood, and it is  espe(1a) E/NE: Then he offered the dwarfs money, and prayed and cially unclear which features may be important for  besought them to let him take her away; but they said, We will their recognition from text. Thus, we experimented  not part with her for all the gold in the world.  Cumulative removal of feature groups  s  ic  Figure 2: Averaged effect of feature group removal, using sep-tune-eval (1b) N: And so the little girl really did grow up; her skin was as We also plan to explore finer emotional meaning dis-white as snow, her cheeks as rosy as the blood, and her hair as tinctions, by using a hierarchical sequential model  black as ebony; and she was called Snowdrop.  which better corresponds to different levels of  cog(2a) E/NE: Ah, she answered, have I not reason to weep?  nitive difficulty in emotional categorization by  hu(2b) N: Nevertheless, he wished to try him first, and took a stone mans, and to classify the full set of basic level emo-in his hand and squeezed it together so that water dropped out tional categories discussed in section 4.3. Sequential  of it.  modeling of simple classifiers has been successfully  Cases (1a) and (1b) are from the well-known FOLK  employed to question classification, for example by  TALE Snowdrop, also called Snow White.  (Li and Roth, 2002). In addition, we are working  and (1b) are also correctly classified by the  simon refining and improving the feature set, and given  ple content BOW approach, although our approach  more data, tuning can be improved on a sufficiently  has higher prediction confidence for E/NE (1a); it  large development set. The three subcorpora in the  also considers, e.g. direct speech, a fairly high verb  annotation project can reveal how authorship affects  count, advanced story progress, connotative words  emotion perception and classification.  and conjunctions thereof with story progress  feaMoreover, arousal appears to be an important  tures, all of which the BOW misses. In addition, the  dimension for emotional prosody (Scherer, 2003),  simple content BOW approach makes incorrect  preespecially in storytelling (Alm and Sproat, 2005).  dictions at both the bipartite and tripartite levels for  Thus, we are planning on exploring degrees of  emoexamples (2a) and (2b) from the JOKES AND  ANECtional intensity in a learning scenario, i.e. a  probDOTES stories Clever Hans and The Valiant Little lem similar to measuring strength of opinion clauses  Tailor, while our classifier captures the affective  differences by considering, e.g. distinctions in verb  Finally, emotions are not discrete objects; rather  count, interjection, POS, sentence length,  connotathey have transitional nature, and blend and overlap  tions, story subtype, and conjunctions.  along the temporal dimension. For example, (Liu,  Next, we intend to use a larger data set to conduct  Lieberman and Selker, 2003) include parallel  estia more complete study to establish mature findings.  mations of emotional activity, and include  smoothing techniques such as interpolation and decay to Vasileios Hatzivassiloglou, and Kathleen McKeown.  capture sequential and interactive emotional activity.  Predicting the semantic orientation of adjectives. In Proceedings of ACL, 174 181.  Observations from tales indicate that some emotions  are more likely to be prolonged than others.  Philip Johnson-Laird, and Keith Oatley. 1989. The language of emotions: an analysis of a semantic field. Cognition and Emotion, 3:81 123.  Conclusion  This paper has discussed an empirical study of the  2005. Generalized inference with multiple semantic role latext-based emotion prediction problem in the  dobeling systems. In Proceedings of the Annual Conference on Computational Language Learning (CoNLL), 181 184.  main of children's fairy tales, with child-directed  expressive text-to-speech synthesis as goal. Besides  Diane Litman, and Kate Forbes-Riley. 2004. Predicting  student emotions in computer-human tutoring dialogues.  In  reporting on encouraging results in a first set of  comProceedings of ACL, 351 358.  putational experiments using supervised machine  Xin Li, and Dan Roth. 2002. Learning question classifiers: the learning, we have set forth a research agenda for  role of semantic information. In Proc. International Confer-tackling the TEP problem more comprehensively.  ence on Computational Linguistics (COLING), 556 562.  Acknowledgments  Hugo Liu, Henry Lieberman, and Ted Selker. 2003. A model of textual affect sensing using real-world knowledge. In ACM  Conference on Intelligent User Interfaces, 125 132.  We are grateful to the annotators, in particular A.  We also thank two  Tony Mullen, and Nigel Collier.  Sentiment  analysis using support vector machines with diverse information anonymous reviewers for comments. This work was  sources. In Proceedings of EMNLP, 412 418.  funded by NSF under award ITR-#0205731, and NS  Bo Pang, and Lillian Lee. 2004. A sentimental education: sen-ITR IIS-0428472. The annotation is supported by  timent analysis using subjectivity summarization based on  UIUC's Research Board. The authors take sole  reminimum cuts. In Proceedings of ACL, 271 278.  sponsibility for the work.  Rosalind Picard. 1997. Affective computing. MIT Press, Cambridge, Mass.  Dan Roth. 1998. Learning to resolve natural language ambigu-ities: a unified approach. In AAAI, 806 813.  Antti Aarne. 1964. The Types of the Folk-Tale: a Classification and Bibliography. Helsinki: Suomalainen Tiedeakatemia.  Vocal communication of emotion: a  review of research paradigms. Speech Commununication, Cecilia O. Alm, and Richard Sproat. 2005. Perceptions of emo-40(1-2):227 256.  tions in expressive storytelling. INTERSPEECH 2005.  The  Balanced  Affective  Word  timent extraction from unstructured text using tabu search-Oliver  Py-WordNet  Philip Beineke, Trevor Hastie, and Shivakumar Vaithyanathan.  Futoshi Sugimoto et al. 2004. A method to classify emotional 2004. The sentimental factor: improving review classifi-expressions of text and synthesize speech. In IEEE, 611  cation via human-provided information. In Proceedings of 614.  Peter Turney. 2002. Thumbs up or thumbs down? Semantic  The generation of affect in synthesized  orientation applied to unsupervised classification of reviews.  Speech. Journal of the American Voice I/O Society, 8:1 19.  In Proceedings of ACL, 417 424.  Andrew Carlson, Chad Cumby, Nicholas Rizzolo, Jeff Rosen,  Jan van Santen et al. 2003. Applications of computer  genand Dan Roth. 1999. The SNoW Learning Architecture.  erated expressive speech for communication disorders. In  Technical Report UIUCDCS-R-99-2101, UIUC Comp. Sci.  Janyce Wiebe et al. 2004. Learning subjective language. Jour-http://www.webuse.umd.edu:9090/  nal of Computational Linguistics, 30(3):277 308.  Paul Ekman. 1993. Facial expression and emotion. American Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004. Just  how mad are you? Finding strong and weak opinion clauses.  In Proceedings of the Nineteenth National Conference on Ar-Christiane Fellbaum, Ed. 1998. WordNet: An Electronic Lexi-tificial Intelligence (AAAI), 761 769.  cal Database. MIT Press, Cambridge, Mass. 