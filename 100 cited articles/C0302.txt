 Assigning Function Tags to Parsed Text*  Don Blaheta and Eugene Charniak  Department of Computer Science  Box 1910 / 115 Waterman St.--4th floor  Brown University  It is generally recognized that the common terminal labels for syntactic constituents (NP, VP, etc.) do not exhaust the syntactic and mantic information one would like about parts of a syntactic tree. For example, the Penn bank gives each constituent zero or more tion tags' indicating semantic roles and other related information not easily encapsulated in the simple constituent labels. We present a tistical algorithm for assigning these function tags that, on text already parsed to a label level, achieves an F-measure of 87%, which rises to 99% when considering 'no tag' as a valid choice.  Introduction  Parsing sentences using statistical information gathered from a treebank was first examined a decade ago in (Chitrao and Grishman, 1990) and is by now a fairly well-studied problem ((Charniak, 1997), (Collins, 1997), (Ratna-parkhi, 1997)). But to date, the end product of the parsing process has for the most part been a bracketing with simple constituent labels like NP, VP, or SBAR. The Penn treebank contains a great deal of additional syntactic and seman-tic information from which to gather statistics; reproducing more of this information ically is a goal which has so far been mostly ignored. This paper details a process by which some of this information--the function may be recovered automatically.  In the Penn treebank, there are 20 tags  (figure 1) that can be appended to constituent  labels in order to indicate additional information  about the syntactic or semantic role of the  con* This research was funded in part by NSF grants SBR-9720368 and IGERT-9870676.  stituent. We have divided them into four gories (given in figure 2) based on those in the bracketing guidelines (Bies et al., 1995). A con-stituent can be tagged with multiple tags, but never with two tags from the same category. 1 In actuality, the case where a constituent has tags from all four categories never happens, but constituents with three tags do occur (rarely).  At a high level, we can simply say that ing the function tag information for a given text is useful just because any further information would help. But specifically, there are distinct advantages for each of the various categories. Grammatical tags are useful for any application trying to follow the thread of the text--they find the 'who does what' of each clause, which can be useful to gain information about the tion or to learn more about the behaviour of the words in the sentence. The form/function tags help to find those constituents behaving in ways not conforming to their labelled type, as well as further clarifying the behaviour of verbial phrases. Information retrieval tions specialising in describing events, as with a number of the MUC applications, could greatly benefit from some of these in determining the where-when-why of things. Noting a topicalised constituent could also prove useful to these plications, and it might also help in discourse analysis, or pronoun resolution. Finally, the 'miscellaneous' tags are convenient at various times; particularly the CLI~ 'closely related' tag, which among other things marks phrasal verbs and prepositional ditransitives.  To our knowledge, there has been no attempt  so far to recover the function tags in  parsing treebank text. In fact, we know of only  1There is a single exception in the corpus: one stituent is tagged with -LOC-I~R. This appears to be an  ADV Non-specific adverbial HLN Headline PUT Locative complement of 'put'  BNF Benefemtive LGS Logical subject SBJ Subject  CLF It-cleft L0C Location TMP Temporal  CLR 'Closely related' MNI~ Manner TPC Topic  DIR Direction N0M Nominal TTL Title  DTV Dative PRD Predicate V0C Vocative  Figure 1: Penn treebank function tags  Grammatical DTV 0.48% LGS 3.0% PRD 18.% PUT 0.26% SBJ 78.% v0c 0.025% 53.% Form/Function 0.25% NOM 6.8% 1.5% ADV 11.% 9.3% BN'F 0.072% 0.13% DIR 8.3% 41.% EXT 3.2% 0.013% LOC 25.% MNR 6.2% PI~ 5.2% 33.% 37.% Topicalisation 2.5% TPC 100% 4.2% 0.026% 3.0% 1.2% 9.2% 2.3% 1.9% 12.% 2.2% 2.2% Miscellaneous CLR 94.% CLF 0.34% HLN 2.6% TTL 3.1% 9.5% 8.8% 0.03% 0.25% 0.29%  Figure 2: Categories of function tags and their relative frequencies  one project that used them at all: (Collins, ture value would be estimated as  1997) defines certain constituents as comple-ments based on a combination of label and P(flYl, , Y,) tion tag information. This boolean condition is  P(flfl, f2,...,fj), j < n, (1)then used to train an improved parser.  where/3 refers to an empirically observed  probability. Of course, if features 1 through i only  Features co-occur a few times in the training, this value  may not be reliable, so the empirical probability We have found it useful to define our is usually smoothed: cal model in terms of features. A 'feature', in this context, is a boolean-valued function, P(flfl, Ii) erally over parse tree nodes and either node  labels or lexical items. Features can be fairly  simple and easily read off the tree (e.g. 'this node's label is X', 'this node's parent's label is Y'), or  The values for )~i can then be determined slightly more complex ('this node's head's  partcording to the number of occurrences of features of-speech is Z'). This is concordant with the  us1 through i together in the training. age in the maximum entropy literature (Berger  One way to think about equation 1 (and et al., 1996).  specifically, the notion that j will depend on  When using a number of known features to the values of fl... fn) is as follows: We begin guess an unknown one, the usual procedure is with the prior probability of f. If we have data to calculate the value of each feature, and then indicating P(flfl), we multiply in that essentially look up the empirically most hood, while dividing out the original prior. If ble value for the feature to be guessed based on we have data for/3(flfl, f2), we multiply that those known values. Due to sparse data, some in while dividing out the P(flfl) term. This is of the features later in the list may need to be repeated for each piece of feature data we have; ignored; thus the probability of an unknown at each point, we are adjusting the probability  P(f) P(flfl) P(flA, A,...,f -x)  we already have estimated. If knowledge about feature fi makes S more likely than with just fl... fi-1, the term where fi is added will be greater than one and the running probability will be adjusted upward. This gives us the new probability shown in equation 3, which is actly equivalent to equation 1 since everything except the last numerator cancels out of the equation. The value of j is chosen such that features fl..-fj are sufficiently represented in the training data; sometimes all n features are used, but often that would cause sparse data problems. Smoothing is performed on this tion exactly as before: each term is interpolated between the empirical value and the prior mated probability, according to a value of Ai that estimates confidence. But aside from haps providing a new way to think about the problem, equation 3 is not particularly useful as it is--it is exactly the same as what we had before. Its real usefulness comes, as shown in (Charniak, 1999), when we move from the  notion of a feature chain to a feature tree.  These feature chains don't capture everything we'd like them to. If there are two independent features that are each relatively sparse but sionally carry a lot of information, then putting one before the other in a chain will effectively block the second from having any effect, since its information is (uselessly) conditioned on the first one, whose sparseness will completely lute any gain. What we'd really like is to be able to have a feature tree,whereby we can condition those two sparse features independently on one common predecessor feature. As we said fore, equation 3 represents, for each feature fi, the probability of f based on fi and all its decessors, divided by the probability of f based only on the predecessors. In the chain case, this means that the denominator is conditioned on every feature from 1 to i -1, but if we use a feature tree, it is conditioned only on those tures along the path to the root of the tree.  A notable issue with feature trees as opposed to feature chains is that the terms do not all cancel out. Every leaf on the tree will be  repreFigure 3: A small example feature tree  sented in the numerator, and every fork in the tree (from which multiple nodes depend) will be represented at least once in the tor. For example: in figure 3 we have a small feature tree that has one target feature and four conditioning features. Features b and d are dependent of each other, but each depends on a; c depends directly only on b. The unsmoothed version of the corresponding equation would be  which, after cancelling of terms and smoothing, results in  Note that strictly speaking the result is not a probability distribution. It could be made into one with an appropriate normalisation--the so-called partition function in the entropy literature. However, if the dence assumptions made in the derivation of equation 4 are good ones, the partition tion will be close to 1.0. We assume this to be the case for our feature trees.  Now we return the discussion to function ging. There are a number of features that seem  parent's head's POS  headS~ parent's P~ead  alt-head's head POs alt-~ead  Figure 4: The feature tree used to guess function tags  to condition strongly for one function tag or other; we have assembled them into the feature tree shown in figure 4. 2 This figure should be relatively self-explanatory, except for the notion of an 'alternate head'; currently, an alternate head is only defined for prepositional phrases, and is the head of the object of the tional phrase. This data is very important in distinguishing, for example, 'by John' (where John might be a logical subject) from 'by next year' (a temporal modifier) and 'by selling it' (an adverbial indicating manner).  In the training phase of our experiment, we gathered statistics on the occurrence of tion tags in sections 2-21 of the Penn treebank. Specifically, for every constituent in the tree-bank, we recorded the presence of its function tags (or lack thereof) along with its ing information. From this we calculated the empirical probabilities of each function tag erenced in section 2 of this paper. Values of )~ were determined using EM on the development corpus (treebank section 24).  To test, then, we simply took the output of our parser on the test corpus (treebank section 23), and applied a postprocessing step to add function tags. For each constituent in the tree, we calculated the likelihood of each function tag according to the feature tree in figure 4, and for each category (see figure 2) we assigned the most likely function tag (which might be the null tag).  2The reader will note that the 'features' listed in the tree are in fact not boolean-valued; each node in the given tree can be assumed to stand for a chain of boolean features, one per potential value at that node, exactly one of which will be true.  To evaluate our results, we first need to mine what is 'correct'. The definition we chose is to call a constituent correct if there exists in the correct parse a constituent with the same start and end points, label, and function tag (or lack thereof). Since we treated each of the four function tag categories as a separate ture for the purpose of tagging, evaluation was also done on a per-category basis.  The denominator of the accuracy measure should be the maximum possible number we could get correct. In this case, that means excluding those constituents that were already wrong in the parser output; the parser we used attains 89% labelled precision-recall, so roughly 11% of the constituents are excluded from the function tag accuracy evaluation. (For ence, we have also included the performance of our function tagger directly on treebank parses; the slight gain that resulted is discussed below.)  Another consideration is whether to count non-tagged constituents in our evaluation. On the one hand, we could count as correct any constituent with the correct tag as well as any correctly non-tagged constituent, and use as our denominator the number of all labelled constituents. (We will henceforth refer to this as the 'with-null' measure.) On the other hand, we could just count constituents with the correct tag, and use as our denominators the total number of tagged, correctly-labelled con-stituents. We believe the latter number null') to be a better performance metric, as it is not overwhelmed by the large number of tagged constituents. Both are reported below.  Table 1: Baseline performance  Category  Grammatical Form/Function Topicalisation Miscellaneous Overall  Baseline 1 (never tag) 86.935% 91.786% 99.406% 98.436% 94.141% Baseline 2 (always choose most likely tag)  Tag Precision Recall F-measure  TPC 0.594% 100.00% 1.181%  CLR 1.317% 84.211% 2.594%  Table 2: Performance within each category  Form/Function 97.104%  There are, it seems, two reasonable baselines for this and future work. First of all, most stituents in the corpus have no tags at all, so obviously one baseline is to simply guess no tag for any constituent. Even for the most com-mon type of function tag (grammatical), this method performs with 87% accuracy. Thus the with-null accuracy of a function tagger needs to be very high to be significant here.  The second baseline might be useful in ex-amining the no-null accuracy values larly the recall): always guess the most common tag in a category. This means that every stituent gets labelled with '-SBJ-THP-TPC-CLR' (meaning that it is a topicalised temporal ject that is 'closely related' to its verb). This combination of tags is in fact entirely illegal by the treebank guidelines, but performs equately for a baseline. The precision is, of course, abysmal, for the same reasons the first baseline did so well; but the recall is (as one might expect) substantial. The performances of the two baseline measures are given in Table  ---No-null--Precision Recall F-measure 95.472% 95.837% 95.654% 80.415% 77.595% 78.980% 92.195% 93.564% 92.875% 55.644% 65.789% 60.293%  5.2 Performance in individual  In table 2, we give the results for each category. The first column is the with-null accuracy, and the precision and recall values given are the null accuracy, as noted in section 4.  Grammatical tagging performs the best of the four categories. Even using the more difficult no-null accuracy measure, it has a 96% accu-racy. This seems to reflect the fact that matical relations can often be guessed based on constituent labels, parts of speech, and high-frequency lexical items, largely avoiding data problems. Topicalisation can similarly be guessed largely on high-frequency information, and performed almost as well (93%).  On the other hand, we have the form/function tags and the 'miscellaneous' tags. These are characterised by much more semantic information, and the relationships between lexical items are very important, making sparse data a real problem. All the same, it should be noted that the performance is still far better than the baselines.  5.3 Performance with other feature  The feature tree given in figure 4 is by no means the only feature tree we could have used.  InTable 3: Overall performance on different inputs  Category Accuracy Precision Recall F-measure  Parsed 98.643% 87.173% 87.381% 87.277%  deed, we tried a number of different trees on the development corpus; this tree gave among the best overall results, with no category ing too badly. However, there is no reason to use only one feature tree for all four categories; the best results can be got by using a separate tree for each one. One can thus achieve slight (one to three point) gains in each category.  The overall performance, given in table 3, pears promising. With a tagging accuracy of about 87%, various information retrieval and knowledge base applications can reasonably pect to extract useful information.  The performance given in the first row is (like all previously given performance values) the function-tagger's performance on the labelled constituents output by our parser. For comparison, we also give its performance when run directly on the original treebank parse; since the parser's accuracy is about 89%, working rectly with the treebank means our statistics are over roughly 12% more constituents. This second version does slightly better.  The main reason that tagging does worse on the parsed version is that although the con-stituent itself may be correctly bracketed and belled, its exterior conditioning information can still be incorrect. An example of this that ac-tually occurred in the development corpus tion 24 of the treebank) is the 'that' clause in the phrase 'can swallow the premise that the wards for such ineptitude are six-figure salaries', correctly diagrammed in figure 5. The function tagger gave this SBAR an ADV tag, indicating an unspecified adverbial function. This seems tremely odd, given that its conditioning mation (nodes circled in the figure) clearly show that it is part of an NP, and hence probably ifies the preceding NN. Indeed, the statistics give the probability of an ADV tag in this ing environment as vanishingly small.  the ( premise )~  Figure 5: SBAR and conditioning info  the premise ~ ...  Figure 6: SBAR and conditioning info, as parsed  However, this was not the conditioning mation that the tagger received. The parser had instead decided on the (incorrect) parse in figure 6. As such, the tagger's decision makes much more sense, since an SBAR under two VPs whose heads are VB and MD is rather likely to be an ADV. (For instance, the 'although' clause of the sentence 'he can help, although he doesn't want to.' has exactly the conditioning ment given in figure 6, except that its cessor is a comma; and this SBAR would be rectly tagged ADV.) The SBAR itself is correctly bracketed and labelled, so it still gets counted in the statistics. Happily, this sort of case seems to be relatively rare.  Another thing that lowers the overall mance somewhat is the existence of error and consistency in the treebank tagging. Some tags seem to have been relatively easy for the human treebank taggers, and have few errors. Other tags have explicit caveats that, however justified, proved difficult to remember for the taggers--for instance, there are 37 instances of a PP being tagged with LGS (logical subject) in spite of the guidelines specifically saying, '[LGS] attaches to the NP object of by and not to the PP node itself.' (Bies et al., 1995) Each ging in the test corpus can cause up to two rious errors, one in precision and one in recall. Still another source of difficulty comes when the guidelines are vague or silent on a specific issue. To return to logical subjects, it is clear that 'the loss' is a logical subject in 'The company was hurt by the loss', but what about in 'The pany was unperturbed by the loss' ? In addition, a number of the function tags are authorised for 'metaphorical use', but what exactly constitutes such a use is somewhat inconsistently marked. It is as yet unclear just to what degree these tagging errors in the corpus are affecting our results.  Conclusion  This work presents a method for assigning tion tags to text that has been parsed to the simple label level. Because of the lack of prior research on this task, we are unable to com-pare our results to those of other researchers; but the results do seem promising. However, a great deal of future work immediately suggests itself:  Although we tested twenty or so feature trees besides the one given in figure 4, the space of possible trees is still rather un-explored. A more systematic tion into the advantages of different feature trees would be useful.  We could add to the feature tree the ues of other categories of function tag, or the function tags of various tree-relatives (parent, sibling).  One of the weaknesses of the lexical tures is sparse data; whereas the part of speech is too coarse to distinguish 'by John'  (LGS) from 'by Monday' (TMP), the lexi-cal information may be too sparse. This could be assisted by clustering the lexical items into useful categories (names, dates, etc.), and adding those categories as an ditional feature type.  There is no reason to think that this work could not be integrated directly into the parsing process, particularly if one's parser is already geared partially or entirely wards feature-based statistics; the func-tion tag information could prove quite ful within the parse itself, to rank several parses to find the most plausible.  Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural guage processing. Computational Linguistics, 22(1):39-71.  Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre, 1995. Bracketing lines for Treebank H Style Penn Treebank Project, January.  Eugene Charniak. 1997. Statistical pars-ing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelli-gence, pages 598-603, Menlo Park. AAAI Press/MIT Press.  Eugene Charniak. 1999. A inspired parser. Technical Report CS-99-12, Brown University, August.  Mahesh V. Chitrao and Ralph Grishman. 1990. Statistical parsing of messages. In DARPA Speech and Language Workshop, pages  263Michael Collins. 1997. Three generative, calised models for statistical parsing. In Pro-ceedings of the 35th Annual Meeting of the Association for Computational Linguistics,  Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum tropy models. In Proceedings of the Second Annual Conference on Empirical Methods in Natural Language Processing, pages 1-10. 