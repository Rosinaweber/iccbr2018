 Accurate Unlexicalized Parsing  Christopher D. Manning  Computer Science Department  Computer Science Department  Stanford University  Stanford University  Stanford, CA 94305-9040  Stanford, CA 94305-9040  mance of an unlexicalized PCFG over the Penn  treebank could be improved enormously simply by  anWe demonstrate that an unlexicalized PCFG can  notating each node by its parent category. The Penn  parse much more accurately than previously shown,  treebank covering PCFG is a poor tool for parsing  beby making use of simple, linguistically motivated  state splits, which break down false independence  cause the context-freedom assumptions it embodies  assumptions latent in a vanilla treebank grammar.  are far too strong, and weakening them in this way  Indeed, its performance of 86.36% (LP/LR F1) is  makes the model much better. More recently, Gildea  better than that of early lexicalized PCFG models, (2001) discusses how taking the bilexical probabil-and surprisingly close to the current  state-of-theart. This result has potential uses beyond  establishities out of a good current lexicalized PCFG parser  ing a strong lower bound on the maximum  possihurts performance hardly at all: by at most 0.5% for  ble accuracy of unlexicalized models: an  unlexicaltest text from the same domain as the training data,  ized PCFG is much more compact, easier to  repliand not at all for test text from a different domain.1  cate, and easier to interpret than more complex  lexical models, and the parsing algorithms are simpler,  But it is precisely these bilexical dependencies that  more widely understood, of lower asymptotic  combacked the intuition that lexicalized PCFGs should be  plexity, and easier to optimize.  very successful, for example in Hindle and Rooth's  demonstration from PP attachment. We take this as a  In the early 1990s, as probabilistic methods swept  reflection of the fundamental sparseness of the  lexNLP, parsing work revived the investigation of  probical dependency information available in the Penn  abilistic context-free grammars (PCFGs) (Booth and  Treebank. As a speech person would say, one  milThomson, 1973; Baker, 1979). However, early  relion words of training data just isn't enough. Even  sults on the utility of PCFGs for parse  disambiguafor topics central to the treebank's Wall Street  Jourtion and language modeling were somewhat  disapnal text, such as stocks, many very plausible depen-pointing. A conviction arose that lexicalized PCFGs dencies occur only once, for example stocks  stabi(where head words annotate phrasal nodes) were  lized, while many others occur not at all, for exam-the key tool for high performance PCFG parsing.  This approach was congruent with the great success  The best-performing lexicalized PCFGs have  inof word n-gram models in speech recognition, and  creasingly made use of subcategorization 3 of the  drew strength from a broader interest in lexicalized  1There are minor differences, but all the current best-known grammars, as well as demonstrations that lexical de-lexicalized PCFGs employ both monolexical statistics, which pendencies were a key tool for resolving ambiguities  describe the phrasal categories of arguments and adjuncts that such as PP attachments (Ford et al., 1982; Hindle and  appear around a head lexical item, and bilexical statistics, or de-Rooth, 1993). In the following decade, great success  pendencies, which describe the likelihood of a head word taking as a dependent a phrase headed by a certain other word.  in terms of parse disambiguation and even language  2This observation motivates various or  similaritymodeling was achieved by various lexicalized PCFG  based approaches to combating sparseness, and this remains a models (Magerman, 1995; Charniak, 1997; Collins,  promising avenue of work, but success in this area has proven 1999; Charniak, 2000; Charniak, 2001).  somewhat elusive, and, at any rate, current lexicalized PCFGs do simply use exact word matches if available, and interpolate However, several results have brought into ques-with syntactic category-based estimates when they are not.  tion how large a role lexicalization plays in such  3In this paper we use the term subcategorization in the origi-parsers.  Johnson (1998) showed that the  perfornal general sense of Chomsky (1965), for where a syntactic  catcategories appearing in the Penn treebank. Charniak  constants. An unlexicalized PCFG parser is much  (2000) shows the value his parser gains from  parentsimpler to build and optimize, including both  stanannotation of nodes, suggesting that this  informadard code optimization techniques and the  investigation is at least partly complementary to information  tion of methods for search space pruning (Caraballo  derivable from lexicalization, and Collins (1999)  and Charniak, 1998; Charniak et al., 1998).  uses a range of linguistically motivated and  careIt is not our goal to argue against the use of lex-fully hand-engineered subcategorizations to break  icalized probabilities in high-performance  probabidown wrong context-freedom assumptions of the  listic parsing. It has been comprehensively  demonnaive Penn treebank covering PCFG, such as  differstrated that lexical dependencies are useful in  reentiating base NPs from noun phrases with phrasal  solving major classes of sentence ambiguities, and a  modifiers, and distinguishing sentences with empty  parser should make use of such information where  subjects from those where there is an overt subject  We focus here on using unlexicalized,  NP. While he gives incomplete experimental results  structural context because we feel that this  inforas to their efficacy, we can assume that these features  mation has been underexploited and  underappreciwere incorporated because of beneficial effects on  ated. We see this investigation as only one part of  parsing that were complementary to lexicalization.  the foundation for state-of-the-art parsing which  emIn this paper, we show that the parsing  perforploys both lexical and structural conditioning.  mance that can be achieved by an unlexicalized  PCFG is far higher than has previously been  demonstrated, and is, indeed, much higher than community  To facilitate comparison with previous work, we  wisdom has thought possible. We describe several  trained our models on sections 2 21 of the WSJ  secsimple, linguistically motivated annotations which  tion of the Penn treebank. We used the first 20 files  do much to close the gap between a vanilla PCFG  (393 sentences) of section 22 as a development set  and state-of-the-art lexicalized models. Specifically,  ( devset). This set is small enough that there is no-we construct an unlexicalized PCFG which  outperticeable variance in individual results, but it allowed  forms the lexicalized PCFGs of Magerman (1995)  rapid search for good features via continually  reparsand Collins (1996) (though not more recent models,  ing the devset in a partially manual hill-climb. All of  such as Charniak (1997) or Collins (1999)).  section 23 was used as a test set for the final model.  One benefit of this result is a much-strengthened  For each model, input trees were annotated or  translower bound on the capacity of an unlexicalized  formed in some way, as in Johnson (1998). Given  PCFG. To the extent that no such strong baseline has  a set of transformed trees, we viewed the local trees  been provided, the community has tended to greatly  as grammar rewrite rules in the standard way, and  overestimate the beneficial effect of lexicalization in  probabilistic parsing, rather than looking critically  for rule probabilities.5 To parse the grammar, we  at where lexicalized probabilities are both needed to used a simple array-based Java implementation of  make the right decision and available in the training a generalized CKY parser, which, for our final best  data. Secondly, this result affirms the value of  linmodel, was able to exhaustively parse all sentences  guistic analysis for feature discovery. The result has  in section 23 in 1GB of memory, taking  approxiother uses and advantages: an unlexicalized PCFG is  mately 3 sec for average length sentences.6  easier to interpret, reason about, and improve than  the more complex lexicalized models. The grammar  The tagging probabilities were smoothed to accommodate  unknown words.  The quantity P( t ag|w or d) was estimated representation is much more compact, no longer re-as follows: words were split into one of several categories quiring large structures that store lexicalized proba-w or dclass, based on capitalization, suffix, digit, and other bilities. The parsing algorithms have lower asymp-character features. For each of these categories, we took the totic complexity4 and have much smaller grammar  maximum-likelihood estimate of P( t ag|w or dclass). This distribution was used as a prior against which observed taggings, if any, were taken, giving P( t ag|w or d) = [ c( t ag, w or d) +  egory is divided into several subcategories, for example  divid P( t ag|w or dclass)]/[ c(w or d)+ ]. This was then inverted to ing verb phrases into finite and non-finite verb phrases, rather give P(w or d| t ag). The quality of this tagging model impacts than in the modern restricted usage where the term refers only all numbers; for example the raw treebank grammar's devset F1  to the syntactic argument frames of predicators.  is 72.62 with it and 72.09 without it.  4 O( n 3) vs. O( n 5) for a naive implementation, or vs. O( n 4) 6The parser is available for download as open source at:  if using the clever approach of Eisner and Satta (1999).  Figure 1: The v=1, h=1 markovization of VP VBZ NP PP.  Figure 2: Markovizations: F1 and grammar size.  The traditional starting point for unlexicalized  parsing is the raw n-ary treebank grammar read from  child always matters). It is a historical accident that  training trees (after removing functional tags and  the default notion of a treebank PCFG grammar takes  null elements). This basic grammar is imperfect in  v = 1 (only the current node matters vertically) and  two well-known ways. First, the category symbols  h = (rule right hand sides do not decompose at  are too coarse to adequately render the expansions  all). On this view, it is unsurprising that increasing  independent of the contexts. For example, subject  v and decreasing h have historically helped.  NP expansions are very different from object NP  exAs an example, consider the case of v = 1,  pansions: a subject NP is 8.7 times more likely than  h = 1. If we start with the rule VP VBZ NP  an object NP to expand as just a pronoun. Having  PP PP, it will be broken into several stages, each a  separate symbols for subject and object NPs allows  binary or unary rule, which conceptually represent  this variation to be captured and used to improve  a head-outward generation of the right hand size, as  parse scoring. One way of capturing this kind of  shown in figure 1. The bottom layer will be a unary  external context is to use parent annotation, as pre-over the head declaring the goal: hVP: [VBZ]i  sented in Johnson (1998). For example, NPs with S  VBZ. The square brackets indicate that the VBZ is  parents (like subjects) will be marked NP S, while  the head, while the angle brackets hXi indicates that  NPs with VP parents (like objects) will be NP VP.  the symbol hXi is an intermediate symbol  (equivThe second basic deficiency is that many rule  alently, an active or incomplete state).  The next  types have been seen only once (and therefore have  layer up will generate the first rightward sibling of  their probabilities overestimated), and many rules  the head child: hVP: [VBZ]. . . NPi hVP: [VBZ]i  which occur in test sentences will never have been  NP. Next, the PP is generated: hVP: [VBZ]. . . PPi  seen in training (and therefore have their  probabilihVP: [VBZ]. . . NPi PP. We would then branch off left  ties underestimated see Collins (1999) for  analysiblings if there were any.7 Finally, we have another  sis). Note that in parsing with the unsplit grammar,  unary to finish the VP. Note that while it is  convenient to think of this as a head-outward process,  failure, but rather a possibly very weird parse  (Charthese are just PCFG rewrites, and so the actual scores  niak, 1996). One successful method of combating  attached to each rule will correspond to a downward  sparsity is to markovize the rules (Collins, 1999). In generation order.  particular, we follow that work in markovizing out  Figure 2 presents a grid of horizontal and  vertifrom the head child, despite the grammar being  uncal markovizations of the grammar. The raw  treelexicalized, because this seems the best way to  capbank grammar corresponds to v = 1, h = (the  ture the traditional linguistic insight that phrases are  upper right corner), while the parent annotation in  organized around a head (Radford, 1988).  (Johnson, 1998) corresponds to v = 2, h = , and  Both parent annotation (adding context) and RHS  the second-order model in Collins (1999), is broadly  markovization (removing it) can be seen as two  ina smoothed version of v = 2, h = 2. In  addistances of the same idea. In parsing, every node has  tion to exact n th-order models, we tried  variablea vertical history, including the node itself, parent,  grandparent, and so on. A reasonable assumption is  7In our system, the last few right children carry over as pre-that only the past v vertical ancestors matter to the  ceding context for the left children, distinct from common practice. We found this wrapped horizon to be beneficial, and it current expansion. Similarly, only the previous h  also unifies the infinite order model with the unmarkovized raw horizontal ancestors matter (we assume that the head  ROOT  Revenue was  CD  CD  including  CONJP  SPLIT-AUX  SPLIT-CC  down slightly from $  CD  CD  Figure 4: An error which can be resolved with the  UNARYINTERNAL annotation (incorrect baseline parse shown).  grammar. Although it does not necessarily jump out  RIGHT-REC-NP  of the grid at first glance, this point represents the  best compromise between a compact grammar and  Figure 3: Size and devset performance of the cumulatively an-useful markov histories.  notated models, starting with the markovized baseline. The  right two columns show the change in F1 from the baseline for each annotation introduced, both cumulatively and for each sin-3  External vs. Internal Annotation  gle annotation applied to the baseline in isolation.  The two major previous annotation strategies,  parent annotation and head lexicalization, can be seen  history models similar in intent to those described  as instances of external and internal annotation,  rein Ron et al. (1994). For variable horizontal  hisspectively.  Parent annotation lets us indicate an  important feature of the external environment of a  occurrences of a symbol. For example, if the symbol  node which influences the internal expansion of that  hVP: [VBZ]. . . PP PPi were too rare, we would  colnode. On the other hand, lexicalization is a  (radilapse it to hVP: [VBZ]. . . PPi. For vertical histories,  cal) method of marking a distinctive aspect of the  we used a cutoff which included both frequency and  otherwise hidden internal contents of a node which  mutual information between the history and the  exinfluence the external distribution. Both kinds of  anpansions (this was not appropriate for the horizontal  notation can be useful. To identify split states, we  case because MI is unreliable at such low counts).  add suffixes of the form -X to mark internal content  Figure 2 shows parsing accuracies as well as the  features, and X to mark external features.  number of symbols in each markovization. These  To illustrate the difference, consider unary  prosymbol counts include all the intermediate states  ductions. In the raw grammar, there are many  unarwhich represent partially completed constituents.  ies, and once any major category is constructed over  The general trend is that, in the absence of further  a span, most others become constructible as well  usannotation, more vertical annotation is better even  ing unary chains (see Klein and Manning (2001) for  exhaustive grandparent annotation. This is not true  discussion). Such chains are rare in real treebank  for horizontal markovization, where the  variabletrees: unary rewrites only appear in very specific  order second-order model was superior. The best  contexts, for example S complements of verbs where  entry, v = 3, h 2, has an F1 of 79.74, already  the S has an empty, controlled subject. Figure 4  a substantial improvement over the baseline.  shows an erroneous output of the parser, using the  In the remaining sections, we discuss other  anbaseline markovized grammar. Intuitively, there are  notations which increasingly split the symbol space.  several reasons this parse should be ruled out, but  Since we expressly do not smooth the grammar, not  one is that the lower S slot, which is intended  priall splits are guaranteed to be beneficial, and not all  marily for S complements of communication verbs,  sets of useful splits are guaranteed to co-exist well.  is not a unary rewrite position (such complements  In particular, while v = 3, h 2 markovization is  usually have subjects). It would therefore be natural  good on its own, it has a large number of states and  to annotate the trees so as to confine unary  producdoes not tolerate further splitting well. Therefore,  tions to the contexts in which they are actually  apwe base all further exploration on the v 2, h 2  propriate. We tried two annotations. First,  UNARYINTERNAL marks (with a -U) any nonterminal node  which has only one child. In isolation, this resulted  in an absolute gain of 0.55% (see figure 3). The  to VB  to  same sentence, parsed using only the baseline and  UNARY-INTERNAL, is parsed correctly, because the  VP rewrite in the incorrect parse ends with an S  VPU with very low probability.8  Alternately, UNARY-EXTERNAL, marked nodes  which had no siblings with  It was similar to  Figure 5: An error resolved with the TAG-PA annotation (of the but provided far less marginal benefit on top of  IN tag): (a) the incorrect baseline parse and (b) the correct TAG-other later features (none at all on top of  UNARYPA parse. SPLIT-IN also resolves this error.  INTERNAL for our top models), and was discarded.9  One restricted place where external unary  annotasomewhat regularly occurs in a non-canonical  position was very useful, however, was at the  pretermition, its distribution is usually distinct. For example,  nal level, where internal annotation was  meaningthe most common adverbs directly under ADVP are  less. One distributionally salient tag conflation in  also (1599) and now (544). Under VP, they are n't the Penn treebank is the identification of  demonstra(3779) and not (922). Under NP, only (215) and just tives ( that, those) and regular determiners ( the, a).  (132), and so on. TAG-PA brought F  Splitting  DT tags based on whether they were only  tially, to 80.62%.  children (UNARY-DT) captured this distinction. The  In addition to the adverb case, the Penn tag set  same external unary annotation was even more  efconflates various grammatical distinctions that are  fective when applied to adverbs (UNARY-RB),  discommonly made in traditional and generative  gramtinguishing, for example, as well from also). Be-mar, and from which a parser could hope to get  useyond these cases, unary tag marking was  detrimenful information. For example, subordinating  contal. The F1 after UNARY-INTERNAL, UNARY-DT,  junctions ( while, as, if ), complementizers ( that, for), and UNARY-RB was 78.86%.  and prepositions ( of, in, from) all get the tag IN.  Many of these distinctions are captured by  PA (subordinating conjunctions occur under S and  The idea that part-of-speech tags are not fine-grained  prepositions under PP), but are not (both  suborenough to abstract away from specific-word  bedinating conjunctions and complementizers appear  haviour is a cornerstone of lexicalization.  The  Also, there are exclusively  nounUNARY-DT annotation, for example, showed that the  modifying prepositions ( of ), predominantly  verbdeterminers which occur alone are usefully  distinmodifying ones ( as), and so on.  The annotation  guished from those which occur with other  nomiSPLIT-IN does a linguistically motivated 6-way split  nal material. This marks the DT nodes with a single  of the IN tag, and brought the total to 81.19%.  bit about their immediate external context: whether  Figure 5 shows an example error in the baseline  there are sisters. Given the success of parent  annowhich is equally well fixed by either TAG-PA or  tation for nonterminals, it makes sense to parent  anSPLIT-IN. In this case, the more common nominal  notate tags, as well (TAG-PA). In fact, as figure 3  use of works is preferred unless the IN tag is anno-shows, exhaustively marking all preterminals with  tated to allow if to prefer S complements.  their parent category was the most effective single  We also got value from three other annotations  annotation we tried. Why should this be useful?  which subcategorized tags for specific lexemes.  Most tags have a canonical category. For example,  First we split off auxiliary verbs with the  SPLITNNS tags occur under NP nodes (only 234 of 70855  AUX annotation, which appends BE to all forms  do not, mostly mistakes).  However, when a tag  of be and HAVE to all forms of have.10 More minorly, SPLIT-CC marked conjunction tags to indicate  8Note that when we show such trees, we generally only  show one annotation on top of the baseline at a time.  More10This is an extended uniform version of the partial  auxilover, we do not explicitly show the binarization implicit by the iary annotation of Charniak (1997), wherein all auxiliaries are horizontal markovization.  marked as AUX and a -G is added to gerund auxiliaries and  9These two are not equivalent even given infinite data.  whether or not they were the strings [ Bb] ut or &, VP S  each of which have distinctly different distributions  from other conjunctions. Finally, we gave the  perto  cent sign (%) its own tag, in line with the dollar sign  appear  appear  ($) already having its own. Together these three  anCD NNS IN  CD NNS IN NP PP last  notations brought the F  three times on NNP JJ  1 to 81.81%.  three times on NNP  What is an Unlexicalized Grammar?  Around this point, we must address exactly what we  Figure 6: An error resolved with the TMP-NP annotation: (a) mean by an unlexicalized PCFG. To the extent that  the incorrect baseline parse and (b) the correct TMP-NP parse.  we go about subcategorizing POS categories, many  of them might come to represent a single word. One  tively means that the subcategories that we break off  might thus feel that the approach of this paper is to  must themselves be very frequent in the language.  walk down a slippery slope, and that we are merely  In such a framework, if we try to annotate  catearguing degrees. However, we believe that there is a  gories with any detailed lexical information, many  fundamental qualitative distinction, grounded in  linsentences either entirely fail to parse, or have only  guistic practice, between what we see as permitted  extremely weird parses. The resulting battle against  in an unlexicalized PCFG as against what one finds  sparsity means that we can only afford to make a few  and hopes to exploit in lexicalized PCFGs. The  didistinctions which have major distributional impact.  vision rests on the traditional distinction between  Even with the individual-lexeme annotations in this  function words (or closed-class words) and content section, the grammar still has only 9255 states com-words (or open class or lexical words). It is  stanpared to the 7619 of the baseline model.  dard practice in linguistics, dating back decades,  to annotate phrasal nodes with important  functionAnnotations Already in the Treebank  word distinctions, for example to have a CP[ for]  or a PP[ to], whereas content words are not part of  At this point, one might wonder as to the wisdom  grammatical structure, and one would not have  speof stripping off all treebank functional tags, only  cial rules or constraints for an NP[ stocks], for exam-to heuristically add other such markings back in to  ple. We follow this approach in our model: various  the grammar. By and large, the treebank out-of-the  closed classes are subcategorized to better represent  package tags, such as PP-LOC or ADVP-TMP, have  important distinctions, and important features  comnegative utility. Recall that the raw treebank  grammonly expressed by function words are annotated  mar, with no annotation or markovization, had an F1  onto phrasal nodes (such as whether a VP is finite,  of 72.62% on our development set. With the  funcor a participle, or an infinitive clause). However, no  tional annotation left in, this drops to 71.49%. The  use is made of lexical class words, to provide either  dropped even further, all the way to 72.87%, when  At any rate, we have kept ourselves honest by  esthese annotations were included.  timating our models exclusively by maximum  likeNonetheless, some distinctions present in the raw  lihood estimation over our subcategorized  gramtreebank trees were valuable. For example, an NP  mar, without any form of interpolation or  shrinkwith an S parent could be either a temporal NP or a  age to unsubcategorized categories (although we do  subject. For the annotation TMP-NP, we retained the  markovize rules, as explained above). This  effecoriginal -TMP tags on NPs, and, furthermore,  propagated the tag down to the tag of the head of the NP.  11It should be noted that we started with four tags in the Penn This is illustrated in figure 6, which also shows an  treebank tagset that rewrite as a single word: EX ( there), WP$  example of its utility, clarifying that CNN last night  ( whose), # (the pound sign), and TO), and some others such as  is not a plausible compound and facilitating the  othWP, POS, and some of the punctuation tags, which rewrite  as barely more. To the extent that we subcategorize tags, there erwise unusual high attachment of the smaller NP.  will be more such cases, but many of them already exist in other TMP-NP brought the cumulative F1 to 82.25%. Note  tag sets. For instance, many tag sets, such as the Brown and that this technique of pushing the functional tags  CLAWS (c5) tagsets give a separate sets of tags to each form of the verbal auxiliaries be, do, and have, most of which rewrite as down to preterminals might be useful more gener-only a single word (and any corresponding contractions).  ally; for example, locative PPs expand roughly the  ROOT  ROOT  Error analysis at this point suggested that many  remaining errors were attachment level and  conjunction scope. While these kinds of errors are  undoubtThis  This  edly profitable targets for lexical preference, most  attachment mistakes were overly high attachments,  indicating that the overall right-branching tendency  of English was not being captured. Indeed, this  tenFigure 7: An error resolved with the SPLIT-VP annotation: (a) dency is a difficult trend to capture in a PCFG be-the incorrect baseline parse and (b) the correct SPLIT-VP parse.  cause often the high and low attachments involve the  very same rules. Even if not, attachment height is  same way as all other PPs (usually as IN NP), but  not modeled by a PCFG unless it is somehow  exthey do tend to have different prepositions below IN.  plicitly encoded into category labels. More  comA second kind of information in the original  plex parsing models have indirectly overcome this  trees is the presence of empty elements. Following  by modeling distance (rather than height).  Collins (1999), the annotation GAPPED-S marks S  Linear distance is difficult to encode in a PCFG  nodes which have an empty subject (i.e., raising and  marking nodes with the size of their yields  mascontrol constructions). This brought F  sively multiplies the state space.13 Therefore, we  1 to 82.28%.  wish to find indirect indicators that distinguish high  Head Annotation  attachments from low ones. In the case of two PPs  following a NP, with the question of whether the  The notion that the head word of a constituent can  second PP is a second modifier of the leftmost NP  affect its behavior is a useful one. However, often  or should attach lower, inside the first PP, the  imthe head tag is as good (or better) an indicator of how  portant distinction is usually that the lower site is a  a constituent will behave.12 We found several head  non-recursive base NP. Collins (1999) captures this  annotations to be particularly effective. First,  posnotion by introducing the notion of a base NP, in  sessive NPs have a very different distribution than  which any NP which dominates only preterminals is  other NPs in particular, NP NP rules are only  marked with a -B. Further, if an NP-B does not have  used in the treebank when the leftmost child is  posa non-base NP parent, it is given one with a unary  sessive (as opposed to other imaginable uses like for  production. This was helpful, but substantially less  New York lawyers, which is left flat). To address this, effective than marking base NPs without introducing POSS-NP marked all possessive NPs. This brought  the unary, whose presence actually erased a useful  the total F1 to 83.06%. Second, the VP symbol is  internal indicator base NPs are more frequent in  very overloaded in the Penn treebank, most severely  subject position than object position, for example. In  in that there is no distinction between finite and  inisolation, the Collins method actually hurt the  basefinitival VPs. An example of the damage this  conline (absolute cost to F1 of 0.37%), while skipping  flation can do is given in figure 7, where one needs  the unary insertion added an absolute 0.73% to the  to capture the fact that present-tense verbs do not  baseline, and brought the cumulative F1 to 86.04%.  generally take bare infinitive VP complements. To  In the case of attachment of a PP to an NP  eiallow the finite/non-finite distinction, and other verb  ther above or inside a relative clause, the high NP  type distinctions, SPLIT-VP annotated all VP nodes  is distinct from the low one in that the already  modwith their head tag, merging all finite forms to a  sinified one contains a verb (and the low one may be  gle tag VBF. In particular, this also accomplished  a base NP as well). This is a partial explanation of  Charniak's gerund-VP marking. This was extremely  the utility of verbal distance in Collins (1999). To  useful, bringing the cumulative F1 to 85.72%, 2.66%  absolute improvement (more than its solo  improve13The inability to encode distance naturally in a naive PCFG  ment over the baseline).  is somewhat ironic. In the heart of any PCFG parser, the fundamental table entry or chart item is a label over a span, for ex-12This is part of the explanation of why (Charniak, 2000)  ample an NP from position 0 to position 5. The concrete use of finds that early generation of head tags as in (Collins, 1999) a grammar rule is to take two adjacent span-marked labels and is so beneficial. The rest of the benefit is presumably in the combine them (for example NP[0,5] and VP[5,12] into S[0,12]).  availability of the tags for smoothing purposes.  Yet, only the labels are used to score the combination.  Length 40  CB  0 CB  Acknowledgements  Collins (1996)  This paper is based on work supported in part by the  this paper  National Science Foundation under Grant No.  IISCollins (1999)  0085896, and in part by an IBM Faculty Partnership  Award to the second author.  Length 100  CB  0 CB  this paper  Figure 8: Results of the final model on the test set (section 23).  James K. Baker. 1979. Trainable grammars for speech recognition. In D. H. Klatt and J. J. Wolf, editors, Speech Communication Papers for the 97th Meeting of the Acoustical Society capture this, DOMINATES-V marks all nodes which  of America, pages 547 550.  dominate any verbal node (V*, MD) with a -V. This  Taylor L. Booth and Richard A. Thomson. 1973. Applying  brought the cumulative F  probability measures to abstract languages. IEEE Transac-1 to 86.91%. We also tried  tions on Computers, C-22:442 450.  marking nodes which dominated prepositions and/or  Sharon A. Caraballo and Eugene Charniak. 1998. New figures  conjunctions, but these features did not help the  cuof merit for best-first probabilistic chart parsing. Computa-mulative hill-climb.  tional Linguistics, 24:275 298.  Eugene Charniak, Sharon Goldwater, and Mark Johnson. 1998.  The final distance/depth feature we used was an  Edge-based best-first chart parsing. In Proceedings of the explicit attempt to model depth, rather than use  Sixth Workshop on Very Large Corpora, pages 127 133.  distance and linear intervention as a proxy. With  Eugene Charniak. 1996. Tree-bank grammars. In Proc. of  the 13th National Conference on Artificial Intelligence, pp.  RIGHT-REC-NP, we marked all NPs which contained  another NP on their right periphery (i.e., as a  rightEugene Charniak. 1997. Statistical parsing with a context-free most descendant). This captured some further at-grammar and word statistics. In Proceedings of the 14th Na-tachment trends, and brought us to a final  developEugene Charniak. 2000. A maximum-entropy-inspired parser.  1 of 87.04%.  Eugene Charniak. 2001. Immediate-head parsing for language  Noam Chomsky. 1965. Aspects of the Theory of Syntax. MIT  We took the final model and used it to parse  secMichael John Collins. 1996. A new statistical parser based on tion 23 of the treebank.  Figure 8 shows the  results. The test set F1 is 86.32% for 40 words,  M. Collins. 1999. Head-Driven Statistical Models for Natural already higher than early lexicalized models, though  Language Parsing. Ph.D. thesis, Univ. of Pennsylvania.  of course lower than the state-of-the-art parsers.  Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and head-automaton grammars. In ACL 37, pages 457 464.  Conclusion  Marilyn Ford, Joan Bresnan, and Ronald M. Kaplan. 1982. A  competence-based theory of syntactic closure. In Joan  BresThe advantages of unlexicalized grammars are clear  nan, editor, The Mental Representation of Grammatical Relations, pages 727 796. MIT Press, Cambridge, MA.  enough easy to estimate, easy to parse with, and  Daniel Gildea. 2001. Corpus variation and parser performance.  and space-efficient. However, the dismal  perIn 2001 Conference on Empirical Methods in Natural  Lanformance of basic unannotated unlexicalized  gramguage Processing (EMNLP).  mars has generally rendered those advantages  irrelDonald Hindle and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103 120.  evant. Here, we have shown that, surprisingly, the  Mark Johnson. 1998. PCFG models of linguistic tree represen-maximum-likelihood estimate of a compact  unlexitations. Computational Linguistics, 24:613 632.  calized PCFG can parse on par with early lexicalized  Dan Klein and Christopher D. Manning. 2001. Parsing with  parsers. We do not want to argue that lexical  setreebank grammars: Empirical bounds, theoretical models,  and the structure of the Penn treebank. In ACL 39/EACL 10.  lection is not a worthwhile component of a  state-ofDavid M. Magerman. 1995. Statistical decision-tree models for the-art parser certain attachments, at least, require  it though perhaps its necessity has been overstated.  Andrew Radford. 1988. Transformational Grammar. Cam-Rather, we have shown ways to improve parsing,  bridge University Press, Cambridge.  Dana Ron, Yoram Singer, and Naftali Tishby. 1994. The power some easier than lexicalization, and others of which  of amnesia. Advances in Neural Information Processing Sys-are orthogonal to it, and could presumably be used  to benefit lexicalized parsers as well. 