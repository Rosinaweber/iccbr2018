 Distilling Opinion in Discourse: A Preliminary Study  Nicholas Asher and Farah Benamara  Yvette Yannick Mathieu  IRIT-CNRS Toulouse,  LLF-CNRS Paris,  {asher, benamara}@irit.fr  opinion analysis using discourse relations. This  analysis is based on a lexical semantic analysis of  In this paper, we describe a preliminary  a wide class of expressions coupled together with  study for a discourse based opinion  catean analysis of how clauses involving these  expresgorization and propose a new annotation  sions are related to each other within a discourse.  schema for a deep contextual opinion  analThe aim of this paper is to establish the  feasibilysis using discourse relations.  ity and stability of our annotation scheme at the  Introduction  subsentential level and propose a way to use this  scheme to calculate the overall opinion expressed  Computational approaches to sentiment analysis  in a text on a given topic.  eschew a general theory of emotions and focus  on extracting the affective content of a text from  A lexical semantic analysis of opinion  the detection of expressions of sentiment. These  We categorize opinion expressions using a  typology of four top-level categories (see table 1):  REwards some topic. Using information retrieval, text  PORTING expressions, which provide an  evalumining and computational linguistic techniques  toation of the degree of commitment of both the  gether with a set of dedicated linguistic resources,  holder and the subject of the reporting verb,  JUDGone can calculate opinions exploiting the detected  MENT expressions, which express normative  eval bag of sentiment words . Recently, new  methuations of objects and actions, ADVISE  expresods aim to assign fine-grained affect labels based  sions, which express an opinion on a course of  acon various psychological theories e.g., the MPQA  tion for the reader, and SENTIMENT expressions,  project (Wiebe et al., 2005) based on literary  thewhich express feelings (for a more detailed  deory and linguistics and work by (Read et al., 2007)  scription of our categories see (Asher et al, 2008)).  based on the Appraisal framework (Martin and  Our approach to categorize opinions uses the  White, 2005).  lexical semantic research of (Wierzbicka, 1987),  We think there is still room for improvement in  (Levin, 1993) and (Mathieu, 2004). From these  this field. To get an accurate appraisal of  opinclassifications, we selected opinion verb classes  ion in texts, NLP systems have to go beyond  posand verbs which take opinion expressions within  itive/negative classification and to identify a wide  their scope and which reflect the holder's  comrange of opinion expressions, as well as how they  mitment on the opinion expressed. We removed  are discursively related in the text. In this paper,  some verb classes, modified others and merged  rewe describe a preliminary study for a discourse  lated classes into new ones. Subjective verbs were  based opinion categorization. We propose a new  split into these new categories which were then  exannotation schema for a fine-grained contextual  tended by adding nouns and adjectives.  Licensed under the Creative Commons  Our classification is the same for French and  EnAttribution-Noncommercial-Share Alike 3.0 Unported li-glish. It differs from psychologically based  classiSome rights reserved.  fications like Martin's Appraisal system : in ours  Coling 2008: Companion volume Posters and Demonstrations, pages 7 10  SubGroups  Using the discourse theory SDRT (Asher and  Lasa) Inform  inform, notify, explain  assert, claim, insist  carides, 2003) as our formal framework, our four  say, announce, report  Reporting  opinion categories are used to label opinion  exe) Think  think, reckon, consider  f) Guess  presume, suspect, wonder  pressions within a discourse segment. For  example, there are three opinion segments in the  sengood, shameful, brilliant  j) Recommend  tence S3: [[It's poignant] d , [sad] e ] g and at the Advise  wish, hope  same time [horrible] f  m) Anger/CalmDown  irritation, anger  astound, daze, impress  We use five types of rhetorical relations:  CONTRAST, CORRECTION, SUPPORT, RESULT and  fear, frighten, alarm  hurt, chock  s) Sadness/Joy  bore, distraction  see (Asher et al, 2008)). Within a discourse  segdisarm, move, touch  ment, negations were treated as reversing the  polarities of the opinion expressions within their  Table 1: Top-Level opinion categories.  scope. Conditionals are hard to interpret because  they affect the opinion expressed within the  consethe contents of the JUDGMENT and SENTIMENT  quent of a conditional in different ways. For  example, conditionals,expressions of ADVISE can block  for SENTIMENT descriptions with 14 sub-classes.  the advice or reverse it. Thus if you want to waste  Ours is also broader: the REPORTING and the  ADyou money, buy this movie will be annotated as a VISE categories do not appear as such in the Ap-recommendation not to buy it. On the other hand,  praisal system. In addition, we choose not to build  conditionals can also strengthen the  recommendaour discourse based opinion categorization on the  tion as in if you want to have good time, go and  top of MPQA (Wiebe et al, 2005) for two reasons.  see this movie. We have left the treatment of con-First, we suggest a more detailed analysis of  priditionals as well as disjunctions for future work.  vate states by defining additional sets of opinion  classes such as HOPES and RECOMMENDATIONS.  Shallow Semantic Representation  We think that refined categories are needed to build  In order to represent and evaluate the overall  a more nuanced appraisal of opinion expressions  opinion of a document, we characterize discourse  in discourse. Second, text anchors which  corresegments using a shallow semantic  representaspond to opinion in MPQA are not well defined  tion using a feature structure (FS) as described  since each annotator is free to identify expression  in (Asher et al, 2008). Figure 1 shows the  disboundaries. This is problematic if we want to  incursive representation of the review movie S4:  [This film is amazing.] a . [[One leaves not com-tion task. MPQA often groups discourse  indicapletely convinced] b.1 , but [one is overcome] b.2 ].  tors (but, because, etc.) with opinion expressions  [[It's poignant] c.1 , [sad] c.2 ] and at the same time leading to no guarantee that the text anchors will  [horrible] c.3 ].[Buy it] d . [You won't regret it] e .  correspond to a well formed discourse unit.  Towards a Discursive Representation of  derstanding opinions conveyed by a text. The  following simple examples drawn from our French  corpus show that discourse relations affect the  strength of a given sentiment. S1 : [I agree with  you] a even if I was shocked and S2 : Buy the DVD,  [you will not regret it]  Figure 1: Discursive representation of S4.  b. Opinions in S1 and S2  are positive but the contrast introduced by even in S1 decreases the strength of the opinion expressed  Once we have constructed the discursive  reprein (a) whereas the explanation provided by (b) in  sentation of a text, we have to combine the  difS2 increases the strength of the recommendation.  ferent FS in order to get a general representation  that goes beyond standard positive/negative  repreion on the same topic) or a SUPPORT relation  sentation of opinion texts. In this section, we first  (holders share the same point of view) ; and  explain the combination process of FS. We then  H T = {(hi, tj, type)/hi H and tj T and type =  show how an opinion text can be summarized  usattribution/commitment} means that an opinion  toing a graphical representation.  wards a topic tj is attributed or committed to a  The combination of low-level FS is performed  holder hi. For example, in John said that the film  in two steps: (1) combine the structures related  was horrible, the opinion is only attributed to John by coordinating relations (such as CONTRAST and  because verbs from the TELL group do not  conCONTINUATION). In figure 1, this allows to build  vey anything about the author view. However, in  from the segments b.1 and b.2 a new FS ; (2)  comJohn infomed the commitee that the situation was  bine the strutures related via subordinating  relahorrible, the writer takes the information to be es-tions (such as SUPPORT and RESULT) in a bottom  tablished. The figure 2 below shows the general  up way. In figure 1, the FS of the segment a is  comrepresentation of the movie review S4.  bined with the structure deduced from step 1.  During this process, a set of dedicated rules is used.  The procedure is formalized as follows. Let a, b be  two segments related by the rhetorical relation R  such as: R(a, b). Let Sa, Sb be the FS associated  Figure 2: General representation of S4.  respectively to a and b i.e Sa : [category : [groupa :  and Sb : [category : [groupb : subgroupb], modality :  [polarity : pb, strength : sb] ] and let S : [category : Experiments and Preliminary Results  [group], modality : [polarity : p, strength : s] ] be the FS deduced from the combination of S  We have analyzed the distribution of our categories  a and  in three different types of digital corpora, each  b. Some of our rules are:  with a distinctive style and audience : movie  reCONTINUATIONS strengthen the polarity of the  views, Letters to the Editor and news reports in  common opinion.  One of the rule used is: if  (group  English and in French. We randomly selected 150  a = groupb) and (subgroupa 6= subgroupb)) then  articles for French corpora (around 50 articles for  if ((pa = neutral) and (pb 6= neutral)) then group =  group  each genre). Two native French speakers  annoa and p = pb and s = max(sa, sb), as in moving  tated respectively around 546 and 589 segments.  and sad news.  To check the cross linguistic feasability of  generFor CONTRAST, let OWi be the set of opinion  alisations made about the French data, we also  anwords that belongs to a segment Si. We have for  notated opinion categories for English. We have  OWa = and OWb 6= : group = groupb, p = pb and  annotated around 30 articles from movie reviews  s = sb + 1, as in I don't know a lot on Edith Piaf's and letters. For news reports, the annotation in En-life but I was enthraled by this movie.  glish was considerably helped by using texts from  Finally, an opinion text is represented by a graph  the MUC 6 corpus (186 articles), which were  anG = ( , ) such as:  notated independently with discourse structure by  = H T is the set of nodes where :  three annotators in the University of Texas's  DISH = {hoi/hoi is an opinion holder} and T =  COR project (NSF grant, IIS-0535154); the  anno{toi : value/toi is a topic and value is a F S}, such as : tation for our opinion expressions involved a col-value = [P olarity : p, Strength : s, Advice : a], where: lapsing of structures proposed in DISCOR.  The annotation methodology is described in  = H T H T where: H =  (Asher et al, 2008). For each corpus, annotators  {(hi, hj)/hi, hj H} means that two  topfirst begin to annotate elementary discourse  segics are related via an ELABORATION relation.  ments, define its shallow representation and finally,  This holds generally between a topic and a  connect the identified segments using the set of  subtopic, such as a movie and a scenario ; T =  rhetorical relations we have identified. A segment  {(ti, tj, type)/ti, tj T and type = support/contrast}  is annotated only if it explicitly contains an opin-means that two holders are related via a  CONion word that belong to our lexicon or if it bears a  TRAST (holders hi and hj have a contrasted  opinrhetorical relation to an opinion segment.  The average distribution of opinion expressions  News (%)  English  French  English  English  in our corpus across our categories for each  lanReporting  guage is shown in table 2. The annotation of movie  reviews was very easy. The opinion expressions  are mainly adjectives and nouns. We found an  average of 5 segments per review. Opinion words in  Letters to the Editor are adjectives and nouns but  also verbs. We found an average of 4 segments per  letter. Finally, opinions in news documents involve  principally reported speech. As we only annotated  segments that clearly expressed opinions or were  related via one of our rhetorical relations to a  segment expressing an opinion, our annotations  typically only covered a fraction of the whole  document. This corpus was the hardest to annotate and  s  generally contained lots of embedded structure  introduced by REPORTING type verbs.  Table 2: Average distribution of our categories.  To compute the inter-annotator agreements  (IAA) we did not take into account the opinion  and moods like the subjunctive, and to (2) provide  holder and the topic as well as the polarity and the  a deep semantic representation that associates for  strength because we chose to focus, at a first step,  each category of opinion a lambda term involving  only on agreements on opinion categorization,  segthe proferred content and a lambda term for the  ment idendification and rhetorical structure  detecpresuppositional content of the expression, if it has  tion. We computed the agreements only on the  one. In terms of automatization, we plan to exploit  French corpus. The French annotators performed  a syntactic parser to get the argument structure of  a two step annotation where an intermediate  analverbs and then a discourse segmenter like that  deysis of agreement and disagreement between the  veloped in the DISCOR project, followed by the  two annotators was carried out. This analysis  aldetection of discourse relations using cue words.  lowed each annotator to understand the reason of  some annotation choices. Using the Kappa  measure, the IAA on opinion categorization is 95% for  movie reviews, 86% for Letters to the Editors and  Asher N. and Benamara F. and Mathieu Y.Y. 2008. Catego-73% for news documents.  rizing Opinions in Discourse. ECAI08.  Annotators had good agreement concerning  Asher N. and Lascarides A. 2003. Logics of Conversation.  what the basic segments were (82%), which shows  Cambridge University Press.  that the discourse approach in sentiment analysis  Levin B. 1993. English Verb Classes and Alterna-tions: A is easier compared to the lexical task where an-Preliminary Investigation. University of Chicago Press notators have low agreements on the identification  Martin J.R and White P.R.R. 2005. Language of Evaluation: of opinion tokens. The principal sources of dis-Appraisal in English. Palgrave Macmillan.  agreement in the annotation process came from  Mathieu Y. Y. 2004. A Computational Semantic Lexicon annotators putting opinion expressions in different  of French Verbs of Emotion. In Shanahan, G., Qu, Y., categories (mainly between PRAISE/BLAME group  Wiebe, J. (eds.): Computing Attitude and Affect in Text.  and APPRECIATION group, such as shame) and the  choice of rhetorical relations. Nevertheless, by  usRead J., Hope D. and Carroll J. 2007. Annotating Expres-ing explicit discourse connectors, we were able  sions of Appraisal in English. The Linguistic Annotation Workshop, ACL 2007.  to get relatively high agreement on the choice of  rhetorical relations. We also remained quite  unWiebe J., Wilson T. and Cardie C. 2005. Annotating Expressions of Opinions and Emotions in Language. Language sure how to distinguish between the reporting of  Resources and Evaluation 1(2).  neutral opinions and the reporting of facts. The  Wierzbicka A. 1987. Speech Act Verbs. Sydney: Academic main extension of this work are to (1) deepen our  Press.  opinion typology, specifically to include modals 