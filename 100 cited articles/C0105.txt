 SENTIWORDNET 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining  In this work we present SENTIWORDNET 3.0, a lexical resource explicitly devised for supporting sentiment classification and opinion mining applications. SENTIWORDNET 3.0 is an improved version of SENTIWORDNET 1.0, a lexical resource publicly available for research purposes, now currently licensed to more than 300 research groups and used in a variety of research projects worldwide. Both SENTIWORDNET 1.0 and 3.0 are the result of automatically annotating all WORDNET synsets according to their degrees of positivity, negativity, and neutrality. SENTIWORDNET 1.0 and 3.0 differ (a) in the versions of WORDNET which they annotate (WORDNET  2.0 and 3.0, respectively), (b) in the algorithm used for automatically annotating WORDNET, which now includes (additionally to the previous semi-supervised learning step) a random-walk step for refining the scores. We here discuss SENTIWORDNET 3.0, especially focussing on the improvements concerning aspect (b) that it embodies with respect to version 1.0. We also report the results of evaluating SENTIWORDNET 3.0 against a fragment of WORDNET 3.0 manually annotated for positivity, negativity, and neutrality; these results indicate accuracy improvements of about 20% with respect to SENTIWORDNET 1.0.  Introduction  charts the history of SENTIWORDNET, from its very  earIn this work we present S  liest release to the current version, thus providing context ENTIWORDNET 3.0, an enhanced  lexical resource explicitly devised for supporting sentiment for the following sections. Section 3. examines in detail the classification and opinion mining applications (Pang and algorithm that we have used for generating SENTIWORD-Lee, 2008). S  NET 3.0, while Section 4. discusses accuracy issues.  ENTIWORDNET 3.0 is an improved version  of S  SENTIWORDNET 3.0 is freely available for non-profit  lexical resource publicly available for research purposes, research  from  now currently licensed to more than 300 research groups isti.cnr.it/  and used in a variety of research projects worldwide.  A brief history of SENTIWORDNET  SENTIWORDNET is the result of the automatic  annotation of all the synsets of WORDNET according to the no-Four different versions of SENTIWORDNET have been  distions of positivity , negativity , and neutrality . Each cussed in publications:  synset s is associated to three numerical scores P os(s), N eg(s), and Obj(s) which indicate how positive, nega-1. SENTIWORDNET 1.0, presented in (Esuli and  Sebastive, and objective (i.e., neutral) the terms contained in tiani, 2006) and publicly made available for research  the synset are. Different senses of the same term may thus purposes;  have different opinion-related properties. For example, in 2. SENTIWORDNET 1.1, only discussed in a technical  SENTIWORDNET 1.0 the synset [estimable(J,3)]1,  report (Esuli and Sebastiani, 2007b) that never reached corresponding to the sense may be computed or esti-the publication stage;  mated of the adjective estimable, has an Obj score  of 1.0 (and P os and N eg scores of 0.0), while the synset 3. SENTIWORDNET 2.0, only discussed in the second  [estimable(J,1)] corresponding to the sense  deauthor's PhD thesis (Esuli, 2008);  serving of respect or high regard has a P os score of 0.75, 4. SENTIWORDNET 3.0, which is being presented here  a N eg score of 0.0, and an Obj score of 0.25.  for the first time.  Each of the three scores ranges in the interval [0.0, 1.0], and their sum is 1.0 for each synset. This means that a Since versions 1.1 and 2.0 have not been discussed in  synset may have nonzero scores for all the three categories, widely known formal publications, we here focus on dis-which would indicate that the corresponding terms have, in cussing the differences between versions 1.0 and 3.0. The the sense indicated by the synset, each of the three opinion-main differences are the following:  related properties to a certain degree.  This paper is organized as follows. Section 2. briefly 1. Version 1.0 (similarly to 1.1 and 2.0) consists of an annotation of the older WORDNET 2.0, while version  1We here adopt the standard convention according to which 3.0 is an annotation of the newer WORDNET 3.0.  a term enclosed in square brackets denotes a synset; thus  [poor(J,7)] refers not just to the term poor but to the synset 2. For SENTIWORDNET 1.0 (and 1.1), automatic anno-consisting of adjectives {inadequate(J,2), poor(J,7),  tation was carried out via a weak-supervision,  semishort(J,4)}.  supervised learning algorithm. Conversely, for  SENTIWORDNET (2.0 and) 3.0 the results of this semi-2. In Step (2), the two sets of synsets generated in the pre-supervised learning algorithm are only an  intermedivious step are used, along with another set of synsets ate step of the annotation process, since they are fed to assumed to have the Obj property, as training sets for an iterative random-walk process that is run to conver-training a ternary classifier (i.e. one that needs to clas-gence. SENTIWORDNET (2.0 and) 3.0 is the output of  sify a synset as P os, N eg, or Obj). The glosses of  the random-walk process after convergence has been  the synsets are used by the training module instead of reached.  the synsets themselves, which means that the resulting classifier is indeed a gloss (rather than a synset) clas-3. Version 1.0 (and 1.1) uses the glosses of  WORDsifier. SENTIWORDNET 1.0 uses a bag of words  NET synsets as semantic representations of the synsets model, according to which the gloss is represented  themselves when a semi-supervised text classification  by the (frequency-weighted) set of words occurring in  process is invoked that classifies the (glosses of the) it. In SENTIWORDNET 3.0 we instead leverage on  synsets into categories P os, N eg and Obj. In  verthe manually disambiguated glosses available from the  sion 2.0 this is the first step of the process; in the sec-Princeton WordNet Gloss Corpus, according to which  ond step the random-walk process mentioned above  a gloss is actually a sequence of WORDNET synsets.  uses not the raw glosses, but their automatically sense-Our gloss classifiers are thus based on what might be  disambiguated versions from EXTENDEDWORDNET  called a bag of synsets model.  both the semi-supervised learning process (first step) 3. In Step (3) all WORDNET synsets (including those  and the random-walk process (second step) use instead  added to the seed sets in Step (2)) are classified as be-the manually disambiguated glosses from the  Princelonging to either P os, N eg, or Obj via the classifier ton WordNet Gloss Corpus2, which we assume to be  more accurate than the ones from  EXTENDEDWORD4. Step (2) can be performed using different values of 3.  the radius parameter, and different supervised  learning technologies. For reasons explained in detail in  We here summarize in more detail the automatic  anno(Esuli and Sebastiani, 2006), annotation turns out to be tation process according to which SENTIWORDNET 3.0  more accurate if, rather that a single ternary classifier, is generated.  This process consists of two steps, (1) a  a committee of ternary classifiers is generated, each of weak-supervision, semi-supervised learning step, and (2) whose members results from a different combination  of choices for these two parameters (radius and  learnThe semi-supervised learning step  ing technology). We have set up our classifier  comThe semi-supervised learning step is identical to the pro-mittee as consisting of 8 members, resulting from four cess used for generating S  different choices of radius (k {0, 2, 4, 6}) and two ENTIWORDNET 1.0; (Esuli and  Sebastiani, 2006) can then be consulted for more details on different choices of learning technology (Rocchio and  this process. The step consists in turn of four substeps: (1) SVMs). In Step (4) the final P os (resp., N eg, Obj)  seed set expansion, (2) classifier training, (3) synset classi-value of a given synset is generated as its average P os fication, and (4) classifier combination.  (resp., N eg, Obj) value across the eight classifiers in the committee.  1. In Step (1), two small seed sets (one consisting of all the synsets containing 7 paradigmatically posi-3.2.  The random-walk step  tive terms, and the other consisting of all the synsets containing 7 paradigmatically negative terms (Tur-The random-walk step consists of viewing WORDNET 3.0  ney and Littman, 2003)) are automatically expanded  as a graph, and running an iterative, random-walk pro-by traversing a number of WORDNET binary relations  cess in which the P os(s) and N eg(s) (and, consequently, than can be taken to either preserve or invert the P os Obj(s)) values, starting from those determined in the pre-and N eg properties (i.e., connect synsets of a given  vious step, possibly change at each iteration. The random-polarity with other synsets either of the same polarity walk step terminates when the iterative process has  con e.g., the also-see relation or of the opposite po-verged.  larity e.g., the direct antonymy relation), and by The graph used by the random-walk step is the one  adding the synsets thus reached to the same seed set  implicitly determined on WORDNET by the  definiens(for polarity-preserving relations) or to the other seed definiendum binary relationship; in other words, we assume set (for polarity-inverting ones). This expansion can  the existence of a directed link from synset s1 to synset s2 if be performed with a certain radius ; i.e., using radius and only if s1 (the definiens) occurs in the gloss of synset s2  k means adding to the seed sets all the synsets that  (the definiendum). The basic intuition here is that, if most of are within distance k from the members of the origi-the terms that are being used to define a given term are pos-nal seed sets in the graph collectively resulting from itive (resp., negative), then there is a high probability that the binary relationships considered.  the term being defined is positive (resp., negative) too. In other words, positivity and negativity are seen as flowing 2  through the graph , from the terms used in the definitions http://wordnet.princeton.edu/glosstag.  to the terms being defined.  However, it should be observed that, in regular  annotated according to their degrees of positivity, negativ-WORDNET, the definiendum is a synset while the definiens ity, and neutrality.  is a non-disambiguated term, since glosses are sequences of Micro-WN(Op) consists of 1,105 synsets manually an-non-disambiguated terms. In order to carry out the random-notated by a group of five human annotators (hereafter walk step, we need the glosses to be disambiguated against called J1, . . . , J5); each synset s is assigned three scores WORDNET itself, i.e., we need them to be sequences of  P os(s), N eg(s), and Obj(s), with the three scores sum-WORDNET synsets. While for carrying out the  randomSynsets 1-110 (here collectively called  walk step for SENTIWORDNET 2.0 we had used the  autoMicro-WN(Op)(1)) were tagged by all the annotators work-matically disambiguated glosses provided by  EXTENDEDing together, so as to develop a common understanding  of the semantics of the three categories; then, J1, J2 and 3.0 we use the manually disambiguated glosses available J3 independently tagged all of synsets 111 606 (Micro-from the above-mentioned Princeton WordNet Gloss  CorWN(Op)(2)), while J4 and J5 independently tagged all  of synsets 607 1105 (Micro-WN(Op)(3)). Our evaluation  The mathematics behind the random-walk step is fully  is performed on the union of synsets composing  Microdescribed in (Esuli and Sebastiani, 2007a), to which the in-WN(Op)(2) and Micro-WN(Op)(3). It is noteworthy that  terested reader is then referred for details. In that paper, Micro-WN(Op) as a whole, and each of its subsets, are rep-the random-walk model we use here is referred to as the resentative of the distribution of parts of speech in WORD-inverse flow model .  NET: this means that, e.g., if x% of WORDNET synsets  Two different random-walk processes are executed for  are nouns, also x% of Micro-WN(Op) synsets are nouns.  the positivity and negativity dimensions, respectively, of Moreover, this property also holds for each single part SENTIWORDNET, producing two different rankings of the  Micro-WN(Op)(x) of Micro-WN(Op).  WORDNET synsets. However, the actual numerical values  As for the evaluation of SENTIWORDNET 3.0, it should  returned by the random-walk process are unfit to be used be noted that Micro-WN(Op) is the annotation of a sub-as the final P os and N eg scores, since they are all too set of WORDNET 2.0, and cannot be directly used for  small (the synset top-ranked for positivity would obtain a evaluating SENTIWORDNET 3.0, which consists of an  P os score of 8.3 10 6); as a result, even the top-ranked annotation of WORDNET 3.0. Deciding which WORD-positive synsets would turn out to be overwhelmingly neu-NET 3.0 synset corresponds to a given synset in  Microtral and only feebly positive. Since, as we have observed, WN(Op) cannot be determined with certainty, and may  both the positivity and negativity scores resulting from the even be considered an ill-posed question. In fact, several semi-supervised learning step follow a power law distribu-of the synsets in Micro-WN(Op) do not exist any longer tion (i.e., very few synsets have a very high P os (resp., in WORDNET 3.0, at least in the same form. For example, N eg) score, while very many synsets are mostly neutral), the synset [good(A,22)] does no longer exist, while the we have thus fit these scores with a function of the form synset {gloomy(A,2), drab(A,3), dreary(A,1),  FP os(x) = a1xb1 (resp., FNeg(x) = a2xb2 ), thus deter-dingy(A,3),  dismal(A,1)} now  mining the a1 and b1 (resp., a2 and b2) values that best fit contains not only all of these words (although with differ-the actual distribution of values. The final SENTIWORDent sense numbers) but also blue(A,3), dark(A,9),  NET 3.0 P os(s) (resp., N eg) values are then determined disconsolate(A,2), and grim(A,6).  by applying the resulting function FP os(x) = a1xb1 (resp., As a result, we decided to develop an automatic map-FP os(x) = a2xb2 ) to the ranking by positivity (resp., by ping method that, given a synset s in WORDNET 2.0, iden-negativity) produced by the random-walk process.  tifies its analogue in WORDNET 3.0. We then took all of Obj(S) values are then assigned so as to make the three the WORDNET 2.0 synsets in Micro-WN(Op), identified  values sum up to one. In the case in which P os(s) +  their WORDNET 3.0 analogues, assigned them the same  N eg(s) > 1 we have normalized the two values to sum P os(s), N eg(s), and Obj(s) as in the original Micro-up to one and we have set Obj(s) = 03.  WN(Op) synset, and used the resulting 1,105 annotated  As an example, Table 1 reports the 10 top-ranked  posiWORDNET 3.0 synsets as the gold standard against which tive synsets and the 10 top-ranked negative synsets in SEN-to evaluate SENTIWORDNET 3.0.  Our synset mapping method is based on the  combination of three mapping strategies, which we apply in this order:  For evaluating the accuracy of SENTIWORDNET 3.0 we  follow the methodology discussed in (Esuli, 2008). This 1. WORDNET sense mappings: We first use the sense  consists in comparing a small, manually annotated subset of mappings between WORDNET 2.0 and 3.0 avail-WORDNET against the automatic annotations of the same  able at http://wordnetcode.princeton.  synsets as from SENTIWORDNET.  edu/3.0/WNsnsmap-3.0.tar.gz. These  mappings were derived automatically using a number of  In (Esuli, 2008), SENTIWORDNET 1.0, 1.1 and 2.0 were  heuristics, and are unfortunately limited to nouns and evaluated on Micro-WN(Op) (Cerini et al., 2007), a care-verbs only. Each mapping has a confidence value  assofully balanced set of 1,105 WORDNET synsets manually  ciated to it, ranging from 0 (lowest confidence) to 100  (highest confidence). The majority of mappings have  3This happened only for 16 synsets.  a 100 confidence score associated to them. As  recomTable 1: The 10 top-ranked positive synsets and the 10 top-ranked negative synsets in SENTIWORDNET 3.0.  Negative  abject#a#2  distressing#a#2  better off#a#1  sorry#a#2  divine#a#6 elysian#a#2 inspired#a#1  scrimy#a#1  good-humored#a#1  homeless#a#2   roofhumoured#a#1  hapless#a#1  pathetic#a#1  wretched#a#5  mended in the documentation associated to the  mapThe final result of this mapping process, that we call pings, we have used only the highest-valued mappings  Micro-WN(Op)-3.0, is publicly available at http://  (those with a 100 score), ignoring the others.  Heurissentiwordnet.isti.cnr.it/. It should be noted  tics used for the determination of mappings include the that the results of the automatic mapping process have not comparison of sense keys, similarity of synset terms,  been completely checked for correctness, since checking and relative tree location (comparison of hypernyms).  if there is a better map for Micro-WN(Op) synset s than By using these mappings we have mapped 269 Micro-the current map requires in theory to search among all the WN(Op) synsets to WORDNET 3.0.  WORDNET 3.0 synsets with the same POS. Therefore, the  results of evaluations obtained on Micro-WN(Op)-3.0 are 2. Synset term matching: If a Micro-WN(Op) synset  not directly comparable with those obtained on the original si (that has not already been mapped in the previous  Micro-WN(Op).  step) contains exactly the same terms of a WORDNET  3.0 synset sj, and such set of terms appears only in one 4.2.  synset in both WORDNET 2.0 and 3.0, we consider si  In order to evaluate the quality of SENTIWORDNET we test and sj to represent the same concept.  how well it ranks by positivity (resp., negativity) the synsets 3. Gloss similarity: For each Micro-WN(Op) synset si  in Micro-WN(Op)-3.0. As our gold standard we thus use a that has not been mapped by the previous two meth-ranking by positivity (resp., negativity) of  Micro-WN(Op)ods, we compute the similarity between its gloss and  3.0, obtained by sorting the Micro-WN(Op)-3.0 synsets ac-the glosses of all WORDNET 3.0 synsets, where a  cording to their P os(s) (resp., N eg(s)) values). Similarly, gloss gi is represented by the set of all character tri-we generate a ranking by positivity (resp., negativity) of grams contained in it. Similarity is computed via the  the same synsets from the P os(s) and N eg(s) values as-Dice coefficient4  signed by SENTIWORDNET 3.0, and compare them against  the gold standard above.  Dice(g1, g2) =  We compare rankings by using the p-normalized  Kendall distance (noted p see e.g., (Fagin et al., 2004)) In Equation 1 a higher Dice(g1, g2) value means a  between the gold standard rankings and the predicted rank-stronger similarity. Given a Micro-WN(Op) synset si,  ings. The p distance, a standard function for the evaluation its most similar WORDNET 3.0 gloss is determined,  of rankings that possibly admit ties, is defined as:  and the corresponding synset is chosen as the one  The Princeton research group had originally not used gloss similarity to produce the sense mappings used in Step 1 be-where nd is the number of discordant pairs, i.e., pairs of cause, as reported in the documentation distributed with the objects ordered one way in the gold standard and the other mappings, Glosses (...) are often significantly modified .  way in the tested ranking; nu is the number of pairs which We have found, by manually inspecting a sample of the re-are ordered (i.e., not tied) in the gold standard and are tied sults, that gloss similarity mapping was rather effective in in the tested ranking; p is a penalization to be attributed to our case.  each such pair, set to p = 1 (i.e., equal to the probability 2  that a ranking algorithm correctly orders the pair by ran-4See also http://en.wikipedia.org/wiki/Dice_  dom guessing); and Z is a normalization factor (equal to coefficient  the number of pairs that are ordered in the gold standard) 2203  Table 2: p values for the positivity and negativity rankings Table 3: p values for the positivity and negativity rankings derived from SENTIWORDNET 1.0 and 3.0, as measured  derived from (a) the results of the semi-supervised learning on Micro-WN(Op) and Micro-WN(Op)-3.0.  step of SENTIWORDNET 3.0, and (b) SENTIWORDNET  3.0, as measured on Micro-WN(Op)-3.0.  Negativity  Negativity  whose aim is to make the range of p coincide with the 5.  [0, 1] interval. Note that pairs tied in the gold standard are S. Cerini, V. Compagnoni, A. Demontis, M. Formentelli, not considered in the evaluation. The lower the p value the and G. Gandini. 2007. Micro-WNOp: A gold stan-better; for a prediction which perfectly coincides with the dard for the evaluation of automatically compiled lexi-gold standard, p equals 0; for a prediction which is exactly cal resources for opinion mining. In Andrea Sans , edi-the inverse of the gold standard, p is equal to 1.  tor, Language resources and linguistic theory: Typology, second language acquisition, English linguistics, pages 4.3.  Table 2 reports the p values for the positivity and negativ-WORDNET: A publicly available lexical resource for  ity rankings derived from SENTIWORDNET 1.0 and 3.0, as opinion mining. In Proceedings of the 5th Conference on measured on Micro-WN(Op) and Micro-WN(Op)-3.0, re-Language Resources and Evaluation (LREC'06), pages  spectively. The values for SENTIWORDNET 1.0 are  extracted from (Esuli and Sebastiani, 2007b). As already Andrea Esuli and Fabrizio Sebastiani. 2007a. Random-pointed out in Section 4.1., we warn the reader that the com-walk models of term semantics:  An application to  parison between the SENTIWORDNET 1.0 and 3.0 results is opinion-related properties. In Proceedings of the 3rd  only partially reliable, since Micro-WN(Op)-3.0 (on which Language Technology Conference (LTC'07), pages 221  the SENTIWORDNET 3.0 results are based) may contain  annotation errors introduced by the automatic mapping pro-Andrea Esuli and Fabrizio Sebastiani. 2007b.  SENTIcess.  WORDNET: A high-coverage lexical resource for  opinTaking into account the above warning, we can  obTechnical Report 2007-TR-02, Istituto  serve that SENTIWORDNET 3.0 is substantially more  accurate than SENTIWORDNET 1.0, with a 19.48% relative  improvement for the ranking by positivity and a 21.96%  Andrea Esuli. 2008. Automatic Generation of Lexical Re-improvement for the ranking by negativity.  sources for Opinion Mining: Models, Algorithms, and  We have also measured (see Table 3) the difference  Applications. Ph.D. thesis, Scuola di Dottorato in Ingeg-in accuracy between the rankings produced by  SENTIWORDNET 3.0-semi and SENTIWORDNET 3.0, where by  Ronald Fagin,  SENTIWORDNET 3.0-semi we refer to the outcome of  the semi-supervised learning step (described in Section gregating rankings with ties. In Proceedings of ACM In-3.1.) that led to the generation of SENTIWORDNET 3.0.  ternational Conference on Principles of Database  SysThe reason we have measured this difference is to check tems (PODS'04), pages 47 58, Paris, FR.  whether the random-walk step of Section 3.2. is indeed ben-Sanda M. Harabagiu, George A. Miller, and Dan I.  eficial. The relative improvement of SENTIWORDNET 3.0  A morphologically  with respect to SENTIWORDNET 3.0-semi is 17.11% for  and semantically enhanced resource. In Proceedings of  the ranking by positivity, and 19.23% for the ranking by the ACL Workshop on Standardizing Lexical Resources  negativity; this unequivocally shows that the random-walk (SIGLEX'99), pages 1 8, College Park, US.  process is indeed beneficial.  Bo Pang and Lillian Lee. 2008. Opinion mining and  senIt would certainly have been interesting to also  meatiment analysis. Foundations and Trends in Information sure the impact that the manually disambiguated glosses Retrieval, 2(1/2):1 135.  available from the Princeton WordNet Gloss Corpus have Peter D. Turney and Michael L. Littman. 2003. Measur-had in generating SENTIWORDNET, by comparing the  ing praise and criticism: Inference of semantic orienta-performance obtained by using them (either in the semition from association. ACM Transactions on Information supervised learning step, or in the random-walk step, or Systems, 21(4):315 346.  in both) with the performance obtained by using the automatically disambiguated ones from  EXTENDEDWORDNET. Unfortunately, this is not possible, since the former glosses are available for WORDNET-3.0 only, while EXTENDEDWORDNET is available for WORDNET-2.0 only. 