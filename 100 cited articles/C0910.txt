 United we stand: improving sentiment analysis  by joining machine learning and rule based methods  National Centre for Scientific Research Demokritos , Inst. of Informatics and Telecommunications, Athens Greece  University of the Aegean, Dpt. of Information and Communication System Enginneering Samos, Greece  Institute of Computational Linguistics, University of Zurich,  In the past, we have succesfully used machine learning approaches for sentiment analysis. In the course of those experiments, we observed that our machine learning method, although able to cope well with figurative language could not always reach a certain decision about the polarity orientation of sentences, yielding erroneous evaluations. We support the conjecture that these cases bearing mild figurativeness could be better handled by a rule-based system. These two systems, acting complementarily, could bridge the gap between machine learning and rule-based approaches. Experimental results using the corpus of the Affective Text Task of SemEval '07, provide evidence in favor of this direction.  Introduction  Such cases are ideal candidates for a rule-based system  Exploiting figurative language using the machine learning  like PolArt (Klenner et al., 2009) that has been designed system proposed in (Rentoumi et al., 2009) has opened an to handle literal or semi-literal language through sentiment  interesting new path to sentiment analysis. This specific  composition. PolArt combines the polarities of syntactic  machine learning method (FigML) has been trained on a  constituents like beauty and polarity shifters like no to  corpus manually annotated with figurative language1. This compose the polarity of larger textual units.  method was more inclined towards the strong figurative use  We propose a method which aims at filling the gap for  poof language, like Record-shattering day on Wall St . It  larity detection in corpora where strong and mild figurative  has been observed, that the cases where the classification  language are prominent. The proposed method, contrary to  decision is taken within a small margin, are those bearing  the one presented in (Rentoumi et al., 2009), which dealt mild figurativeness, often yielding erroneous evaluations.  with strong figurative language phenomena, deals more  efSuch a marginal case is:  fectively with all expressions of figurative language, strong  and mild. In this paper we introduce a novel method for  sentiment analysis of figurative language, which overcomes  (a) Ancient coin shows Cleopatra was no beauty.  the fallacies of the machine learning method attributed to  the existence of mild figurative language, by delegating  those to a rule-based approach (Klenner et al., 2009).  In example (a), beauty extends its primary sense, and it is  In particular this paper argues in favor of a collaborative  apused as an expanded sense2 denoting a beautiful woman .  proach to sentiment analysis consisting of two sub-methods  Despite the use of this metaphorical extension, the negative  acting complementarily: a machine learning method  (Renpolarity of this sentence can still be obtained without the  toumi et al., 2009) that handles the non-marginal cases need for word sense disambiguation. According to (Cruse,  which bear strong figurativeness and a compositional  rule2000), such cases of figurative language, are synchroni-based method (Klenner et al., 2009), for the marginal  cally as literal as their primary sense, as a result of  stanones. Results verify that integrating a machine learning  approach with a finer-grained linguistics-based one leads to a  superior, best-of-breed coalition system.  1Two subsets were extracted from the AffectiveText  corpus (SemEval 07') and annotated with metaphors and  exIn Section 2 we present related work and in Section 3 we  They are available at: http://www.iit.  describe our methodology. Our experimental setup and  results are presented in Section 4. In Section 5 we present  2expanded sense: a metaphorical extension/restriction of the  evaluation results. We conclude and list some ideas about  future work in Section 6.  Related Work  Methodology Description  So far there is not much work in sentiment classification  The proposed method involves four consecutive steps:  in sentences using supervised machine learning. The main  (a)Word sense disambiguation(WSD): We chose an  algoreason recorded, is lack of sufficient labelled data for  trainrithm3 that assigns to every word in a sentence the Word-ing. A recent work that describes sentiment classification of  Net sense that is mostly related to the WordNet senses of its  sentences, using statistical machine learning (SVM) is  (Ganeighbouring words, revealing the meaning of that word.  mon and Aue, 2005). In (Andreevskaia and Bergler, 2008)  This WSD algorithm takes as input each sentence of our  the authors created a domain adaptive system for sentence  corpus and a relatedness measure(Pedersen et al., 2005).  The algorithm supports several WordNet based similarity  cation of newspapers' headlines exploiting a variety of  mameasures, and among these, Gloss Vector (GV) (Pedersen  chine learning techniques has been the goal of the  Affecet al., 2005) performs best for non-literal verbs and nouns tive Text Task of SemEval 07'(Strapparava and Mihalcea,  (Rentoumi et al., 2008). GV exploits what is called  sec2007). A similar task, concerning the sentiment classifica-ond order co-occurance, which claims that two concepts  tion of headlines is addressed in (Rentoumi et al., 2009).  (senses) are semantically related when they both occur with  Moreover in (Rentoumi et al., 2009), structured models the same third concept. GV predicts the similarity for two  such as Hidden Markov Models (HMMs) are exploited in  WordNet senses by finding the cosine similarity of their  resentiment classification of headlines. In our collaborative  spective Gloss Vectors. Integrating GV in the WSD step is  approach presented here, the supervised machine learning  method adopted (HMMs) in order to identify polarity in  (b)Sense level polarity assignment(SLPA): We adopted a  sentences bearing strong figurative language is identical to  machine learning approach which exploits graphs based on  the one presented in (Rentoumi et al., 2009). The advan-character n-grams (Giannakopoulos et al., 2008). A (char-tage of HMMs against other machine learning approaches  acter) n-gram Sn contained in a text T can be any substring  employed till now in sentiment analysis is that the  majorof length n of the original text. The n-gram graph is a graph  ity of them is based on flat bag-of-features representations  G = {V G, EG, L, W }, where V G is the set of vertices, EG  of sentences, without capturing the structural nature of a  is the set of edges, L is a function assigning a label to each  sub-sentential interactions. On the contrary, HMMs being  vertex and edge, and W is a function assigning a weight  sequential models encode this structural information, since  to every edge. n-grams label the vertices vG V G of the  sentence elements are represented as sequential features.  graph. The (directed) edges are labeled by the  concatenaOn the other hand, rule-based approaches for sentiment  detion of the labels of the vertices they connect in the direction  tection in sentences are not used extensively. The  ruleof the connection. The edges eG EG connecting the  nbased approach employed in this paper originally presented  grams indicate proximity of these n-grams in the text within  in (Klenner et al., 2009) is based on the principle of compo-a given window Dwin of the original text (Giannakopoulos  et al., 2008). The edges are weighted by measuring the  2007) as well adopt a compositional approach to sentiment number of co-occurrences of the vertices' n-grams within  the window Dwin.  We have also noticed that the bibliography for sentiment  To compute models of polarity using n-gram graphs, we  analysis is rather poor in methods which combine the  have used two sets of positive and negative examples of  benefits of machine learning together with rule-based  apwords and definitions provided by the General Inquirer 4  proaches. In (Choi and Cardie, 2008) a combined method (GI). To represent a text set using n-gram graphs, we have  is presented performing expression level polarity  classification which integrates inference-rules inspired by  compographs of the same rank. Specifically, given two graphs,  sitional semantics into learning. In this way the  subsenG1 and G2, each representing a subset of the set of texts, we  tential interactions are captured by compositional rules and  create a single graph that represents the merging of the two  learned by the system. The interaction of subsentential  constituents, such as word level polarity and valence shifters  such that Eu = EG EG  2 , where EG  are the edge  yield the overall polarity of the whole phrase. Valence  sets of G1, G2 correspondingly. The model construction  shifters originally studied in (Polanyi and Zaenen, 2004)  process for each class (e.g. of the positive/negative polarity  are contextual elements in discourse that could modify the  class) comprises the initialization of a graph with the first  valence of opinionated words, thus affecting the overall  posense of a class, and the subsequent update of this initial  larity of a sentence. In our collaborative approach a  manugraph with the graphs of the other senses in the class using  ally annotated list of valence shifters is compiled and  intethe union operator. As we need the model of a class to hold  grated in our system for sentence level polarity  classificathe average weights of all the individual graphs  contributtion.  ing to this model, functioning as a representative graph for  Our method for sentiment analysis combines in a  complethe class documents, the i-th graph that updates the class  mentary way two approaches, a machine learning together  graph (model) uses a learning factor of l = i 1 , i > 1.  with a compositional rule-based one, and aims at revealing  The polarity class of each test sense, represented as n-gram  graph exploiting its Synset and Gloss Example derived in the following consecutive steps:  from WordNet, is determined by computing its similarity  with the models of the classes: the class whose model is  1. no[DT :shifter] beauty[NN:positive] NEG1  the most similar to the test sense n-gram graph, is the class  2. was[V BD:] NEG [:negative]  of the document. The Graph Similarity exploited here in  3. Ancient[JJ:] coin[NN:] shows[V V D:] Cleopatra[NP :] NEG [:negative]  order to compare two graphs sets  G1, G2 (one representing  a sense and the other the model of a polarity class) is Value  Similarity (VS) which indicates for every n-gram rank  (GiFirst, a determiner that operates as a polarity shifter is  comannakopoulos et al., 2008), how many of the edges con-bined with a positive noun into NEG1, a negative chunk.  tained in graph Gi of rank n are also contained in graph Gj  Then, a verb is combined with NEG1 to produce NEG2.  also of rank n, considering also the weights of the  matchFinally, the sentence's polarity is determined to be negative  ing edges. VS is a measure converging to 1 for graphs that  driven by NEG2's negative polarity.  share their edges and have identical edge weights. Then  The performance of FigML is added up with that of PolArt,  the overall similarity VSOof the sets G1, G2 is computed  and gives the total performance of the collaborative system.  as the weighted sum of the VS over all ranks. More details  for Graph Similarity are given in (Rentoumi et al., 2009).  c)HMMs training: HMMs serve two purposes, computing  the threshold which divides the sentences in  marginal/nonWe ran our experiments on three datasets:  marginal and judging the polarity (positive/negative) of  The AffectiveText corpus5 from SemEval '07 com-In the training step, two different HMMs are trained (one  prising 1000 polarity annotated headlines (Strapparava  for the positive and one for the negative class) and the  threshold for the distinction of marginal and non-marginal  sentences is also computed. Only positive instances are  Figurative Language Datasets:  used for the training procedure of HMMpos (the model  manually  annotated  headlines  containing  for positive sentences) and only negative instances are used  metaphors extracted from the AffectiveText  corfor training HMMneg (the model for negative sentences).  After the extraction of the two models we use both  positive and negative sentences to calculate the threshold for  190 manually annotated headlines containing  exmarginal/non-marginal cases. Each instance is tested with  panded senses extracted from the AffectiveText  corboth trained models (positive and negative) and log  probabilities are computed denoting the probability of the  instance to belong to each model. For each polarity class  We ran 4 variations of Polart, modifying the polarity  lexiwe compute the absolute difference of the log probabilities  and sort these differences in ascending order. The threshold  SL: This is the subjectivity lexicon(Wilson et al.,  for distinguishing marginal from non-marginal sentences is  2005). SL contains among others parts from the GI computed using the first Quartile (Q1) which separates the  lower 25% of the sample population from the remaining  data. Marginal cases are the ones below that threshold.  SL+: This is the subjectivity lexicon(Wilson et al.,  HMMs testing: Each instance in the testing data set is  2005) with 54 added valence operators.  tested against both models (HMMpos and HMMneg) and  the calculated log probabilities are used in order to decide  The FigML system produces  automatiif it is a marginal or a non-marginal case, according to the  cally 3 sense-level polarity lexica (AutSPs), one  aforementioned threshold. If we decide that the instance  for each dataset.  For the non-literal datasets  is a non-marginal case the greater log probability (between  (metaphors/expanded senses) these lexica target  HMMpos and HMMneg) provides us with the decision in  metaphors and expanded senses accordingly. For the  which class (positive or negative) the specific instance  beAffectiveText dataset all word senses are targeted. 3  Merged lexica are produced by merging the SL+  lexiFor our experiments we use data formed according to the  con with the AutSPs.  format presented in (Rentoumi et al., 2009) and we  per MergedManual: We use 2 handcrafted sense-level  poform ten fold cross validation approach for the evaluation  larity lexica (ManSPs)7. These lexica target metaphors step. For each fold 90% of the data are used for the training  and expanded senses accordingly. 2 MergedManual  procedure and 10% for the testing step.  lexica are produced by merging SL+ with the ManSPs.  (d) Sentence-level polarity detection: The polarity of each  sentence is determined by HMMs (Rentoumi et al., 2009)  for non-marginal cases and by PolArt(Klenner et al., 2009)  affectivetext/  for marginal ones.  and obtains word-level polarities from a subjectivity  lexi7ManSPs were produced by manually mapping expanded and  Example (a) from Introduction would be treated by PolArt  metaphorical senses from Wordnet to GI (Rentoumi et al., 2009)  PolartMerged  In the following sections experimental results are presented  for both alternatives of the collaborative system, compared  to the pure machine learning (FigML) method and the pure  rule-based approach (Polart) for the Figurative Language  Datasets, namely the datasets which are annotated with  Performance scores for marginal cases on  metaphorical expressions and expanded senses (see 4.1.).  Metaphors and Expanded senses data sets  Evaluation of the Systems on marginal cases  Focusing only on marginal cases we compared FigML  collaborative system which combines the virtues of a  rulewith three variants of the rule-based method, Polart  (usbased and a machine learning method, can more properly  ing the SL lexicon), PolartSL+ (using the SL+ lexicon)  handle a corpus which bristles with strong and mild  figuand PolartMerged (using the Merged lexica) (see 4.1.).  rative language, than two separate language specific  sysTable 1 presents performances in terms of recall (rec), tems functioning alone. Another observation that can be  precision (prec), f-score, for each polarity class  (negamade from Table 3 and Table 2, is that FigML's perfor-tives/positives), for the four systems, across both figurative  mance drops for the full data sets (Table 3) while pure language datasets (metaphors (Met) and expanded senses  Polart's performance remains the same both for the  non(Exp)). All rule-based alternatives (Polart, PolartSL+,  Pomarginal data set (Table 2) and the full data sets (Table 3).  lartMerged) outperform the pure machine learning (FigML)  According to the aforementioned observation we can  ratiomethod. In particular PolartSL+ presents the best  perfornally attribute FigML's performance drop to the existence  mance across all rule-based alternatives, for both data sets.  of marginal cases, a fact that was also our primary intuition.  Paired t-tests report that PolartSL+'s superior performance  PolartMerged  is statistically significant within: 2% statistical error  threshold for both classes on the expanded senses dataset  (pvalue=0.02), while for metaphors' data set we cannot  support within 5% statistical error that PolartSL+ performs  better than FigML (p-value=0.54).  Evaluation of the Systems on non marginal  Table 2: Performance scores for non-marginal cases on  Metaphors and Expanded senses data sets  We have claimed that the non-marginal cases are the ones  that bear strong figurativeness, and can be better treated by  Paired t-tests report that CollabSL+'s superior performance  the machine learning method (FigML) which is originally  compared to FigML's is statistically significant within 2%  designed to treat such cases.  statistical error threshold for both polarity classes on the  Focusing on non-marginal cases we compared FigML, with  expanded senses data set (p-value = 0.019). On the other  three variants of the rule-based method, Polart (using the  hand, for the metaphors data set we cannot support within  SL lexicon), PolartSL+ (using the SL+ lexicon) and  Polart5% statistical error that CollabSL+ is better than FigML's  Merged (using the merged lexica). Table 2 presents  performances in terms of recall (rec), precision (prec), f-score,  CollabMerged  for each polarity class for the four systems across both  figurative language data sets (metaphors (Met) and expanded  senses (Exp) datasets). FigML outperforms all rule-based  (Polart) variants. Such an observation strengthens our  initial conjecture that FigML can effectively treat strong  figurative language.  Performance scores for full system runs on  Evaluation of the Systems on full data sets  Metaphors and Expanded senses data sets  For the complete datasets we compared the pure machine  learning method (FigML) and the pure rule-based method  Moreover, we cannot support within 5% statistical error  (Polart) with two variants of the collaborative system,  Colthat CollabSL+ is better than Polart for the metaphors  (plabSL+ (using the SL+ lexicon) and CollabMerged  (usvalue = 0.13) or the expanded senses data set (p-value =  ing the Merged lexica). Table 3 presents scores for each 0.10).  polarity class, across both figurative language datasets  (metaphors (Met) and expanded senses (Exp)).  For the majority of cases, both system variants outperform  In the following sections experimental results are presented  FigML8 and Polart. This fact leads to the conclusion that a for both alternatives of the collaborative system, compared  to the FigML method for the whole data set (1000  head8The FigML system presented here is almost identical to the  one presented in (Rentoumi et al., 2009). The activation of valence shifters, for the experiments on the AffectiveText dataset, results in an increased performance.  These experiments were further performed in order to have Paired t-tests report that CollabSL+'s superior performance  a comparison with FigML under a more extended data set,  compared to FigML's is statistically significant within: 1%  although none of these methods, neither the collaborative  statistical error threshold across both polarity classes on the  nor the FigML system were originally designed to handle  raw (unannotated corpus), which in its majority consists of  Evaluation of the Collaborative Method  For thoroughly evaluating the proposed collaborative  Evaluation of the systems on marginal cases  method we need to test our basic working assumption:  In this experiment, focusing on marginal cases we  comCompositional rules work sanely, so that Polart's fallacies  pared FigML with two variants of Polart, PolartSL+  (usare a result of erroneous polarities upon which the rules are  ing the SL+ lexicon) and PolartMerged using the Merged  applied.  It is important to mention that only one third  To test this, we will run the CollabMergedManual system,  of the Affective Text corpus contains figurative language  a variation of the CollabMerged system, this time  exploit(metaphors and expanded senses), whereas the remaining  ing the MergedManual lexica (see 4.1.). Doing so, we shall sentences (700) are considered literal. Therefore we have  assess the role of the ManSPs lexica within the  Mergedan additional reason to claim that most marginal cases  Manual lexica as a performance boost factor of the  Collabwould probably belong to literal language, which is why  MergedManual system.  a rule-based system such as Polart was selected to treat  Table 6 presents the performance of the CollabMergedMan-them. Table 4 presents scores for each polarity class (neg-ual against that of CollabSL+ and FigML for both  polarative/positive), for the three systems, concerning the whole  ity classes, tested upon the metaphors (Met) and expanded  AffectiveText corpus (All) (see section 4.1.). For this ex-senses (Exp) datasets. Note that FigML is also exploiting  tended data set of marginal cases the corpus is bigger,  the ManSPs lexica (see 4.1.).  thus marginal cases are relatively more our initial  intuition is getting verified, since both rule-based alternatives  CollabMergedManual  (PolartSL+, PolartMerged), perform better than pure  machine learning FigML. In particular paired t-tests report that  PolartSL+'s superior performance is statistical significant  within 1% statistical error threshold across both polarity  classes on the Affective Text data set (p-value = 0.011).  Table 6: Manual Evaluation of the Collaborative system  PolartMerged  Concerning the expanded senses dataset we did not observe  any performance boost of CollabMergedManual relative to  CollabSL+. For the CollabMergedManual system, 41% of  Table 4: Performance scores for marginal cases on the  Afthe word polarities were contributed by the ManSPs  lexfective Text corpus  ica, which includes polarized words introduced by the  human experts that either did not exist in the SL+ lexicon or  that existed in the SL+ but the experts assigned them with  Evaluation of the systems on full data set  a different polarity than the one in SL+. Another 30% was  For the complete data set of the Affective Text corpus we  found in both the ManSPs and the SL+ lexicon, which we  compared the performance of FigML (already presented  can call the agreement percentage. And finally, the  remainin (Rentoumi et al., 2009)) with two variants of the col-ing 29% was found only in the SL+ lexicon.  laborative system, CollabSL+ and CollabMerged. Table  Concerning the metaphors dataset we do observe, in both  5 presents scores for each polarity class concerning the positive and negative classes, a performance boost of Col-Affective Text corpus (All). Both alternatives of the  collabMergedManual relative to CollabSL+. For the  Collablaborative system (CollabSL+, CollabMerged) outperform  MergedManual system, 57% of the word polarities were  FigML. This fact can leads us to the conclusion that  percontributed by the ManSPs lexica, which includes  polarformance boost obtained with the use of the rule-based  apized words introduced by the human experts that either did  proach propagates to the overall performance of the  sysnot exist in the SL+ lexicon or that existed in the SL+ but  tem. Moreover our proposed approach performed  consisthe experts assigned them with a different polarity than the  tently well for an extended data set. It also performs well  one in SL+. Another 26% was found in both the ManSPs  even in mixed corpora, where style varies, and figurative  and the SL+ lexicon, which we can call the agreement  perlanguage coexists with literal language.  centage. And finally, the remaining 17% was found only in  the SL+ lexicon.  CollabMerged  Reflecting on these measurements we can see that a higher  degree of participation of the ManSPs lexica for the  metaphors dataset (57% > 41%) leads to an noticeable  performance boost. We can also see that for expanded senses  Table 5: Performance scores for full system runs on the  the agreement between manually disambiguated lexica and  Affective Text corpus  the Subjectivity lexicon is slightly higher(30% > 26%) than  for the metaphors dataset, which could point to a more lit-T. Pedersen, S. Banerjee, and S. Patwardhan. 2005.  Maxieral reading for the expanded senses dataset. Despite the  mizing semantic relatedness to perform word sense  dissmall size of our sample, we conjecture that our  collaboambiguation. Supercomputing institute research report  rative system has potential to further performance gains if  we integrate a fully manual sense-level polarity lexicon for  all words of both datasets. That remains to be tested with a  shifters. Computing Attitude and Affect in Text: Theory  broader manually prepared polarity lexicon.  and Applications, pages 1 9.  Conclusions and Future Work  2008. Sentiment analysis exploring metaphorical and  idiomatic senses: A word sense disambiguation approach.  We present a novel collaborative methodology for  sentiProccedings of International Workshop on  Computament analysis. It provides evidence for the  complementional Aspects of Affectual and Emotional Interaction  tarity of a rule-based compositional system (PolArt) and a  machine learning system (FigML). Sentiment composition  proved its applicability as PolArt treated successfully the  G. Vouros. 2009. Sentiment analysis of figurative  lanmarginal cases that would otherwise cause FigML's  perforguage using a word sense disambiguation approach.  mance to drop. Experiments showed that the performance  In Recent Advances in Natural Language Processing  boost for marginal cases gets propagated to the overall  performance of the collaborative system surpassing the  machine learning approach.  Task 14: Affective Text. In Proceedings of the  thInterThe initial observation that marginal cases bear mild  figunational Workshop on Semantic Evaluations (SemEval  rativeness and are therefore treated by PolArt effectively is  2007), Prague, Czeck Republic, pages 70 74.  supported by the experimental results.  Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.  We will test the collaborative method on a more extensive  corpus. Since correct sense-level polarity is vital for the  ment analysis. In HLT/EMNLP.  evolution of the collaborative system, we intend to  dynamically produce proper sense-level polarity lexica exploiting  additional machine learning approaches (e.g. SVMs).  A. Andreevskaia and S. Bergler. 2008. When specialists  and generalists work together: overcoming domain  dependence in sentiment tagging. In Proceedings of  ACLY. Choi and C Cardie. 2008. Learning with compositional  semantics as structural inference for subsentential  sentiment analysis. In Proceedings of the Conference on  Empirical Methods in Natural Language Processing, 793  801, 2008, Association for Computational Linguistics.  D.A. Cruse. 2000. Meaning in language. Oxford  University Press.  M. Gamon and A. Aue. 2005. Automatic identification  of sentiment vocabulary exploiting low association with  known sentiment terms. In Proceedings of the ACL  Workshop on Feature Engineering for Machine Learning  in Natural Language Processing, 57 64.  atopoulos. 2008. Summarization system evaluation  revisited: N-gram graphs. ACM Transactions on Speech  and Language Processing (TSLP), 5(3).  M. Klenner, S. Petrakis, and A. Fahrni. 2009. Robust  compositional polarity classification. In Recent Advances in  Natural Language Processing (RANLP), Borovets,  Bulcomposition. In Proceedings of the Recent Advances  in Natural Language Processing International  ConferIntroduction  Related Work  Methodology Description  Experimental Results I (Figurative Language Datasets) Evaluation of the Systems on marginal cases  Evaluation of the Systems on non marginal cases  Evaluation of the Systems on full data sets  Experimental results II (whole data set) Evaluation of the systems on marginal cases  Evaluation of the systems on full data set  Evaluation of the Collaborative Method  Conclusions and Future Work 