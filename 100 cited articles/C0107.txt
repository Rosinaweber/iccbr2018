 Kenneth Bloom and Navendu Garg and Shlomo Argamon Computer Science Department  Illinois Institute of Technology  Chicago, IL 60616  Popescu and Etzioni, 2005). Much of this work has utilized the fundamental concept of semantic orien-Sentiment analysis seeks to characterize  tation', (Turney, 2002); however, sentiment analysis opinionated or evaluative aspects of nat-still lacks a unified field theory'.  ural language text. We suggest here that  We propose in this paper that a fundamental task appraisal expression extraction should be  underlying many of these formulations is the extrac-viewed as a fundamental task in sentiment  tion and analysis of appraisal expressions, defined analysis. An appraisal expression is a tex-as those structured textual units which express an tual unit expressing an evaluative stance  evaluation of some object. An appraisal expression towards some target. The task is to find  has three main components: an attitude (which takes and characterize the evaluative attributes  an evaluative stance about an object), a target (the of such elements. This paper describes a  object of the stance), and a source (the person tak-system for effectively extracting and  dising the stance) which may be implied.  The idea of appraisal extraction is a generaliza-sions in English outputting a generic  reption of problem formulations developed in earlier resentation in terms of their evaluative  works. Mullen and Collier's (2004) notion of classi-function in the text. Data mining on  apfying appraisal terms using a multidimensional set praisal expressions gives meaningful and  of attributes is closely tied to the definition of an non-obvious insights.  appraisal expression, which is classified along several dimensions.  In previous work (Whitelaw et  al., 2005), we presented a related technique of find-1  Introduction  ing opinion phrases, using a multidimensional set Sentiment analysis, which seeks to analyze opin-of attributes and modeling the semantics of  modion in natural language text, has grown in interest ifiers in these phrases.  The use of multiple text  in recent years. Sentiment analysis includes a vari-classifiers by Wiebe and colleagues (Wilson et al., ety of different problems, including: sentiment clas-2005; Wiebe et al., 2004) for various kinds of senti-sification techniques to classify reviews as positive ment classification can also be viewed as a sentence-or negative, based on bag of words (Pang et al., level technique for analyzing appraisal expressions.  2002) or positive and negative words (Turney, 2002; Nigam and Hurst's (2004) work on detecting opin-Mullen and Collier, 2004); classifying sentences in ions about a certain topic presages our notion of a document as either subjective or objective (Riloff connecting attitudes to targets, while Popescu and and Wiebe, 2003; Pang and Lee, 2004); identifying Etzioni's (2005) opinion mining technique also fits or classifying appraisal targets (Nigam and Hurst, well into our framework.  2004); identifying the source of an opinion in a text In this paper we describe a system for extracting (Choi et al., 2005), whether the author is expressing adjectival appraisal expressions, based on a hand-the opinion, or whether he is attributing the opinion built lexicon, a combination of heuristic shallow to someone else; and developing interactive and vi-parsing and dependency parsing, and  expectationsual opinion mining methods (Gamon et al., 2005; maximization word sense disambiguation. Each ex-308  Proceedings of NAACL HLT 2007, pages 308 315,  2007 Association for Computational Linguistics  tracted appraisal expression is represented as a set of Attitude Type  Appreciation  feature values in terms of its evaluative function in Composition  the text. We have applied this system to two domains Balance: consistent, discordant, ...  of texts: product reviews, and movie reviews. Man-Complexity: elaborate, convoluted, ...  ual evaluation of the extraction shows our system to Reaction  work well, as well as giving some directions for im-Quality: beautiful, elegant, hideous, ...  provement. We also show how straightforward data Valuation: innovative, profound, inferior, ...  mining can give users very useful information about Affect: happy, joyful, furious, ...  public opinion.  Capacity: clever, competent, immature, ...  Tenacity: brave, hard-working, foolhardy, ...  Normality: famous, lucky, obscure, ...  We define an appraisal expression to be an elemen-Social Sanction  tary linguistic unit that conveys an attitude of some Propriety: generous, virtuous, corrupt, ...  kind towards some target. An appraisal expression Veracity: honest, sincere, sneaky, ...  is defined to comprise a source, an attitude, and a Figure 1: The Attitude Type taxonomy, with exam-target, each represented by various attributes. For ples of adjectives from the lexicon.  example, in I found the movie quite monotonous', the speaker (the Source) expresses a negative Attitude ( quite monotonous') towards the movie' (the ( good') or negative ( bad').  Target). Note that attitudes come in different types; Force describes the intensity of the appraisal. Force for example, monotonous' describes an inherent  is largely expressed via modifiers such as  quality of the Target, while loathed' would describe  very' (increased force), or slightly'  (dethe emotional reaction of the Source.  creased force), but may also be expressed  lexAttitude may be expressed through nouns, verbs,  ically, for example greatest' vs. great' vs.  adjectives and metaphors. Extracting all of this  information accurately for all of these types of ap-Polarity of an appraisal is marked if it is scoped in praisal expressions is a very difficult problem. We a polarity marker (such as not'), or unmarked  therefore restrict ourselves for now to adjectival ap-otherwise. Other attributes of appraisal are  afpraisal expressions that are each contained in a sin-fected by negation; e.g., not good' also has the gle sentence. Additionally, we focus here only on opposite orientation from good'.  extracting and analyzing the attitude and the target, Target type is a domain-dependent semantic type  but not the source. Even with these restrictions, we for the target. This attribute takes on values  obtain interesting results (Sec. 7).  ing important (and easily extractable)  distinctions between targets in the domain.  Our method is grounded in Appraisal Theory, developed by Martin and White (2005), which analyzes  the way opinion is expressed. Following Martin and Two domain-dependent target type taxonomies are  shown in Figure 2. In both, the primary distinction Attitude type is  type  of  is between a direct naming of a kind of Thing or a expressed one of affect, appreciation, or  deictic/pronominal reference (e.g., those or it ), judgment (Figure 1).  Affect refers to an  since the system does not currently rely on corefer-emotional state (e.g., happy', angry'), and  ence resolution. References are further divided into is the most explicitly subjective type of ap-references to the writer/reader ( interactants') and to praisal. The other two types express evaluation  other people or objects.  of external entities, differentiating between  The Thing subtrees for the two domains  difintrinsic appreciation of object properties (e.g., fer somewhat. In the movie domain, Things such  as this movie', Nicholas Cage', or  cinematogra heroic', idiotic').  phy', are classified into six main categories: movies Orientation is whether the attitude is positive  (the one being reviewed, or another one), people 309  Product Target Type  con contains head adjectives (which specify values Movie Thing  Product Thing  for the attributes attitude type, force, polarity, and Any Movie  Any Product  orientation), and appraisal modifiers (which specify This Movie  This Product  transformations to the four attributes). Some head Other Movie  Other Product  Product Part  adjectives are ambiguous, having multiple entries in Real Person. . .  the lexicon with different attribute values. In all Character  cases, different entries for a given word have dif-Movie Aspect. . .  ferent attitude types. If the head adjective is am-Company  Company  biguous, multiple groups are created, to be disam-Reference  biguated later. See our previous work (Whitelaw et Interactant  Reference  al., 2005) for a discussion of the technique.  Target groups are found by matching phrases in  Second Person  Other  Second Person  the lexicon with corresponding phrases in the text Third Person  Other  and assigning the target type listed in the lexicon.  Deictic  Third Person  Deictic  After finding attitude groups and candidate targets, Figure 2: Target taxonomies for movie and product the system links each attitude to a target.  reviews.  sentence is parsed to a dependency representation, and a ranked list of linkage specifications is used (whether characters, or real people involved in mak-to look for paths in the dependency tree connecting ing the film), aspects of the movie itself (its plot, some word in the source to some word in the target.  special effects, etc.), the companies involved in mak-Such linkage specifications are hand-constructed, ing it, or aspects of marketing the movie (such  and manually assigned priorities, so that when two as trailers). For target Things in product reviews, linkage specifications match, only the highest prior-we replace Movie Person' and Movie Aspect' by  ity specification is used. For example, the two  high Product Part' with two subcategories: Integral', for est priority linkage specifications are:  parts of the product itself (e.g., wheels or lenses), and Replaceable', for parts or supplies meant to nsubj  be periodically replaced (e.g., batteries or ink car-1. target x y attitude  tridges). The categories of Support', for references to aspects of customer support, and Experience' for amod  things associated with the experience of using the product (such as pictures' or resolution', were also The first specification selects the subject of a sen-added.  tence where the appraisal modifies a noun in the predicate, for example The Matrix' in The Matrix 3  is a good movie'. The second selects the noun mod-In our system, appraisal extraction runs in several in-ified by an adjective group, for example movie' in dependent stages. First, the appraisal extractor finds  The Matrix is a good movie'.  appraisal expressions by finding the chunks of text If no linkage is found connecting an attitude to a that express attitudes and targets. Then, it links each candidate target, the system goes through the link-attitude group found to a target in the text. Finally, it age specifications again, trying to find any word in uses a probabilistic model to determine which atti-the sentence connected to the appraisal group by a tude type should be assigned when attitude chunks known linkage. The selected word is assigned the were ambiguous.  generic category of movie thing or product thing (depending on the domain of the text). If no linkage is 3.1  found at all, the system assigns the default category The chunker is based on our earlier work (Whitelaw movie thing or product thing, assuming that there is et al., 2005), which finds attitude groups and tar-an appraised thing that couldn't be found using the gets using a hand-built lexicon (Sec. 4). This lexi-given linkage specifications.  After linkages are made, this information is used to P (A = a|exp) = P (A = a|T = t, L = l)  disambiguate multiple senses that may be present in a given appraisal expression.  Most cases are  To do this, we define a model M of this probability, unambiguous, but in some cases two, or occasion-and then estimate the maximum likelihood model  ally even three, senses are possible. We bootstrap using Expectation-Maximization.  from the unambiguous cases, using a probabilistic We model PM (A = a|T = t, L = l) by first  model, to resolve the ambiguities.  The attitude  applying Bayes' theorem:  type places some grammatical/semantic constraints on the clause. Two key constraints are the syntactic PM (A = a|T = t, L = l) =  relation with the target (which can differentiate af-P  fect from the other types of appraisal), and whether M (T = t, L = l|A = a)PM (A = a)  the target type has consciousness (which helps dif-PM (T = t, L = l)  ferentiate judgment and affect from appreciation).  Assuming conditional independence of target type To capture these constraints, we model the proba-and linkage, this becomes:  bility of a given attitude type being correct, given the target type and the linkage specification used to connect the attitude to the target, as follows.  The correct attitude type of an appraisal expres-PM (T = t)PM (L = l)  sion is modeled by a random variable A, the set of M 's parameters thus represent the conditional and all attitude types in the system is denoted by A, and marginal probabilities on this right-hand-side.  a specific attitude type is denoted by a. As described Given a set of (possibly ambiguous) appraisal ex-above, other attributes besides attitude type may pressions E identified by chunking and linkage de-also vary between word senses, but attitude type tection, we seek the maximum likelihood model  always changes between word senses, so when the  system assigns a probability to an attitude type, it Y  is assigning that probability to the whole word sense.  We denote the linkage type used in a given  appraisal expression by L, the set of all possible link-M will be our best estimate of P , given the pro-ages as L, and a specific linkage type by l. Note that cessed data in a given corpus.  The system  estithe first attempt with a linkage specification (to find mates M using an implementation of Expectation-a chunked target) is considered to be different from Maximization over the entire corpus. The highest-the second attempt with the same linkage specifica-probability attitude type (hence sense) according to tion (which attempts to find any word). Failure to M is then chosen for each appraisal expression.  find an applicable linkage rule is considered as yet another linkage' for the probability model. Since 4  The Lexicon  our system uses 29 different linkage specifications, As noted above, attitude groups were identified via a there are a total of 59 different possible linkages domain-independent lexicon of appraisal adjectives, types.  adverbs, and adverb modifiers. 1 For the movie  The target type of a given appraisal expression is domain, appraised things were identified based on denoted by T , the set of all target types by T , and a a manually constructed lexicon containing generic specific target type by t. We consider an expression movie words, as well as automatically constructed to have a given target type T = t only if that is its lexicons of proper names specific to each movie be-specific target type; if its target type is a descendant ing reviewed. For each product type considered, we of t, then its target type is not t in the model. E  manually constructed a lexicon containing generic denotes the set of all extracted appraisal expressions.  product words; we did not find it necessary to con-The term exp denotes a specific expression.  Our goal is to estimate, for each appraisal expres-1All of the lexicons used in the paper can be  sion exp in the corpus, the probability of its attitude found  type being a, given the expression's target type t appraisal lexicon 2007a.tar.gz  For adjectival attitudes, we used the lexicon reviews of ink-jet printers and 8479 reviews of digi-developed we developed in our previous work  tal cameras, covering 516 individual products.  (Whitelaw et al., 2005) on appraisal. We reviewed Each document in each corpus was preprocessed  the entire lexicon to determine its accuracy and into individual sentences, lower-cased, and tok-made numerous improvements.  enized. We used an implementation of Brill's (1992) Generic target lexicons were constructed by start-part-of-speech tagger to find adjectives and modi-ing with a small sample of the kind of reviews  fiers; for parsing, we used the Stanford dependency that the lexicon would apply to.  We examined  these manually to find generic words referring to appraised things to serve as seed terms for the lexicon 6  Evaluating Extraction  and used WordNet (Miller, 1995) to suggest  additional terms to add to the lexicon.  We performed two manual evaluations on the  sysSince movie reviews often refer to the specific  tem. The first was to evaluate the overall accuracy contents of the movie under review by proper names of the entire system. The second was to  specifi(of actors, the director, etc.), we also automatically cally evaluate the accuracy of the probabilistic dis-constructed a specific target lexicon for each movie ambiguator.  in the corpus, based on lists of actors, characters, writers, directors, and companies listed for the film 6.1  Evaluating Accuracy  at imdb.com. Each such specific lexicon was only We evaluated randomly selected appraisal expres-used for processing reviews of the movie it was gen-sions for extraction accuracy on a number of binary erated for, so the system had no specific knowledge measures. This manual evaluation was performed  of terms related to other movies during processing.  by the first author.We evaluated interrater reliability 5  between this rater and another author on 200 randomly selected appraisal expressions (100 on each We evaluated our appraisal extraction system on two corpus). The first rater rated an additional 120 ex-corpora. The first is the standard publicly available pressions (60 for each corpus), and combined these collection of movie reviews constructed by Pang and with his ratings for interrater reliability to compute Lee (2004). This standard testbed consists of 1000  system accuracy, for a total of 320 expressions (160  positive and 1000 negative reviews, taken from the for each corpus). The (binary) rating criteria were as IMDb movie review archives2. Reviews with neu-follows. Relating to the appraisal group:  tral' scores (such as three stars out of five) were re-APP Does the expression express appraisal at all?  moved by Pang and Lee, giving a data set with only ARM If so, does the appraisal group have all rele-clearly positive and negative reviews. The average vant modifiers?  document length in this corpus is 764 words, and HEM Does the appraisal group include extra mod-1107 different movies are reviewed.  (Results are shown negated, so that  The second corpus is a collection of user  prodhigher numbers are better.)  uct reviews taken from epinions.com supplied  Relating to the target:  in 2004 for research purposes by Amir Ashkenazi  HT If there is appraisal, is there an identifiable tar-of Shopping.Com. The base collection contains re-get (even if the system missed it)?  views for three types of products: baby strollers, dig-FT If there is appraisal, did the system identify ital cameras, and printers. Each review has a numer-some target? (Determined automatically.)  ical rating (1 5); based on this, we labeled positive RT If so, is it the correct one?  and negative reviews in the same way as Pang and Relating to the expression's attribute values (if it ex-Lee did for the movie reviews corpus. The  products corpus has 15162 documents, averaging 442  Att Is the attitude type assigned correct?  words long. This comprises 11769 positive  docuOri Is the orientation assigned correct?  ments, 1420 neutral documents, and 1973 negative Pol Is the polarity assigned correct?  documents. There are 905 reviews of strollers, 5778  Tar Is the target type assigned correct?  Pre Is the target type the most precise value in the http://www.cs.cornell.edu/people/pabo  taxonomy for this target?  appraisal' option, asking the rater to select which Table 1: System accuracy at evaluated tasks. 95%  one applied to the selected expression in context.  confidence one-proportion z-intervals are reported.  Products  Combined  Baseline disambiguator accuracy, if the computer APP  were to simply pick randomly from the choices  ARM  specified in the lexicon is 48% for both corpora.  Interrater agreement was 80% for movies and 73% for FT  products (taken over 100 expressions from each cor-RT  which the raters decided were appraisal, the disTar  ambiguator achieved 58% accuracy on appraisal exPre  pressions from the movies corpus and 56% accuracy on the products corpus. Further analysis of the reTable 2: Interrater reliability of manual evaluation.  sults of the disambiguator shows that most of the er-95% confidence intervals are reported.  rors occur when the target type is the generic cate-Measure  Products  Combined  gory thing which occurs when the target is not in the APP  ARM  target lexicon. Performance on words recognized as 95% 5%  having more specific target types is better: 68% for HT  movies, and 59% for products. This indicates that FT  RT  specific target type is an important indicator of at-Att  We (briefly) demonstrate the usefulness of appraisal expression extraction by using it for opinion mining.  Results are given in Table 1, and interrater relia-In opinion mining, we find large numbers of reviews bility is given in Table 2. In nearly all cases agree-and perform data mining to determine which aspects ment percentages are above 80%, indicating good  of a product people like or dislike, and in which inter-rater consensus. Regarding precision, we note ways. To do this, we search for association rules de-that most aspects of extraction seem to work quite scribing the appraisal features that can be found in well. The area of most concern in the system is  a single appraisal expression. We generally look for precision of target classification. This may be im-rules that contain attitude type, orientation, thing proved with further development of the target lex-type, and a product name, when these rules occur icons to classify more terms to specific leaves in more frequently than expected.  the target type hierarchy. The other area of con-The  to  and  cern is the APP test, which encountered difficulties Srikant's (1995) notion of generalized associa-when a word could be used as appraisal in some  tion rules. We treat each appraisal expression as contexts, but not in others, particularly when an ap-a transaction, with the attributes of attitude type, praisal word appeared as a nominal classifier.  orientation, polarity, force, and thing type, as well as the document attributes product name, product 6.2  type, and document classification (based on the  The second experiment evaluated the accuracy of  number of stars the reviewer gave the product).  EM in disambiguating the attitude type of appraisal We use CLOSET+ (Wang et al., 2003) to find all  expressions. We evaluated the same number of  exof the frequent closed itemsets in the data, with a pressions as used for the overall accuracy experi-support greater than or equal to 20 occurrences.  ment (100 used for interrater reliability and accu-Let hb, a1, a2, . . . ani or hb, Ai denote the contents racy, plus 60 used only for accuracy on each corpus), of an itemset, and c (hb, Ai) denote the support for each having two or more word senses, presenting all this itemset. For a given item b, (b) denotes its of the attitude types possible for each appraisal ex-immediate parent its value taxonomy, or root' for pression, as well as a none of the above' and a not flat sets.  Table 3: The most interesting specific rules for products.  Int.  Product Name  Polarity  class  this-product  Lexmark Color JetPrinter 1100  reaction  this-product  reaction  this-product  reaction  this-product  this-product  reaction  this-product  quality  this-product  reaction  reaction  quality  Table 4: The most interesting oppositional rules for products.  Int.  Product Name  Polarity  class  Lexmark Color JetPrinter 1100 (3)  reaction  this-product  Lexmark Color JetPrinter 1100  this-product  Lexmark Color JetPrinter 1100  reaction  this-product  Lexmark Color JetPrinter 1100  this-product  Lexmark Color JetPrinter 1100  this-product  For each item set, we collect rules hb, Ai and  all. An example sentence that created this rule says compute their interestingness relative to the itemset  Not only is it an excellent stroller, because of it's h (b), Ai. Interestingness is defined as follows:  [sic] size it even doubled for us as a portable crib.'  The specificity rules for the Agfa ePhoto Smile  Int =  Digital Camera (2) are an example of the kind of P (A| (b))  rule we expect to see when bad user experience con-Int is the relative probability of finding the child tributes to bad reviews. The text of the reviews that itemset in an appraisal expression, compared to find-gave these rules quite clearly convey that users were ing it in a parent itemset. Values greater than 1 in-not happy specifically with the photo quality.  dicate that the child itemset appears more frequently In the oppositional rules for the Lexmark Color  than we would expect.  JetPrinter 1100 (3), we see that users made positive We applied two simple filters to the output, to help comments about the product overall, while neverthe-find more meaningful results. Specificity requires less giving the product a negative review. Drilling that b be a product name, and that attitude type and down into the text, we can see some examples of re-thing type be sufficiently deep nodes in the hier-views like On the surface it looks like a good printer archy to describe something specific. (For exam-but it has many flaws that cause it to be frustrating.'  ple, product thing' gives no real information about what part of the product is being appraised.) Oppo-8  sition chooses rules with a different rating than the review as a whole, that is, document classification We have presented a new task, appraisal expres-is the opposite of appraisal orientation. The filter sion extraction, which, we suggest, is a fundamental also ensures that thing type is sufficiently specific, tasks for sentiment analysis. Shallow parsing based as with specificity, and requires that b be a product on a set of appraisal lexicons, together with sparse name.  use of syntactic dependencies, can be used to ef-We present the ten most interesting' rules from fectively address the subtask of extracting adjectival each filter, for the products corpus. Rules from the appraisal expressions. Indeed, straightforward data specificity filter are shown in Table 3 and rules from mining applied to appraisal expressions can yield in-the opposition filter are shown in Table 4. We con-sights into public opinion as expressed in patterns of sider the meaning of some of these rules.  evaluative language in a corpus of product reviews.  The first specificity rule (1) describes a typical ex-Immediate future work includes extending the  apample of users who like the product very well over-proach to include other types of appraisal expres-314  sions, such as where an attitude is expressed via a ference on Empirical Methods in Natural Language noun or a verb. In this regard, we will be examin-Processing, Barcelon, ES.  ing extension of existing methods for automatically Kamal Nigam and Matthew Hurst. 2004. Towards a ro-building lexicons of positive/negative words (Tur-bust metric of opinion. In Proceedings of the AAAI ney, 2002; Esuli and Sebastiani, 2005) to the more Spring Symposium on Exploring Attitude and Affect in complex task of estimating also attitude type and Text: Theories and Applications, Standford, US.  force. As well, a key problem is the fact that eval-Bo Pang and Lillian Lee. 2004. A sentimental education: uative language is often context-dependent, and so Sentiment analysis using subjectivity summarization proper interpretation must consider interactions be-based on minimum cuts. In Proc. 42nd ACL, pages  tween a given phrase and its larger textual context.  271 278, Barcelona, Spain, July.  2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of EMNLP.  Rakesh Agrawal and Ramakrishnan Srikant. 1995. Mining generalized association rules. In Umeshwar Dayal, Ana-Maria Popescu and Oren Etzioni.  Peter M. D. Gray, and Shojiro Nishio, editors, Proc.  tracting product features and opinions from reviews.  21st Int. Conf. Very Large Data Bases, VLDB, pages In Proceedings of HLT-EMNLP-05, the Human Lan-407 419. Morgan Kaufmann, 11 15 September.  guage Technology Conference/Conference on Empirical Methods in Natural Language Processing, Vancou-Eric Brill. 1992. A simple rule-based part of speech tag-ver, CA.  ger. In Proc. of ACL Conference on Applied Natural Language Processing.  Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceed-Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth ings of EMNLP.  Patwardhan. 2005. Identifying sources of opinions with conditional random fields and extraction patterns.  Peter D. Turney. 2002. Thumbs up or thumbs down? se-Human Language Technology Conference/Conference  mantic orientation applied to unsupervised classifica-on Empirical Methods in Natural Language Process-tion of reviews. In Proceedings 40th Annual Meeting ing (HLT-EMNLP 2005), October.  of the ACL (ACL'02), pages 417 424, Philadelphia, Pennsylvania.  Andrea Esuli and Fabrizio Sebastiani. 2005. Determin-ing the semantic orientation of terms through gloss Jianyong Wang, Jiawei Han, and Jian Pei.  analysis. In Proceedings of CIKM-05, the ACM SIGIR  CLOSET+: searching for the best strategies for min-Conference on Information and Knowledge  Manageing frequent closed itemsets.  Christos Faloutsos, Ted Senator, Hillol Kargupta, and Lise Getoor, editors, Proceedings of the ninth Michael Gamon, Anthony Aue, Simon Corston-Oliver, ACM SIGKDD International Conference on Knowl-and Eric Ringger.  Pulse: Mining customer  edge Discovery and Data Mining (KDD-03), pages  opinions from free text. In Proceedings of IDA-05, the 236 245, New York, August 24 27. ACM Press.  6th International Symposium on Intelligent Data Analysis, Lecture Notes in Computer Science, Madrid, ES.  Casey Whitelaw, Navendu Garg, and Shlomo Argamon.  2005. Using appraisal taxonomies for sentiment analysis. In Proceedings of CIKM-05, the ACM SIGIR  Dan Klein and Christopher D. Manning. 2003. Accu-Conference on Information and Knowledge  Managerate unlexicalized parsing. In Proceedings of the 41st ment, Bremen, DE.  Meeting of the Association for Computational Linguistics.  Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective J. R. Martin and P. R. R. White. 2005. The Language of language. Computational Linguistics, 30(3).  Evaluation: Appraisal in English. Palgrave, London.  Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.  George A. Miller. 1995. Wordnet: A lexical database for level sentiment analysis.  In Proceedings of  HuEnglish. Commun. ACM, 38(11):39 41.  man Language Technologies Conference/Conference  on Empirical Methods in Natural Language Process-Tony Mullen and Nigel Collier. 2004. Sentiment analysis ing (HLT/EMNLP 2005), Vancouver, CA.  using support vector machines with diverse information sources. In Proceedings of EMNLP-04, 9th Con-315 