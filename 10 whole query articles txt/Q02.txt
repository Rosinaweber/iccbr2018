 In Proceedings of HLT/EMNLP 2005  Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns  Yejin Choi and Claire Cardie  Ellen Riloff and Siddharth Patwardhan  Department of Computer Science  School of Computing  Cornell University  University of Utah  Ithaca, NY 14853  Salt Lake City, UT 84112  (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al. (2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength). We pursue  anand Riloff (2005)). Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments. We view this problem as  an information extraction task and adopt  (e.g., Cardie et al. (2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al. (2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a). While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.  Identifying opinion sources will  toSlog learns extraction patterns. Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')  one alone. The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.  indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.  To illustrate the nature of this problem, consider the 1  Introduction  examples below:  In recent years, there has been a great deal of  interest in methods for automatically identifying opinions, emotions, and sentiments in text. Much of  this research explores sentiment classification, a text 1In related work, we investigate methods to identify the categorization task in which the goal is to classify opinion expressions (e.g., Riloff and Wiebe (2003), Wiebe and a document as having positive or negative polar-Riloff (2005), Wilson et al. (2005)) and the nesting structure of sources (e.g., Breck and Cardie (2004)). The target of each ity (e.g., Das and Chen (2001), Pang et al. (2002),  opinion, i.e., what the opinion is directed towards, is currently Turney (2002), Dave et al. (2003), Pang and Lee  being annotated manually for our corpus.  S2: According to the report, the human rights ilarly, an IE system that extracts information about record in China is horrendous.  terrorism must distinguish between the person who  S3: International officers believe that the EU will is the perpetrator and the person who is the victim.  We hypothesized that IE techniques would be  wellS4: International officers said US officials want the suited for source identification because an opinion  statement can be viewed as a kind of speech event  with the source as the agent.  In S1, the phrase 'Taiwan-born voters' is the  diWe investigate two very different learning-based  rect (i.e., first-hand) source of the 'favoring' sen-methods from information extraction for the  probtiment. In S2, 'the report' is the direct source of  lem of opinion source identification: graphical  modthe opinion about China's human rights record. In  els and extraction pattern learning. In particular, we S3, 'International officers' are the direct source of consider Conditional Random Fields (Lafferty et al., an opinion regarding the EU. The same phrase in  2001) and a variation of AutoSlog (Riloff, 1996a).  S4, however, denotes an indirect (i.e., second-hand, CRFs have been used successfully for Named En-third-hand, etc.) source of an opinion whose direct  tity recognition (e.g., McCallum and Li (2003),  source is 'US officials'.  Sarawagi and Cohen (2004)), and AutoSlog has  perIn this paper, we view source identification as an formed well on information extraction tasks in sev-information extraction task and tackle the problem  eral domains (Riloff, 1996a).  While CRFs treat  using sequence tagging and pattern matching  techsource identification as a sequence tagging task, Au-niques simultaneously. Using syntactic, semantic,  toSlog views the problem as a pattern-matching task, and orthographic lexical features, dependency parse  acquiring symbolic patterns that rely on both the  features, and opinion recognition features, we train a syntax and lexical semantics of a sentence. We hy-linear-chain Conditional Random Field (CRF)  (Lafpothesized that a combination of the two techniques  ferty et al., 2001) to identify opinion sources. In ad-would perform better than either one alone.  dition, we employ features based on automatically  Section 3 describes the CRF approach to  identifylearned extraction patterns and perform feature  ining opinion sources and the features that the system duction on the CRF model.  uses. Section 4 then presents a new variation of  AuWe evaluate our hybrid approach using the NRRC  toSlog, AutoSlog-SE, which generates IE patterns to corpus (Wiebe et al., 2005), which is manually  extract sources. Section 5 describes the hybrid  sysannotated with direct and indirect opinion source  tem: we encode the IE patterns as additional features information.  Experimental results show that the  in the CRF model. Finally, Section 6 presents our  CRF model performs well, and that both the  extraction patterns and feature induction produce  performance gains. The resulting system identifies opinion 3  sources with 79.3% precision and 59.5% recall  using a head noun matching measure, and 81.2%  precision and 60.6% recall using an overlap measure.  We defined the problem of opinion source  identification as a sequence tagging task via CRFs as  folThe Big Picture  lows. Given a sequence of tokens, x = x1x2...xn,  we need to generate a sequence of tags, or labels,  The goal of information extraction (IE) systems is  to extract information about events, including the  1y2...yn. We define the set of possible label  values as 'S', 'T', '-', where 'S' is the first  toparticipants of the events. This task goes beyond  Named Entity recognition (e.g., Bikel et al. (1997)) ken (or Start) of a source, 'T' is a non-initial token because it requires the recognition of role  relation(i.e., a conTinuation) of a source, and '-' is a token ships. For example, an IE system that extracts in-that is not part of any source.2  A detailed description of CRFs can be found in  guish between the company that is doing the  acquir2This is equivalent to the IOB tagging scheme used in syn-ing and the company that is being acquired.  Simtactic chunkers (Ramshaw and Marcus, 1995).  For our sequence tagging  With these properties in mind, we define the  folproblem, we create a linear-chain CRF based on  lowing features for each token/word xi in an input  an undirected graph G = (V, E), where V is the  sentence. For pedagogical reasons, we will describe  some of the features as being multi-valued or  cateone for each of n tokens in an input sentence;  gorical features. In practice, however, all features and E = {(Yi 1, Yi)|1 < i n} is the set  are binarized for the CRF model.  of n 1 edges forming a linear chain. For each  Capitalization features We use two boolean fea-sentence x, we define a non-negative clique  potentures to represent the capitalization of a word:  k=1 kfk(yi 1, yi, x)) for each edge, and  i, x)) for each node, where fk(...)  Part-of-speech features Based on the lexical catis a binary feature indicator function, k is a weight egories produced by GATE (Cunningham et al.,  assigned for each feature function, and K and K 0  2002), each token xi is classified into one of a set are the number of features defined for edges and  of coarse part-of-speech tags: noun, verb, adverb,  nodes respectively. Following Lafferty et al. (2001), wh-word, determiner, punctuation, etc. We do the  the conditional probability of a sequence of labels y same for neighboring words in a [ 2, +2] window  given a sequence of tokens x is:  in order to assist noun phrase segmentation.  Opinion lexicon features For each token xi, we in-1  clude a binary feature that indicates whether or not k fk (yi  the word is in our opinion lexicon a set of words (1)  that indicate the presence of an opinion. We do the  same for neighboring words in a [ 1, +1] window.  Additionally, we include for x  i a feature that  indicates the opinion subclass associated with xi, if  available from the lexicon. (e.g., 'bless' is  classified as 'moderately subjective' according to the  where Zx is a normalization constant for each  lexicon, while 'accuse' and 'berate' are classified  Given the training data D, a set of  senmore specifically as 'judgments'.) The lexicon is  tences paired with their correct 'ST-' source  lainitially populated with approximately 500 opinion  bel sequences, the parameters of the model are  words 3 from (Wiebe et al., 2002), and then  augtrained to maximize the conditional log-likelihood  mented with opinion words identified in the training (  P (y|x). For inference, given a sentence x  data. The training data contains manually produced  in the test data, the tagging sequence y is given by phrase-level annotations for all expressions of opin-argmaxy0P (y0|x).  Features  lected all content words that occurred in the training set such that at least 50% of their occurrences were To develop features, we considered three properties  in opinion annotations.  of opinion sources. First, the sources of opinions are Dependency tree features For each token x  mostly noun phrases. Second, the source phrases  create features based on the parse tree produced by  should be semantic entities that can bear or express the Collins (1999) dependency parser. The purpose  opinions. Third, the source phrases should be  diof the features is to (1) encode structural  informarectly related to an opinion expression. When  contion, and (2) indicate whether x  sidering only the first and second criteria, this task i is involved in any  grammatical relations with an opinion word. Two  reduces to named entity recognition. Because of the  pre-processing steps are required before features can third condition, however, the task requires the recog-be constructed:  nition of opinion expressions and a more  sophisticated encoding of sentence structure to capture  re3Some words are drawn from Levin (1993); others are from lationships between source phrases and opinion ex-Framenet lemmas (Baker et al. 1998) associated with commu-pressions.  nication verbs.  1. Syntactic chunking. We traverse the  dependency tree using breadth-first search to identify  and group syntactically related nodes,  producing a flatter, more concise tree. Each  syntactic 'chunk' is also assigned a grammatical role  (e.g., subject, object, verb modifier, time,  location, of-pp, by-pp) based on its  conand the phrase 'according to X' are handled as  special cases in the chunking process.  The semantic hierarchy for opinion  2. Opinion word propagation.  Although the  opinion lexicon contains only content words  and no multi-word phrases, actual opinions  ofthat are learned by AutoSlog-SE, which is described  ten comprise an entire phrase, e.g., 'is really  in the next section.  willing' or 'in my opinion' . As a result, we mark as an opinion the entire chunk that con-4  Semantic Tagging via Extraction  tains an opinion word. This allows each token  in the chunk to act as an opinion word for  feaWe also learn patterns to extract opinion sources us-ture encoding.  ing a statistical adaptation of the AutoSlog IE learn-After syntactic chunking and opinion word  propaing algorithm. AutoSlog (Riloff, 1996a) is a  supergation, we create the following dependency tree  feavised extraction pattern learner that takes a  traintures for each token xi:  ing corpus of texts and their associated answer keys  as input. A set of heuristics looks at the context  the grammatical role of its chunk  surrounding each answer and proposes a  lexicothe grammatical role of xi 1's chunk  syntactic pattern to extract that answer from the text.  whether the parent chunk includes an opinion  The heuristics are not perfect, however, so the result-word  ing set of patterns needs to be manually reviewed by whether xi's chunk is in an argument position  with respect to the parent chunk  whether x  In order to build a fully automatic system that  i represents a constituent boundary  does not depend on manual review, we combined  Semantic class features We use 7 binary  feaAutoSlog's heuristics with statistics from the  antures to encode the semantic class of each word  notated training data to create a fully automatic  xi: authority, government, human, media,  supervised learner.  We will refer to this learner  organization or company, proper name,  as AutoSlog-SE (Statistically Enhanced variation  and other. The other class captures 13  semanof AutoSlog). AutoSlog-SE's learning process has  tic classes that cannot be sources, such as vehicle  three steps:  and time.  Semantic class information is derived from named  Step 1: AutoSlog's heuristics are applied to every entity and semantic class labels assigned to xi by the noun phrase (NP) in the training corpus. This  Sundance shallow parser (Riloff, 2004). Sundance  generates a set of extraction patterns that,  coluses named entity recognition rules to label noun  lectively, can extract every NP in the training  phrases as belonging to named entity classes, and  assigns semantic tags to individual words based on  a semantic dictionary. Table 1 shows the hierarchy  Step 2: The learned patterns are augmented with that Sundance uses for semantic classes associated  selectional restrictions that semantically  conwith opinion sources. Sundance is also used to  recstrain the types of noun phrases that are  legitiognize and instantiate the source extraction patterns mate extractions for opinion sources. We used  the semantic classes shown in Figure 1 as  seconsists of 535 documents that have been  manually annotated with opinion-related information  including direct and indirect sources. We used 135  Step 3: The patterns are applied to the training cor-documents as a tuning set for model development  pus and statistics are gathered about their  exand feature engineering, and used the remaining 400  tractions.  We count the number of  extracdocuments for evaluation, performing 10-fold cross  tions that match annotations in the corpus  (corvalidation. These texts are English language  verrect extractions) and the number of extractions  sions of articles that come from many countries and  that do not match annotations (incorrect  extraccover many topics.5  tions). These counts are then used to estimate  We evaluate performance using 3 measures:  overthe probability that the pattern will extract an  lap match (OL), head match (HM), and exact match  opinion source in new texts:  (EM). OL is a lenient measure that considers an  extraction to be correct if it overlaps with any of the an-correct sources  P (source | pattern ) =  notated words. HM is a more conservative measure  that considers an extraction to be correct if its head matches the head of the annotated source. We report  This learning process generates a set of extraction  these somewhat loose measures because the  annotapatterns coupled with probabilities. In the next sectors vary in where they place the exact boundaries  tion, we explain how these extraction patterns are  of a source. EM is the strictest measure that requires represented as features in the CRF model.  an exact match between the extracted words and the  annotated words. We use three evaluation metrics:  Extraction Pattern Features for the CRF  recall, precision, and F-measure with recall and preThe extraction patterns provide two kinds of  inforcision equally weighted.  mation. SourcePatt indicates whether a word  activates any source extraction pattern. For  example, the word 'complained' activates the pattern We developed three baseline systems to assess the  '< subj> complained' because it anchors the ex-difficulty of our task. Baseline-1 labels as sources pression. SourceExtr indicates whether a word is  all phrases that belong to the semantic categories  extracted by any source pattern. For example, in the authority, government, human, media,  organization or company, proper name.  plained about France's economy', the words  'PresTable 1 shows that the precision is poor,  suggestident', 'Jacques', and 'Chirac' would all be  exing that the third condition described in Section 3.1  tracted by the '< subj> complained' pattern.  (opinion recognition) does play an important role in Each extraction pattern has frequency and prob-source identification. The recall is much higher but ability values produced by AutoSlog-SE, hence we  still limited due to sources that fall outside of the se-create four IE pattern-based features for each token mantic categories or are not recognized as belong-x  ing to these categories. Baseline-2 labels a noun i:  SourcePatt-Freq, SourceExtr-Freq,  SourcePatt-Prob, and SourceExtr-Prob,  phrase as a source if any of the following are true: where the frequency values are divided into three  (1) the NP is the subject of a verb phrase containing ranges: {0, 1, 2+} and the probability values are di-an opinion word, (2) the NP follows 'according to' , vided into five ranges of equal size.  (3) the NP contains a possessive and is preceded by  an opinion word, or (4) the NP follows 'by' and at-6  taches to an opinion word. Baseline-2' s heuristics are designed to address the first and the third condi-We used the Multi-Perspective Question Answering  tions in Section 3.1. Table 1 shows that Baseline-2  (MPQA) corpus4 for our experiments. This corpus  is substantially better than Baseline-1. Baseline-3  4The MPQA corpus can be freely obtained at  5This data was obtained from the Foreign Broadcast Infor-http://nrrc.mitre.org/NRRC/publications.htm.  mation Service (FBIS), a U.S. government agency.  nearly half of the opinion sources with good  accuWe developed our CRF model using the MALLET  code from McCallum (2002). For training, we used  a Gaussian prior of 0.25, selected based on the  tuning data. We evaluate the CRF using the basic fea-Baseline-3  tures from Section 3, both with and without the IE  pattern features from Section 5. Table 1 shows that  the CRF with basic features outperforms all of the  Extraction Patterns  baselines as well as the extraction patterns, achiev-EM  ing an F-measure of 66.3 using the OL measure,  65.0 using the HM measure, and 59.2 using the  EM measure. Adding the IE pattern features  furbasic features  ther increases performance, boosting recall by about EM  3 points for all of the measures and slightly increas-CRF:  ing precision as well.  basic + IE pattern  features  CRF with feature induction.  One limitation of  CRF-FI:  log-linear function models like CRFs is that they  basic features  cannot form a decision boundary from conjunctions  of existing features, unless conjunctions are explic-CRF-FI:  itly given as part of the feature vector.  For the  basic + IE pattern  task of identifying opinion sources, we observed  features  that the model could benefit from conjunctive  features. For instance, instead of using two separate  Table 1: Source identification performance table  features, HUMAN and  PARENT-CHUNK-INCLUDESOPINION-EXPRESSION, the conjunction of the two  is more informative.  labels a noun phrase as a source if it satisfies both For this reason, we applied the CRF feature in-Baseline-1 and Baseline-2's conditions (this should duction approach introduced by McCallum (2003).  satisfy all three conditions described in Section 3.1).  As shown in Table 1, where CRF-FI stands for the  As shown in Table 1, the precision of this approach  CRF model with feature induction, we see  consisis the best of the three baselines, but the recall is the tent improvements by automatically generating con-lowest.  junctive features.  The final system, which  combines the basic features, the IE pattern features,  Extraction Pattern Experiment  and feature induction achieves an F-measure of 69.4  We evaluated the performance of the learned  extrac(recall=60.6%, precision=81.2%) for the OL  meation patterns on the source identification task. The sure, an F-measure of 68.0 (recall=59.5%, preci-learned patterns were applied to the test data and  sion=79.3%) for the HM measure, and an F-measure  the extracted sources were scored against the manual of 62.0 (recall=54.1%, precision=72.7%) for the EM  annotations.6 Table 1 shows that the extraction  patterns produced lower recall than the baselines, but  with considerably higher precision. These results  Error Analysis  show that the extraction patterns alone can identify An analysis of the errors indicated some common  6These results were obtained using the patterns that had a probability > .50 and frequency > 1.  Some errors resulted from error propagation in  our subsystems. Errors from the sentence  boundin scope than ours in several ways.  Their work  ary detector in GATE (Cunningham et al., 2002)  only addresses propositional opinions, which are  were especially problematic because they caused  'localized in the propositional argument' of  certhe Collins parser to fail, resulting in no  depentain verbs such as 'believe' or 'realize'. In  condency tree information.  trast, our work aims to find sources for all opinions,  Some errors were due to complex and unusual  emotions, and sentiments, including those that are  sentence structure, which our rather simple  feanot related to a verb at all. Furthermore, Berthard  ture encoding for CRF could not capture well.  et al.'s task definition only requires the  identifica Some errors were due to the limited coverage of  tion of direct sources, while our task requires the  the opinion lexicon. We failed to recognize some  identification of both direct and indirect sources.  cases when idiomatic or vague expressions were  Bethard et al. evaluate their system on manually  used to express opinions.  annotated FrameNet (Baker et al., 1998) and  PropBelow are some examples of errors that we found  Bank (Palmer et al., 2005) sentences and achieve  interesting. Doubly underlined phrases indicate  in48% recall with 57% precision.  correctly extracted sources (either false positives  Our IE pattern learner can be viewed as a cross  Opinion words are singly  between AutoSlog (Riloff, 1996a) and  AutoSlogTS (Riloff, 1996b). AutoSlog is a supervised learner False positives:  that requires annotated training data but does not  (1) Actually, these three countries do have one common compute statistics. AutoSlog-TS is a weakly super-denominator, i.e., that their values and policies do not vised learner that does not require annotated data  agree with those of the United States and none of them but generates coarse statistics that measure each pat-are on good terms with the United States.  tern's correlation with relevant and irrelevant  docu(2) Perhaps this is why Fidel Castro has not spoken out against what might go on in Guantanamo.  ments. Consequently, the patterns learned by both  In (1), 'their values and policies' seems like a rea-AutoSlog and AutoSlog-TS need to be manually  resonable phrase to extract, but the annotation does not viewed by a person to achieve good accuracy. In  mark this as a source, perhaps because it is  somewhat abstract. In (2), 'spoken out' is negated, which statistics directly from the annotated training data, means that the verb phrase does not bear an opinion, creating a fully automatic variation of AutoSlog.  but our system failed to recognize the negation.  Conclusion  (3) And for this reason, too, they have a moral duty to speak out, as Swedish Foreign Minister Anna Lindh,  We have described a hybrid approach to the problem  among others, did yesterday.  of extracting sources of opinions in text. We cast  (4) In particular, Iran and Iraq are at loggerheads with this problem as an information extraction task, using each other to this day.  both CRFs and extraction patterns. Our research is  the first to identify both direct and indirect sources that our system could not deal with. (4) involves an for all types of opinions, emotions, and sentiments.  uncommon opinion expression that our system did  Directions for future work include trying to  increase recall by identifying relationships between  opinions and sources that cross sentence boundaries, 7  Related Work  and relationships between multiple opinion  expresTo our knowledge, our research is the first to  autosions by the same source. For example, the fact that matically identify opinion sources using the MPQA  a coreferring noun phrase was marked as a source  opinion annotation scheme. The most closely  rein one sentence could be a useful clue for extracting lated work on opinion analysis is Bethard et al.  the source from another sentence. The probability or (2004), who use machine learning techniques to  the strength of an opinion expression may also play  identify propositional opinions and their holders  a useful role in encouraging or suppressing source  However, their work is more limited  Acknowledgments  the 8th Internatinal Conference on Knowledge Discover and Data Mining.  We thank the reviewers for their many helpful  comM. Palmer, D. Gildea & P. Kingsbury. 2005. The Proposition ments, and the Cornell NLP group for their advice  Bank: An Annotated Corpus of Semantic Roles. In Computational Linguistics 31.  and suggestions for improvement. This work was  B. Pang, L. Lee & S. Vaithyanathan. 2002. Thumbs up? sen-supported by the Advanced Research and  Developtiment classification using machine learning techniques. In ment Activity (ARDA), by NSF Grants IIS-0208028  Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing.  and IIS-0208985, and by the Xerox Foundation.  B. Pang & L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on mini-mum cuts. In Proceedings of the 42nd Annual Meeting of References  the Association for Computational Linguistics.  L. A. Ramshaw & M. P. Marcus. 1995. Nymble: A High-C. Baker, C. Fillmore & J. Lowe.  The Berkeley  Performance Learning Name-Finder. In Proceedings of the FrameNet Project. In Proceedings of the COLING-ACL.  3rd Workshop on Very Large Corpora.  S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou & D. Juraf-E. Riloff. 1996a. An Empirical Study of Automated Dictionary sky. 2004. Automatic extraction of opinion propositions and Construction for Information Extraction in Three Domains.  their holders. In Proceedings of AAAI Spring Symposium on In Artificial Intelligence, Vol. 85.  Exploring Attitude and Affect in Text.  from Untagged Text. In Proceedings of the 13th National Nymble: A High-Performance Learning Name-Finder. In  Conference on Artificial Intelligence.  Proceedings of the Fifth Conference on Applied Natural Lan-E. Riloff & J. Wiebe. 2003. Learning extraction patterns for guage Processing.  subjective expressions. In Proceesings of 2003 Conference E. Breck & C. Cardie. 2004. Playing the Telephone Game: on Empirical Methods in Natural Language Processing.  Determining the Hierarchical Structure of Perspective and E. Riloff & W. Phillips. 2004. An Introduction to the Sun-Speech Expressions. In Proceedings of 20th International dance and AutoSlog Systems Technical Report UUCS-04-Conference on Computational Linguistics.  015, School of Computing, University of Utah.  level annotations and summary representations of opinions tional Random Fields for Information Extraction 18th An-for multiperspective QA. In New Directions in Question Annual Conference on Neural Information Processing Systems.  swering. AAAI Press/MIT Press.  P. Turney. 2002. Thumbs up or thumbs down? semantic orien-M. Collins. 1999. Head-driven Statistical Models for Natural tation applied to unsupervised classification of reviews. In Language Parsing. Ph.D. thesis, University of Pennsylvania.  Proceedings of the 40th Annual Meeting of the Association H. Cunningham, D. Maynard, K. Bontcheva & V. Tablan. 2002.  for Computational Linguistics.  GATE: A Framework and Graphical Development  EnvironT. Wilson, J. Wiebe & R. Hwa. 2004. Just how mad are you?  ment for Robust NLP Tools and Applications. In Proceed-finding strong and weak opinion clauses. In Proceedings of ings of the 40th Anniversary Meeting of the Association for the 9th National Conference on Artificial Intelligence.  Computational Linguistics.  T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J. Wiebe, S. Das & M. Chen. 2001. Yahoo for amazon: Extracting market Y. Choi, C. Cardie, E. Riloff & S. Patwardhan. 2005. Opin-sentiment from stock message boards. In Proceedings of the ionFinder: A system for subjectivity analysis. Demonstra-8th Asia Pacific Finance Association Annual Conference.  tion Description in Conference on Empirical Methods in K. Dave, S. Lawrence & D. Pennock. 2003. Mining the peanut Natural Language Processing.  gallery: Opinion extraction and semantic classification of J. Yi, T. Nasukawa, R. Bunescu & W. Niblack. 2003. Sentiment product reviews. In International World Wide Web Confer-Analyzer: Extracting Sentiments about a Given Topic using ence.  Natural Language Processing Techniques. In Proceedings of J. Lafferty, A. K. McCallum & F. Pereira. 2001. Conditional the 3rd IEEE International Conference on Data Mining.  Random Fields: Probabilistic Models for Segmenting and H. Yu & V. Hatzivassiloglou. 2003. Towards answering opin-Labeling Sequence Data. In Proceedings of 18th Interna-ion questions: Separating facts from opinions and identify-tional Conference on Machine Learning.  ing the polarity of opinion sentences. In Proceedings of the B. Levin. 1993. English Verb Classes and Alternations: A Conference on Empirical Methods in Natural Language Pro-Preliminary Investigation. University of Chicago Press.  cessing.  A. K. McCallum. 2002. MALLET: A Machine Learning for J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis, B. Fraser, Language Toolkit. http://mallet.cs.umass.edu.  D. Litman, D. Pierce, E. Riloff & T. Wilson. 2002. NRRC  A. K. McCallum. 2003. Efficiently Inducing Features of Con-Summer Workshop on Multiple-Perspective Question  Anditional Random Fields. In Conference on Uncertainty in swering: Final Report.  J. Wiebe & E. Riloff. 2005. Creating subjective and objective A. K. McCallum & W. Li. 2003. Early Results for Named sentence classifiers from unannotated texts. Sixth Interna-Entity Recognition with Conditional Random Fields, Feature tional Conference on Intelligent Text Processing and Com-Induction and Web-Enhanced Lexicons. In Conference on putational Linguistics.  Natural Language Learning.  J. Wiebe, T. Wilson & C. Cardie. 2005. Annotating expressions S. Morinaga, K. Yamanishi, K. Tateishi & T. Fukushima 2002.  of opinions and emotions in language. Language Resources Mining Product Reputations on the Web. In Proceedings of and Evaluation, 1(2). 