 Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank  Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng and Christopher Potts Stanford University, Stanford, CA 94305, USA  richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu  {jeaneis,manning,cgpotts}@stanford.edu  Semantic word spaces have been very  useThis  ful but cannot express the meaning of longer  phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank. It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.  To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.  trained on the new treebank, this model  outperforms all previous methods on several  metrics. It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%. The accuracy of predicting  nomena presented in such data. To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines. Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.  for both positive and negative phrases.  The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1  Introduction  complete analysis of the compositional effects of  sentiment in language.  The corpus is based on  Semantic vector spaces for single words have been  the dataset introduced by Pang and Lee (2005) and  widely used as features (Turney and Pantel, 2010).  consists of 11,855 single sentences extracted from  Because they cannot capture the meaning of longer  It was parsed with the Stanford  phrases properly, compositionality in semantic  vecparser (Klein and Manning, 2003) and includes a  tor spaces has recently received a lot of attention  total of 215,154 unique phrases from those parse  trees, each annotated by 3 human judges. This new  Zanzotto et al., 2010; Yessenalina and Cardie, 2011; dataset allows us to analyze the intricacies of senti-Socher et al., 2012; Grefenstette et al., 2013). How-ment and to capture complex linguistic phenomena.  ever, progress is held back by the current lack of  Fig. 1 shows one of the many examples with clear  large and labeled compositionality resources and  compositional structure. The granularity and size of  this dataset will enable the community to train com-word appears in a certain syntactic context (Pado  positional models that are based on supervised and  and Lapata, 2007; Erk and Pad , 2008). However,  structured machine learning techniques. While there  distributional vectors often do not properly capture are several datasets with document and chunk labels  the differences in antonyms since those often have  available, there is a need to better capture sentiment similar contexts. One possibility to remedy this is to from short comments, such as Twitter data, which  use neural word vectors (Bengio et al., 2003). These provide less overall signal per document.  vectors can be trained in an unsupervised fashion  In order to capture the compositional effects with  to capture distributional similarities (Collobert and higher accuracy, we propose a new model called the  Weston, 2008; Huang et al., 2012) but then also be  Recursive Neural Tensor Network (RNTN).  Recurfine-tuned and trained to specific tasks such as sensive Neural Tensor Networks take as input phrases  timent detection (Socher et al., 2011b). The models  of any length. They represent a phrase through word  in this paper can use purely supervised word  reprevectors and a parse tree and then compute vectors for sentations learned entirely on the new corpus.  higher nodes in the tree using the same tensor-based Compositionality in Vector Spaces.  Most of  composition function. We compare to several  superthe compositionality algorithms and related datasets vised, compositional models such as standard recur-capture two word compositions. Mitchell and  Lasive neural networks (RNN) (Socher et al., 2011b),  pata (2010) use e.g. two-word phrases and analyze  matrix-vector RNNs (Socher et al., 2012), and  basesimilarities computed by vector addition, multiplicalines such as neural networks that ignore word order, tion and others. Some related models such as holo-Naive Bayes (NB), bi-gram NB and SVM. All  modgraphic reduced representations (Plate, 1995),  quanels get a significant boost when trained with the new tum logic (Widdows, 2008), discrete-continuous  dataset but the RNTN obtains the highest  performodels (Clark and Pulman, 2007) and the recent  mance with 80.7% accuracy when predicting  finegrained sentiment for all nodes. Lastly, we use a test Giesbrecht, 2010) have not been experimentally val-set of positive and negative sentences and their  reidated on larger corpora. Yessenalina and Cardie  spective negations to show that, unlike bag of words (2011) compute matrix representations for longer  models, the RNTN accurately captures the sentiment  phrases and define composition as matrix  multiplichange and scope of negation. RNTNs also learn  cation, and also evaluate on sentiment.  that sentiment of phrases following the contrastive  stette and Sadrzadeh (2011) analyze  subject-verbobject triplets and find a matrix-based categorical  The complete training and testing code, a live  model to correlate well with human judgments. We  demo and the Stanford Sentiment Treebank dataset  compare to the recent line of work on supervised  In particular we will  describe and experimentally compare our new RNTN  model to recursive neural networks (RNN) (Socher  Related Work  This work is connected to five different areas of NLP  al., 2012) both of which have been applied to bag of research, each with their own large amount of related words sentiment corpora.  work to which we cannot do full justice given space  A related field that tackles  compositionality from a very different angle is that of Semantic Vector Spaces.  The dominant  aptrying to map sentences to logical form (Zettlemoyer proach in semantic vector spaces uses distributional and Collins, 2005). While these models are highly  similarities of single words. Often, co-occurrence  interesting and work well in closed domains and  statistics of a word and its context are used to  deon discrete sets, they could only capture sentiment  scribe each word (Turney and Pantel, 2010; Baroni  distributions using separate mechanisms beyond the  and Lenci, 2010), such as tf-idf. Variants of this idea currently used logical forms.  use more complex frequencies such as how often a  Apart from the above mentioned  work on RNNs, several compositionality ideas re-nerdy folks  lated to neural networks have been discussed by  Bottou (2011) and Hinton (1990) and first models such  Very  Negative Somewhat Neutral Somewhat Positive  Very  as Recursive Auto-associative memories been  experimented with by Pollack (1990). The idea to relate  phenomenal fantasy best sellers  inputs through three way interactions, parameterized by a tensor have been proposed for relation  classifiNegative Somewhat Neutral Somewhat Positive  Very  extending Restricted Boltzmann machines (Ranzato  Figure 3: The labeling interface. Random phrases were and Hinton, 2010) and as a special layer for speech  shown and annotators had a slider for selecting the senti-recognition (Yu et al., 2012).  Sentiment Analysis.  Apart from the  abovementioned work, most approaches in sentiment  analtences, half of which were considered positive and  ysis use bag of words representations (Pang and Lee, the other half negative. Each label is extracted from 2008). Snyder and Barzilay (2007) analyzed larger  a longer movie review and reflects the writer's over-reviews in more detail by analyzing the sentiment  all intention for this review. The normalized, lower-of multiple aspects of restaurants, such as food or  cased text is first used to recover, from the  origiatmosphere. Several works have explored sentiment  nal website, the text with capitalization. Remaining compositionality through careful engineering of fea-HTML tags and sentences that are not in English  tures or polarity shifting rules on syntactic structures are deleted. The Stanford Parser (Klein and  Manning, 2003) is used to parses all 10,662 sentences.  In approximately 1,100 cases it splits the snippet  into multiple sentences. We then used Amazon  MeStanford Sentiment Treebank  chanical Turk to label the resulting 215,154 phrases.  Bag of words classifiers can work well in longer  Fig. 3 shows the interface annotators saw. The slider documents by relying on a few words with strong  has 25 different values and is initially set to neutral.  sentiment like awesome' or exhilarating.'  HowThe phrases in each hit are randomly sampled from  ever, sentiment accuracies even for binary  posithe set of all phrases in order to prevent labels being tive/negative classification for single sentences has influenced by what follows. For more details on the  not exceeded 80% for several years. For the more  dataset collection, see supplementary material.  difficult multiclass case including a neutral class, Fig. 2 shows the normalized label distributions at  accuracy is often below 60% for short messages  each n-gram length. Starting at length 20, the  maon Twitter (Wang et al., 2012). From a linguistic  jority are full sentences. One of the findings from  or cognitive standpoint, ignoring word order in the  labeling sentences based on reader's perception is  treatment of a semantic task is not plausible, and, as that many of them could be considered neutral. We  we will show, it cannot accurately classify hard  exalso notice that stronger sentiment often builds up  amples of negation. Correctly predicting these hard  in longer phrases and the majority of the shorter  cases is necessary to further improve performance.  phrases are neutral. Another observation is that most In this section we will introduce and provide some  annotators moved the slider to one of the five  poanalyses for the new Sentiment Treebank which  includes labels for every syntactically plausible phrase tive or somewhat positive. The extreme values were  in thousands of sentences, allowing us to train and  rarely used and the slider was not often left in  between the ticks. Hence, even a 5-class classification We consider the corpus of movie review excerpts  into these categories captures the main variability  from the rottentomatoes.com website  origof the labels. We will name this fine-grained  sentiinally collected and published by Pang and Lee  ment classification and our main experiment will be  (2005). The original dataset includes 10,662  sento recover these five labels for phrases of all lengths.  Distributions of sentiment values for (a) unigrams,  Very Positive  (b) 10-grams, (c) 20-grams, and (d) full sentences.  Somewhat Positive  of Sentim%  Somewhat Negative  Negative  Very Negative  Figure 2: Normalized histogram of sentiment annotations at each n-gram length. Many shorter n-grams are neutral; longer phrases are well distributed. Few annotators used slider positions between ticks or the extreme values. Hence the two strongest labels and intermediate tick positions are merged into 5 classes.  The models in this section compute compositional  vector representations for phrases of variable length  and syntactic type. These representations will then  be used as features to classify each phrase. Fig. 4  displays this approach. When an n-gram is given to  the compositional models, it is parsed into a binary not very good ...  tree and each leaf node, corresponding to a word,  is represented as a vector. Recursive neural  models will then compute parent vectors in a bottom  Figure 4: Approach of Recursive Neural Network  modup fashion using different types of  compositionalels for sentiment: Compute parent vectors in a bottom up ity functions g. The parent vectors are again given  fashion using a compositionality function g and use node as features to a classifier. For ease of exposition, vectors as features for a classifier at that node. This func-we will use the tri-gram in this figure to explain all tion varies for the different models.  We first describe the operations that the below  relabels given the word vector via:  cursive neural models have in common: word vector  representations and classification. This is followed ya = softmax(Wsa),  by descriptions of two previous RNN models and  where W  s R  is the sentiment classification  matrix. For the given tri-gram, this is repeated for Each word is represented as a d-dimensional vec-vectors b and c. The main task of and difference  tor.  We initialize all word vectors by randomly  between the models will be to compute the hidden  sampling each value from a uniform distribution:  vectors p  i R in a bottom up fashion.  U ( r, r), where r = 0.0001. All the word  vectors are stacked in the word embedding matrix L  RNN: Recursive Neural Network  , where |V | is the size of the vocabulary.  IniThe simplest member of this family of neural  nettially the word vectors will be random but the L  mawork models is the standard recursive neural  nettrix is seen as a parameter that is trained jointly with work (Goller and K chler, 1996; Socher et al.,  the compositionality models.  2011a). First, it is determined which parent already We can use the word vectors immediately as  has all its children computed. In the above tree  exparameters to optimize and as feature inputs to  ample, p1 has its two children's vectors since both  a softmax classifier.  For classification into five  are words. RNNs use the following equations to  classes, we compute the posterior probability over  compute the parent vectors:  p1 = f  the MV-RNN computes the first parent vector and its  where f = tanh is a standard element-wise  nonlinmatrix via two equations:  earity, W  is the main parameter to learn  and we omit the bias for simplicity. The bias can be Cb  added as an extra column to W if an additional 1 is  added to the concatenation of the input vectors. The parent vectors must be of the same dimensionality to where W  and the result is again a d d  be recursively compatible and be used as input to the matrix. Similarly, the second parent node is com-next composition. Each parent vector pi, is given to puted using the previously computed (vector,matrix)  the same softmax classifier of Eq. 1 to compute its  pair (p1, P1) as well as (a, A). The vectors are used label probabilities.  for classifying each phrase using the same softmax  This model uses the same compositionality  funcclassifier as in Eq. 1.  tion as the recursive autoencoder (Socher et al.,  RNTN:Recursive Neural Tensor Network  2011b) and recursive auto-associate memories  (Pollack, 1990). The only difference to the former model One problem with the MV-RNN is that the number  is that we fix the tree structures and ignore the re-of parameters becomes very large and depends on  construction loss. In initial experiments, we found  the size of the vocabulary. It would be cognitively  that with the additional amount of training data, the more plausible if there was a single powerful com-reconstruction loss at each node is not necessary to position function with a fixed number of parameters.  obtain high performance.  The standard RNN is a good candidate for such a  function. However, in the standard RNN, the input  vectors only implicitly interact through the  nonlinearity (squashing) function. A more direct, possibly multiplicative, interaction would allow the model to The MV-RNN is linguistically motivated in that  have greater interactions between the input vectors.  most of the parameters are associated with words  Motivated by these ideas we ask the question: Can  and each composition function that computes  veca single, more powerful composition function  pertors for longer phrases depends on the actual words  form better and compose aggregate meaning from  being combined. The main idea of the MV-RNN  smaller constituents more accurately than many  in(Socher et al., 2012) is to represent every word and put specific ones? In order to answer this question, longer phrase in a parse tree as both a vector and  we propose a new model called the Recursive  Neua matrix. When two constituents are combined the  ral Tensor Network (RNTN). The main idea is to use  matrix of one is multiplied with the vector of the  the same, tensor-based composition function for all  other and vice versa. Hence, the compositional  function is parameterized by the words that participate in Fig. 5 shows a single tensor layer. We define the  output of a tensor product h  R via the  followEach word's matrix is initialized as a d d identity  ing vectorized notation and the equivalent but more  matrix, plus a small amount of Gaussian noise.  Simdetailed notation for each slice V [i]  ilar to the random word vectors, the parameters of  these matrices will be trained to minimize the  clasb T  b T  sification error at each node. For this model, each n-c  gram is represented as a list of (vector,matrix) pairs, together with the parse tree. For the tree with (vec-where V [1:d]  is the tensor that defines  softmax classifier trained on its vector representa-Neural Tensor Layer  tion to predict a given ground truth or target vector Slices of  t. We assume the target distribution vector at each  node has a 0-1 encoding. If there are C classes, then it has length C and a 1 at the correct label. All other entries are 0.  p = f +  We want to maximize the probability of the  correct prediction, or minimize the cross-entropy error between the predicted distribution yi  node i and the target distribution ti  at that  node. This is equivalent (up to a constant) to  minip = f V[1:2] + W  mizing the KL-divergence between the two  distributions. The error as a function of the RNTN  parameters = (V, W, Ws, L) for a sentence is:  Figure 5: A single layer of the Recursive Neural Ten-E( ) =  sor Network. Each dashed box represents one of d-many i  slices and can capture a type of influence a child can have on its parent.  The derivative for the weights of the softmax  classifier are standard and simply sum up from each  node's error. We define xi to be the vector at node  The RNTN uses this definition for computing p1:  i (in the example trigram, the xi  's are  1, p2)). We skip the standard derivative for  b T  s. Each node backpropagates its error through to  the recursively used weights V, W . Let i,s  be the softmax error vector at node i:  where W is as defined in the previous models. The  next parent vector p2 in the tri-gram will be  com i,s = W T  puted with the same weights:  where is the Hadamard product between the two  a T  vectors and f 0 is the element-wise derivative of f  p2 = f  which in the standard case of using f = tanh can  be computed using only f (xi).  The remaining derivatives can only be computed  The main advantage over the previous RNN  in a top-down fashion from the top node through the  model, which is a special case of the RNTN when  tree and into the leaf nodes. The full derivative for is set to 0, is that the tensor can directly relate in-V and W is the sum of the derivatives at each of  put vectors. Intuitively, we can interpret each slice the nodes. We define the complete incoming error  of the tensor as capturing a specific type of  compomessages for a node i as i,com. The top node, in  An alternative to RNTNs would be to make the  2, only received errors from the top node's  softmax. Hence, p2,com = p2,s which we can  compositional function more powerful by adding a  use to obtain the standard backprop derivative for  second neural network layer. However, initial  experiments showed that it is hard to optimize this model For the derivative of each slice k = 1, . . . , d, we get: and vector interactions are still more implicit than in the RNTN.  a a T  Tensor Backprop through Structure  We describe in this section how to train the RNTN  where p2,com is just the k'th element of this vector.  As mentioned above, each node has a  Now, we can compute the error message for the two  children of p2:  Fine-grained  Positive/Negative  where we define  The children of p2, will then each take half of this Table 1: Accuracy for fine grained (5-class) and binary vector and add their own softmax error message for  predictions at the sentence level (root) and for all nodes.  the complete . In particular, we have  showed that the recursive models worked  significantly worse (over 5% drop in accuracy) when no  where p2,down[d + 1 : 2d] indicates that p  nonlinearity was used. We use f = tanh in all  ex1 is the  right child of p  2 and hence takes the 2nd half of the  error, for the final word vector derivative for a, it We compare to commonly used methods that use  will be p2,down[1 : d].  bag of words features with Naive Bayes and SVMs,  The full derivative for slice V [k] for this trigram as well as Naive Bayes with bag of bigram features.  tree then is the sum at each node:  We abbreviate these with NB, SVM and biNB. We  also compare to a model that averages neural word  vectors and ignores word order (VecAvg).  The sentences in the treebank were split into a  train (8544), dev (1101) and test splits (2210) and  and similarly for W . For this nonconvex  optimizathese splits are made available with the data release.  tion we use AdaGrad (Duchi et al., 2011) which  conWe also analyze performance on only positive and  verges in less than 3 hours to a local optimum.  negative sentences, ignoring the neutral class. This filters about 20% of the data with the three sets hav-5  ing 6920/872/1821 sentences.  We include two types of analyses. The first type  inFine-grained Sentiment For All Phrases  cludes several large quantitative evaluations on the test set. The second type focuses on two linguistic  The main novel experiment and evaluation metric  phenomena that are important in sentiment.  analyze the accuracy of fine-grained sentiment  clasFor all models, we use the dev set and  crosssification for all phrases. Fig. 2 showed that a fine validate over regularization of the weights, word  grained classification into 5 classes is a reasonable vector size as well as learning rate and minibatch  approximation to capture most of the data variation.  size for AdaGrad. Optimal performance for all  modFig. 6 shows the result on this new corpus. The  RNTN gets the highest performance, followed by  and 35 dimensions and batch sizes between 20 and  the MV-RNN and RNN. The recursive models work  30. Performance decreased at larger or smaller  vecvery well on shorter phrases, where negation and  tor and batch sizes. This indicates that the RNTN  composition are important, while bag of features  does not outperform the standard RNN due to  simbaselines perform well only with longer sentences.  ply having more parameters. The MV-RNN has  orThe RNTN accuracy upper bounds other models at  ders of magnitudes more parameters than any other  model due to the word matrices. The RNTN would  Table 1 (left) shows the overall accuracy numbers  usually achieve its best performance on the dev set  for fine grained prediction at all phrase lengths and after training for 3 5 hours. Initial experiments  Figure 6: Accuracy curves for fine grained sentiment classification at each n-gram lengths. Left: Accuracy separately for each set of n-grams. Right: Cumulative accuracy of all n-grams.  Full Sentence Binary Sentiment  This setup is comparable to previous work on the  original rotten tomatoes dataset which only used  full sentence labels and binary classification of pos-0  itive/negative. Hence, these experiments show the  There  has  enough  with the sentiment treebank. Table 1 shows results  of this binary classification for both all phrases and 0  and  for only full sentences. The previous state of the  Figure 7: Example of correct prediction for contrastive art was below 80% (Socher et al., 2012). With the  coarse bag of words annotation for training, many of the more complex phenomena could not be captured,  Set 1: Negating Positive Sentences.  The first set  even by more powerful models. The combination of  contains positive sentences and their negation. In  the new sentiment treebank and the RNTN pushes  this set, the negation changes the overall sentiment the state of the art on short phrases up to 85.4%.  of a sentence from positive to negative. Hence, we  Model Analysis: Contrastive Conjunction  compute accuracy in terms of correct sentiment  reIn this section, we use a subset of the test set which versal from positive to negative. Fig. 9 shows two  includes only sentences with an X but Y ' structure: examples of positive negation the RNTN correctly  A phrase X being followed by but which is followed  classified, even if negation is less obvious in the case by a phrase Y . The conjunction is interpreted as  of least'. Table 2 (left) gives the accuracies over 21  an argument for the second conjunct, with the first  positive sentences and their negation for all models.  functioning concessively (Lakoff, 1971; Blakemore,  The RNTN has the highest reversal accuracy,  show1989; Merin, 1999). Fig. 7 contains an example. We  ing its ability to structurally learn negation of posi-analyze a strict setting, where X and Y are phrases  tive sentences. But what if the model simply makes  of different sentiment (including neutral). The  exphrases very negative when negation is in the  senample is counted as correct, if the classifications for tence? The next experiments show that the model  both phrases X and Y are correct. Furthermore,  captures more than such a simplistic negation rule.  the lowest node that dominates both of the word  Set 2: Negating Negative Sentences.  The  secbut and the node that spans Y also have to have the  ond set contains negative sentences and their  negasame correct sentiment. For the resulting 131 cases, tion. When negative sentences are negated, the sen-the RNTN obtains an accuracy of 41% compared to  timent treebank shows that overall sentiment should  MV-RNN (37), RNN (36) and biNB (27).  become less negative, but not necessarily positive.  For instance, The movie was terrible' is negative  Model Analysis: High Level Negation  but the The movie was not terrible' says only that it We investigate two types of negation. For each type, was less bad than a terrible one, not that it was good we use a separate dataset for evaluation.  (Horn, 1989; Israel, 2001). Hence, we evaluate  acof  of  the  the  this theme  this theme  most compelling  least compelling  It  's  just incredibly dull  It  every  of  of  this  's definitely  this  Figure 9: RNTN prediction of positive and negative (bottom right) sentences and their negation.  Negated Positive  Negated Negative  Table 2: Accuracy of negation detection. Negated posi-5171  tive is measured as correct sentiment inversions. Negated negative is measured as increases in positive activations.  Figure 8: Change in activations for negations. Only the RNTN correctly captures both types. It decreases positive curacy in terms of how often each model was able  sentiment more when it is negated and learns that negating negative phrases (such as not terrible) should increase to increase non-negative activation in the sentiment neutral and positive activations.  of the sentence. Table 2 (right) shows the accuracy.  In over 81% of cases, the RNTN correctly increases  the positive activations. Fig. 9 (bottom right) shows age positive activation (for set 1) and positive values a typical case in which sentiment was made more  mean an increase in average positive activation (set positive by switching the main class from negative  2). The RNTN has the largest shifts in the correct di-to neutral even though both not and dull were  negarections. Therefore we can conclude that the RNTN  tive. Fig. 8 shows the changes in activation for both is best able to identify the effect of negations upon sets. Negative values indicate a decrease in aver-both positive and negative sentiment sentences.  engaging; best; powerful; love; beautiful  bad; dull; boring; fails; worst; stupid; painfully  worst movie; very bad; shapeless mess; worst  film; wonderful movie; marvelous performances  thing; instantly forgettable; complete failure  an amazing performance; wonderful all-ages  trifor worst movie; A lousy movie; a complete  failumph; a wonderful movie; most visually stunning  ure; most painfully marginal; very bad sign  nicely acted and beautifully shot; gorgeous  imsilliest and most incoherent movie; completely  agery, effective performances; the best of the  crass and forgettable movie; just another bad  year; a terrific American sports movie;  refreshmovie. A cumbersome and cliche-ridden movie;  ingly honest and ultimately touching  a humorless, disjointed mess  one of the best films of the year; A love for films  A trashy, exploitative, thoroughly unpleasant  exshines through each frame; created a masterful  perience ; this sloppy drama is an empty  vespiece of artistry right here; A masterful film from  sel.; quickly drags on becoming boring and  predictable.; be the worst special-effects creation of  the year  Table 3: Examples of n-grams for which the RNTN predicted the most positive and most negative responses.  Conclusion  We introduced Recursive Neural Tensor Networks  and the Stanford Sentiment Treebank. The  combination of new model and data results in a system  for single sentence sentiment detection that pushes  state of the art by 5.4% for positive/negative  sentence classification. Apart from this standard  setting, the dataset also poses important new challenges Figure 10: Average ground truth sentiment of top 10 most and allows for new evaluation metrics. For instance, positive n-grams at various n. The RNTN correctly picks the RNTN obtains 80.7% accuracy on fine-grained  the more negative and positive examples.  sentiment prediction across all phrases and captures negation of different sentiments and scope more accurately than previous models.  Model Analysis: Most Positive and  Acknowledgments  Negative Phrases  Tariq for the first version of the online demo.  We queried the model for its predictions on what  Richard is partly supported by a Microsoft  Rethe most positive or negative n-grams are, measured  search PhD fellowship. The authors gratefully  acas the highest activation of the most negative and  knowledge the support of the Defense Advanced  Remost positive classes. Table 3 shows some phrases  search Projects Agency (DARPA) Deep Exploration  from the dev set which the RNTN selected for their  and Filtering of Text (DEFT) Program under Air  Force Research Laboratory (AFRL) prime contract  no. FA8750-13-2-0040, the DARPA Deep Learning  Due to lack of space we cannot compare top  program under contract number FA8650-10-C-7020  phrases of the other models but Fig. 10 shows that  and NSF IIS-1159679. Any opinions, findings, and  the RNTN selects more strongly positive phrases at  conclusions or recommendations expressed in this  most n-gram lengths compared to other models.  material are those of the authors and do not  necesFor this and the previous experiment, please find  sarily reflect the view of DARPA, AFRL, or the US  additional examples and descriptions in the  suppleA. Merin. 1999. Information, relevance, and social deci-sionmaking: Some principles and results of  decisionM. Baroni and A. Lenci. 2010. Distributional  memtheoretic semantics. In Lawrence S. Moss, Jonathan  ory: A general framework for corpus-based semantics.  Computational Linguistics, 36(4):673 721.  guage, and Information, volume 2. CSLI, Stanford,  Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin.  Mach. Learn. Res., 3, March.  J. Mitchell and M. Lapata. 2010. Composition in  disD. Blakemore. 1989. Denial and contrast: A relevance tributional models of semantics. Cognitive Science,  theoretic analysis of but'. Linguistics and Philoso-34(8):1388 1429.  K. Moilanen and S. Pulman. 2007. Sentiment  composiL. Bottou. 2011. From machine learning to machine  tion. In In Proceedings of Recent Advances in Natural reasoning. CoRR, abs/1102.1808.  Language Processing.  S. Clark and S. Pulman. 2007. Combining symbolic and T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Depen-distributional models of meaning. In Proceedings of  dency tree-based sentiment classification using CRFs the AAAI Spring Symposium on Quantum Interaction,  with hidden variables. In NAACL, HLT.  S. Pado and M. Lapata. 2007. Dependency-based  conR. Collobert and J. Weston. 2008. A unified architecture struction of semantic space models. Computational  for natural language processing: deep neural networks Linguistics, 33(2):161 199.  with multitask learning. In ICML.  B. Pang and L. Lee. 2005. Seeing stars: Exploiting class J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-relationships for sentiment categorization with respect gradient methods for online learning and stochastic op-to rating scales. In ACL, pages 115 124.  timization. JMLR, 12, July.  K. Erk and S. Pad . 2008. A structured vector space  ment analysis. Foundations and Trends in Information model for word meaning in context. In EMNLP.  dependent distributed representations by backpropaga-T. A. Plate. 1995. Holographic reduced representations.  tion through structure. In Proceedings of the Interna-IEEE Transactions on Neural Networks, 6(3):623  tional Conference on Neural Networks (ICNN-96).  support for a categorical compositional distributional shifters. In W. Bruce Croft, James Shanahan, Yan Qu, model of meaning. In EMNLP.  and Janyce Wiebe, editors, Computing Attitude and Af-E. Grefenstette, G. Dinu, Y.-Z. Zhang, M. Sadrzadeh, and fect in Text: Theory and Applications, volume 20 of  M. Baroni. 2013. Multi-step regression learning for  The Information Retrieval Series, chapter 1.  J. B. Pollack. 1990. Recursive distributed representa-G. E. Hinton. 1990. Mapping part-whole hierarchies into tions. Artificial Intelligence, 46, November.  connectionist networks. Artificial Intelligence, 46(1-M. Ranzato and A. Krizhevsky G. E. Hinton.  Factored 3-Way Restricted Boltzmann Machines For  L. R. Horn. 1989. A natural history of negation, volume Modeling Natural Images. AISTATS.  960. University of Chicago Press Chicago.  Improving Word Representations via Global  sentiment analysis by joining machine learning and  Context and Multiple Word Prototypes. In ACL.  rule based methods. In Proceedings of the Seventh  Minimizers, maximizers, and the  conference on International Language Resources and  rhetoric of scalar reasoning.  Journal of Semantics,  2012. A latent factor model for highly multi-relational matrix-space models of language. In ACL.  B. Snyder and R. Barzilay. 2007. Multiple aspect rank-D. Klein and C. D. Manning. 2003. Accurate unlexicaling using the Good Grief algorithm. In HLT-NAACL.  ized parsing. In ACL.  R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning R. Lakoff. 1971. If's, and's, and but's about conjunction.  continuous phrase representations and syntactic pars-In Charles J. Fillmore and D. Terence Langendoen, eding with recursive neural networks. In Proceedings of itors, Studies in Linguistic Semantics, pages 114 149.  the NIPS-2010 Deep Learning and Unsupervised  FeaHolt, Rinehart, and Winston, New York.  ture Learning Workshop.  R. Socher, C. Lin, A. Y. Ng, and C.D. Manning. 2011a.  Parsing Natural Scenes and Natural Language with  Recursive Neural Networks. In ICML.  R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning. 2011b. Semi-Supervised Recursive  Autoencoders for Predicting Sentiment Distributions.  In EMNLP.  R. Socher, B. Huval, C. D. Manning, and A. Y. Ng. 2012.  Semantic compositionality through recursive  matrixvector spaces. In EMNLP.  tered tensor factorization. In NIPS.  P. D. Turney and P. Pantel. 2010. From frequency to  meaning: Vector space models of semantics. Journal  of Artificial Intelligence Research, 37:141 188.  S. Narayanan. 2012. A system for real-time  twitter sentiment analysis of 2012 u.s. presidential election cycle. In Proceedings of the ACL 2012 System  Demonstrations.  D. Widdows. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second AAAI Symposium on Quantum Interaction.  A. Yessenalina and C. Cardie.  tional matrix-space models for sentiment analysis. In EMNLP.  D. Yu, L. Deng, and F. Seide. 2012. Large vocabulary speech recognition using deep tensor neural networks.  F.M. Zanzotto, I. Korkontzelos, F. Fallucchi, and S. Man-andhar. 2010. Estimating linear models for  compositional distributional semantics. In COLING.  L. Zettlemoyer and M. Collins.  Learning to  map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI. 