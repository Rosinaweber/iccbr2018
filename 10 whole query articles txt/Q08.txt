 Semi-Supervised Recursive Autoencoders  for Predicting Sentiment Distributions  Jeffrey Pennington  Christopher D. Manning  Computer Science Department, Stanford University, Stanford, CA 94305, USA  SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA richard@socher.org  Predicted  We introduce a novel machine learning  frameSorry, Hugs You Rock Teehee I Understand Wow, Just Wow work based on recursive autoencoders for  Distribution  sentence-level prediction of sentiment label  distributions. Our method learns vector space  Representations  representations for multi-word phrases.  Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules. We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases. Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue). Then they are recursively  on confessions from the experience project.  merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation. The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.  gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.  manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules). This limits the  applicability of these methods to a broader range of  Introduction  tasks and languages.  Lastly, almost all previous  The ability to identify sentiments about personal  exwork is based on single, positive/negative categories  or scales such as star ratings. Examples are movie  derstand user generated content in social networks,  reviews (Pang and Lee, 2005), opinions (Wiebe et  blogs or product reviews. Detecting sentiment in  al., 2005), customer reviews (Ding et al., 2008) or  these data is a challenging task which has recently  multiple aspects of restaurants (Snyder and Barzilay,  spawned a lot of interest (Pang and Lee, 2008).  Current baseline methods often use bag-of-words  rately reflect the complexity of human emotions and  representations which cannot properly capture more  complex linguistic phenomena in sentiment  analyIn this work, we seek to address three issues. (i)  sis (Pang et al., 2002). For instance, while the two  Instead of using a bag-of-words representation, our  phrases white blood cells destroying an infection  model exploits hierarchical structure and uses  comand an infection destroying white blood cells have  the same bag-of-words representation, the former is  Our system can be trained both on unlabeled  doa positive reaction while the later is very negative.  main data and on supervised sentiment data and does  More advanced methods such as (Nakagawa et al.,  parsers, etc. (iii) Rather than limiting sentiment to learn phrase representations, phrase structure and  nected sentiments.  Neural Word Representations  We introduce an approach based on  semiWe represent words as continuous vectors of  paramsupervised, recursive autoencoders (RAE) which  eters. We explore two settings. In the first setting  use as input continuous word vectors. Fig. 1 shows  we simply initialize each word vector x  by  an illustration of the model which learns vector  repsampling it from a zero mean Gaussian distribution:  resentations of phrases and full sentences as well as  x N (0, 2). These word vectors are then stacked  their hierarchical structure from unsupervised text.  into a word embedding matrix L  , where  We extend our model to also learn a distribution over  |V | is the size of the vocabulary. This initialization  sentiment labels at each node of the hierarchy.  works well in supervised settings where a network  We evaluate our approach on several standard  can subsequently modify these vectors to capture  datasets where we achieve state-of-the art  performance. Furthermore, we show results on the  reIn the second setting, we pre-train the word  veccently introduced experience project (EP) dataset  tors with an unsupervised neural language model  (Potts, 2010) that captures a broader spectrum of  (Bengio et al., 2003; Collobert and Weston, 2008).  human sentiments and emotions. The dataset  conThese models jointly learn an embedding of words  sists of very personal confessions anonymously  into a vector space and use these vectors to predict  made by people on the experience project website  how likely a word occurs given its context. After  www.experienceproject.com.  learning via gradient ascent the word vectors  capbeled with a set of five reactions by other users.  Reture syntactic and semantic information from their  co-occurrence statistics.  tehee (amusement), I understand, Sorry, hugs and  In both cases we can use the resulting matrix of  Wow, just wow (displaying shock). For evaluation on  word vectors L for subsequent tasks as follows.  Asthis dataset we predict both the label with the most  sume we are given a sentence as an ordered list of  votes as well as the full distribution over the  sentim words. Each word has an associated vocabulary  ment categories. On both tasks our model  outperindex k into the embedding matrix which we use to  forms competitive baselines. A set of over 31,000  retrieve the word's vector representation.  Mathematconfessions as well as the code of our model are  ically, this look-up operation can be seen as a  simavailable at www.socher.org.  ple projection layer where we use a binary vector b  After describing the model in detail, we  evaluwhich is zero in all positions except at the kth index,  ate it qualitatively by analyzing the learned n-gram  vector representations and compare quantitatively  against other methods on standard datasets and the  In the remainder of this paper, we represent a  sentence (or any n-gram) as an ordered list of these  Semi-Supervised Recursive  vectors (x1, . . . , xm). This word representation is  better suited to autoencoders than the binary number  representations used in previous related autoencoder  Our model aims to find vector representations for  models such as the recursive autoassociative  memvariable-sized phrases in either unsupervised or  ory (RAAM) model (Pollack, 1990; Voegtlin and  semi-supervised training regimes. These  representaDominey, 2005) or recurrent neural networks  (Eltions can then be used for subsequent tasks. We first  man, 1991) since sigmoid units are inherently  condescribe neural word representations and then  protinuous. Pollack circumvented this problem by  havceed to review a related recursive model based on  ing vocabularies with only a handful of words and  by manually defining a threshold to binarize the  re(RAE) and describe how it can be modified to jointly  sulting vectors.  wise activation function such as tanh to the result-y  ing vector. One way of assessing how well this  ndimensional vector represents its children is to try to  reconstruct the children in a reconstruction layer:  During training, the goal is to minimize the  reconstruction errors of this input pair. For each pair, we  compute the Euclidean distance between the original  input and its reconstruction:  Figure 2: Illustration of an application of a recursive au-Erec([c1; c2]) =  toencoder to a binary tree. The nodes which are not filled are only used to compute reconstruction errors. A stan-This model of a standard autoencoder is boxed in  dard autoencoder (in box) is re-used at each node of the  Fig. 2. Now that we have defined how an  autoencoder can be used to compute an n-dimensional  vector representation (p) of two n-dimensional children  Traditional Recursive Autoencoders  (c1, c2), we can describe how such a network can be  The goal of autoencoders is to learn a representation  used for the rest of the tree.  of their inputs. In this section we describe how to  Essentially, the same steps repeat. Now that y1  obtain a reduced dimensional vector representation  is given, we can use Eq. 2 to compute y2 by setting  the children to be (c1, c2) = (x2, y1). Again, after  In the past autoencoders have only been used in  computing the intermediate parent vector y2, we can  setting where the tree structure was given a-priori.  assess how well this vector capture the content of  We review this setting before continuing with our  the children by computing the reconstruction error  model which does not require a given tree structure.  as in Eq. 4. The process repeat until the full tree  Fig. 2 shows an instance of a recursive autoencoder  is constructed and we have a reconstruction error at  (RAE) applied to a given tree. Assume we are given  each nonterminal node. This model is similar to the  a list of word vectors x = (x1, . . . , xm) as described  RAAM model (Pollack, 1990) which also requires a  in the previous section as well as a binary tree  strucfixed tree structure.  ture for this input in the form of branching triplets  of parents with children: (p c  Unsupervised Recursive Autoencoder for  1c2). Each child  can be either an input word vector x  Structure Prediction  nal node in the tree. For the example in Fig. 2, we  Now, assume there is no tree structure given for  have the following triplets: ((y1 x3x4), (y2  the input vectors in x. The goal of our  structurex2y1), (y1 x1y2)). In order to be able to apply  prediction RAE is to minimize the reconstruction  erthe same neural network to each pair of children, the  ror of all vector pairs of children in a tree. We  dehidden representations yi have to have the same  difine A(x) as the set of all possible trees that can be  mensionality as the xi's.  built from an input sentence x. Further, let T (y) be  Given this tree structure, we can now compute the  a function that returns the triplets of a tree indexed  parent representations. The first parent vector y1 is  by s of all the non-terminal nodes in a tree. Using  computed from the children (c1, c2) = (x3, x4):  the reconstruction error of Eq. 4, we compute  f (W (1)[c  where we multiplied a matrix of parameters W (1)  by the concatenation of the two children.  We now describe a greedy approximation that  conAfter adding a bias term we applied an  elementstructs such a tree.  Greedy Unsupervised RAE. For a sentence with the number of words underneath a current poten-m words, we apply the autoencoder recursively. It  tial child, we re-define the reconstruction error to be  takes the first pair of neighboring vectors, defines  them as potential children of a phrase (c1; c2) =  1; x2), concatenates them and gives them as  input to the autoencoder. For each word pair, we save  Length Normalization.  One of the goals of  the potential parent node p and the resulting  reconRAEs is to induce semantic vector representations  struction error.  that allow us to compare n-grams of different  After computing the score for the first pair, the  lengths. The RAE tries to lower reconstruction error  network is shifted by one position and takes as input  of not only the bigrams but also of nodes higher in  vectors (c1, c2) = (x2, x3) and again computes a  pothe tree. Unfortunately, since the RAE computes the  tential parent node and a score. This process repeats  hidden representations it then tries to reconstruct, it  until it hits the last pair of words in the sentence:  can just lower reconstruction error by making the  (c1, c2) = (xm 1, xm). Next, it selects the pair  hidden layer very small in magnitude. To prevent  which had the lowest reconstruction error (Erec) and  such undesirable behavior, we modify the hidden  its parent representation p will represent this phrase  layer such that the resulting parent representation  aland replace both children in the sentence word list.  ways has length one, after computing p as in Eq. 2,  For instance, consider the sequence (x1, x2, x3, x4)  we simply set: p = p .  and assume the lowest Erec was obtained by the pair  Semi-Supervised Recursive Autoencoders  3, x4). After the first pass, the new sequence then  consists of (x1, x2, p(3,4)). The process repeats and  So far, the RAE was completely unsupervised and  treats the new vector p(3,4) like any other input  vecinduced general representations that capture the  setor. For instance, subsequent states could be either:  mantics of multi-word phrases.In this section, we  (x1, p(2,(3,4))) or (p(1,2), p(3,4)). Both states would  extend RAEs to a semi-supervised setting in order  then finish with a deterministic choice of collapsing  to predict a or phrase-level target  distributhe remaining two states into one parent to obtain  tion t.1  (p(1,(2,(3,4)))) or (p((1,2),(3,4))) respectively. The tree One of the main advantages of the RAE is that  is then recovered by unfolding the collapsing  decieach node of the tree built by the RAE has  associated with it a distributed vector representation (the  The resulting tree structure captures as much of  parent vector p) which could also be seen as  feathe single-word information as possible (in order  tures describing that phrase. We can leverage this  to allow reconstructing the word vectors) but does  representation by adding on top of each parent node  not necessarily follow standard syntactic constraints.  a simple softmax layer to predict class distributions:  We also experimented with a method that finds  better solutions to Eq. 5 based on CKY-like beam  search algorithms (Socher et al., 2010; Socher et al.,  Assuming there are K labels, d  2011) but the performance is similar and the greedy  version is much faster.  k = 1. Fig. 3 shows such a semi-supervised  Weighted Reconstruction. One problem with  RAE unit. Let tk be the kth element of the  multinosimply using the reconstruction error of both  chilmial target label distribution t for one entry. The  dren equally as describe in Eq. 4 is that each child  softmax layer's outputs are interpreted as  condicould represent a different number of previously  tional probabilities dk = p(k|[c1; c2]), hence the  collapsed words and is hence of bigger importance  for the overall meaning reconstruction of the  sentence.  For instance in the case of (x1, p(2,(3,4)))  one would like to give more importance to  reconstructing p than x1. We capture this desideratum  1For the binary label classification case, the distribution is by adjusting the reconstruction error. Let n1, n2 be  of the form [1, 0] for class 1 and [0, 1] for class 2.  Learning  Let = (W (1), b(1), W (2), b(1), W label, L) be the set  of our model parameters, then the gradient becomes:  Figure 3: Illustration of an RAE unit at a nonterminal tree node. Red nodes show the supervised softmax layer for  To compute this gradient, we first greedily construct  label distribution prediction.  all trees and then derivatives for these trees are  computed efficiently via backpropagation through  strucUsing this cross-entropy error for the label and the  ture (Goller and K chler, 1996). Because the  algoreconstruction error from Eq. 6, the final  semirithm is greedy and the derivatives of the supervised  supervised RAE objective over (sentences,label)  cross-entropy error also modify the matrix W (1),  pairs (x, t) in a corpus becomes  this objective is not necessarily continuous and a  step in the gradient descent direction may not  necessarily decrease the objective. However, we found  that L-BFGS run over the complete training data  (batch mode) to minimize the objective works well  where we have an error for each entry in the training  in practice, and that convergence is smooth, with the  set that is the sum over the error at the nodes of the  algorithm typically finding a good solution quickly.  tree that is constructed by the greedy RAE:  s T (RAE (x))  We first describe the new experience project (EP)  The error at each nonterminal node is the weighted  dataset, results of standard classification tasks on  sum of reconstruction and cross-entropy errors,  this dataset and how to predict its sentiment label  distributions. We then show results on other  commonly used datasets and conclude with an analysis  of the important parameters of the model.  The hyperparameter weighs reconstruction and  In all experiments involving our model, we  reprecross-entropy error. When minimizing the  crossentropy error of this softmax layer, the error will  explore the two settings mentioned in Sec. 2.1. We  backpropagate and influence both the RAE  paramcompare performance on standard datasets when  useters and the word representations. Initially, words  ing randomly initialized word vectors (random word  such as good and bad have very similar  representainit.) or word vectors trained by the model of  Coltions. This is also the case for Brown clusters and  lobert and Weston (2008) and provided by Turian  other methods that use only cooccurrence statistics  et al. (2010).2 These vectors were trained on an  in a small window around each word. When  learnunlabeled corpus of the English Wikipedia. Note  ing with positive/negative sentiment, the word  emthat alternatives such as Brown clusters are not  suitbeddings get modified and capture less syntactic and  able since they do not capture sentiment information  more sentiment information.  (good and bad are usually in the same cluster) and  In order to predict the sentiment distribution of a  cannot be modified via backpropagation.  sentence with this model, we use the learned vector  representation of the top tree node and train a simple  Corpus  tion over interconnected response categories. This  is in contrast to most other datasets (including  multiaspect rating) where several distinct aspects are rated  independently but on the same scale. The topics  range from generic happy statements, daily  clumsiness reports, love, loneliness, to relationship abuse  Table 1: Statistics on the different datasets. K is the num-and suicidal notes. As is evident from the total  number of classes. Distr. is the distribution of the different classes (in the case of 2, the positive/negative classes, for ber of label votes, the most common user reaction  EP the rounded distribution of total votes in each class).  is one of empathy and an ability to relate to the  au|W | is the average number of words per instance. We use  thors experience. However, some stories describe  EP 4, a subset of entries with at least 4 votes.  horrible scenarios that are not common and hence  receive more offers of condolence. In the following  EP Dataset: The Experience Project  sections we show some examples of stories with  preThe confessions section of the experience project  dicted and true distributions but refrain from listing  website3 lets people anonymously write short  perthe most horrible experiences.  sonal stories or confessions . Once a story is on  For all experiments on the EP dataset, we split the  the site, each user can give a single vote to one of  five label categories (with our interpretation):  Sorry, Hugs: User offers condolences to author.  EP: Predicting the Label with Most Votes  2. You Rock: Indicating approval, congratulations.  The first task for our evaluation on the EP dataset is  3. Teehee: User found the anecdote amusing.  to simply predict the single class that receives the  4. I Understand: Show of empathy.  most votes. In order to compare our novel joint  5. Wow, Just Wow: Expression of surprise,shock.  phrase representation and classifier learning  frameThe EP dataset has 31,676 confession entries, a  towork to traditional methods, we use the following  tal number of 74,859 votes for the 5 labels above, the  average number of votes per entry is 2.4 (with a  variance of 33). For the five categories, the numbers of  Random Since there are five classes, this gives 20%  Since an entry with less than 4 votes is not very well  Most Frequent Selecting the class which most  freidentified, we train and test only on entries with at  quently has the most votes (the class I  underleast 4 total votes. There are 6,129 total such entries.  The distribution over total votes in the 5 classes  Baseline 1: Binary BoW This baseline uses  logisis similar: [0.22; 0.2; 0.11; 0.37; 0.1]. The average  tic regression on binary bag-of-word  represenlength of entries is 129 words. Some entries  contations that are 1 if a word is present and 0  othtain multiple sentences. In these cases, we average  the predicted label distributions from the sentences.  Table 1 shows statistics of this and other commonly  Baseline 2: Features This model is similar to  traused sentiment datasets (which we compare on in  later experiments). Table 2 shows example entries  in that it uses many hand-engineered resources.  as well as gold and predicted label distributions as  We first used a spell-checker and Wordnet to  described in the next sections.  map words and their misspellings to synsets to  Compared to other datasets, the EP dataset  conreduce the total number of words. We then  retains a wider range of human emotions that goes far  placed sentiment words with a sentiment  catbeyond positive/negative product or movie reviews.  egory identifier using the sentiment lexica of  Each item is labeled with a multinomial  distributhe Harvard Inquirer (Stone, 1966) and LIWC  (Pennebaker et al., 2007). Lastly, we used tf-idf  weighting on the bag-of-word representations  and trained an SVM.  Predicted&Gold  Entry (Shortened if it ends with ...)  I reguarly shoplift. I got caught once and went to jail, but I've found that this was not a deterrent. I don't buy groceries, I don't buy school supplies for my kids, I don't buy gifts for my kids, we don't pay for movies, and I  dont buy most incidentals for the house (cleaning supplies, toothpaste, etc.)...  i am a very succesfull buissnes man.i make good money but i have been addicted to crack for 13 years.i moved 1  hour away from my dealers 10 years ago to stop using now i dont use daily but once a week usally friday nights.  i used to use 1 or 2 hundred a day now i use 4 or 5 hundred on a friday.my problem is i am a funcational addict...  Hi there, Im a guy that loves a girl, the same old bloody story... I met her a while ago, while studying, she Is so perfect, so mature and yet so lonely, I get to know her and she get ahold of me, by opening her life to me and so  did I with her, she has been the first person, male or female that has ever made that bond with me,...  be kissing you right now. i should be wrapped in your arms in the dark, but instead i've ruined everything. i've piled bricks to make a wall where there never should have been one. i feel an ache that i shouldn't feel because  i've never had you close enough. we've never touched, but i still feel as though a part of me is missing. ...  Dear Love, I just want to say that I am looking for you. Tonight I felt the urge to write, and I am becoming more and more frustrated that I have not found you yet. I'm also tired of spending so much heart on an old dream. ...  I wish I knew somone to talk to here.  I loved her but I screwed it up. Now she's moved on. I'll never have her again. I don't know if I'll ever stop thinking about her.  i am 13 years old and i hate my father he is alwas geting drunk and do's not care about how it affects me or my sisters i want to care but the truthis i dont care if he dies  well i think hairy women are attractive  As soon as I put clothings on I will go down to DQ and get a thin mint blizzard. I need it. It'll make my soul feel a bit better :)  I am a 45 year old divoced woman, and I havent been on a date or had any significant relationship in 12  years...yes, 12 yrs. the sad thing is, Im not some dried up old granny who is no longer interested in men, I just can't meet men. (before you judge, no Im not terribly picky!) What is wrong with me?  When i was in kindergarden i used to lock myself in the closet and eat all the candy. Then the teacher found out it was one of us and made us go two days without freetime. It might be a little late now, but sorry guys it was me haha  My paper is due in less than 24 hours and I'm still dancing round my room!  Table 2: Example EP confessions from the test data with KL divergence between our predicted distribution (light blue, left bar on each of the 5 classes) and ground truth distribution (red bar and numbers underneath), number of votes. The 5 classes are [Sorry, Hugs ;You Rock; Teehee;I Understand;Wow, Just Wow]. Even when the KL divergence is higher, our model makes reasonable alternative label choices. Some entries are shortened.  Baseline 3: Word Vectors We can ignore the RAE  Accuracy  tree structure and only train softmax layers  directly on the pre-trained words in order to  influence the word vectors. This is followed by an  Baseline 1: Binary BoW  SVM trained on the average of the word  vectors.  Baseline 3: Word Vectors  RAE (our method)  We also experimented with latent Dirichlet  allocation (Blei et al., 2003) but performance was very  Table 3: Accuracy of predicting the class with most votes.  Table 3 shows the results for predicting the class  with the most votes. Even the approach that is based  maximum label task, we backprop using the gold  on sentiment lexica and other resources is  outpermultinomial distribution as a target. Since we  maxformed by our model by almost 3%, showing that  imize likelihood and because we want to predict a  for tasks involving complex broad-range human  sendistribution that is closest to the distribution of labels timent, the often used sentiment lexica lack in cover-that people would assign to a story, we evaluate  usage and traditional bag-of-words representations are  not powerful enough.  where g is the gold distribution and p is the predicted  EP: Predicting Sentiment Distributions  one. We report the average KL divergence, where a  We now turn to evaluating our  distributionsmaller value indicates better predictive power. To  prediction approach. In both this and the previous  get an idea of the values of KL divergence,  predictVoting with two lexica  Rule-based reversal on trees  Bag of features with reversal  Features Word Vec.  RAE  Figure 4: Average KL-divergence between gold and  predicted sentiment distributions (lower is better).  Table 4: Accuracy of sentiment classification on movie  review polarity (MR) and the MPQA dataset.  ing random distributions gives a an average of 1.2 in  KL divergence, predicting simply the average  distribution in the training data give 0.83. Fig. 4 shows  that our RAE-based model outperforms the other  baselines. Table 2 shows EP example entries with  predicted and gold distributions, as well as numbers  of votes.  Binary Polarity Classification  Figure 5: Accuracy on the development split of the MR  polarity dataset for different weightings of reconstruction In order to compare our approach to other meth-error and supervised cross-entropy error: err = Erec +  ods we also show results on commonly used  sentiment datasets: movie reviews4 (MR) (Pang and  Lee, 2005) and opinions5 (MPQA) (Wiebe et al.,  2005).We give statistical information on these and  pared to randomly initialized word vectors (setting:  the EP corpus in Table 1.  random word init). This shows that our method can  We compare to the state-of-the-art system of  work well even in settings with little training data.  (Nakagawa et al., 2010), a dependency tree based  We visualize the semantic vectors that the recursive  classification method that uses CRFs with hidden  autoencoder learns by listing n-grams that give the  variables. We use the same training and testing  regihighest probability for each polarity. Table 5 shows  men (10-fold cross validation) as well as their  basesuch n-grams for different lengths when the RAE is  lines: majority phrase voting using sentiment and  trained on the movie review polarity dataset.  reversal lexica; rule-based reversal using a  depenOn a 4-core machine, training time for the smaller  dency tree; Bag-of-Features and their full Tree-CRF  corpora such as the movie reviews takes around 3  model. As shown in Table 4, our algorithm  outperhours and for the larger EP corpus around 12 hours  forms their approach on both datasets. For the movie  until convergence. Testing of hundreds of movie  rereview (MR) data set, we do not use any  handviews takes only a few seconds.  designed lexica. An error analysis on the MPQA  dataset showed several cases of single words which  never occurred in the training set. Correctly  classifyReconstruction vs. Classification Error  ing these instances can only be the result of having  them in the original sentiment lexicon. Hence, for  In this experiment, we show how the  hyperparamethe experiment on MPQA we added the same  senter influences accuracy on the development set of  timent lexicon that (Nakagawa et al., 2010) used in  one of the cross-validation splits of the MR dataset.  their system to our training set. This improved  acThis parameter essentially trade-off the supervised  curacy from 86.0 to 86.4.Using the pre-trained word  and unsupervised parts of the objective. Fig. 5 shows  vectors boosts performance by less than 1%  comthat a larger focus on the supervised objective is  important but that a weight of = 0.2 for the  reconwww.cs.cornell.edu/people/pabo/  struction error prevents overfitting and achieves the  5www.cs.pitt.edu/mpqa/  highest performance.  bad; boring; dull; flat; pointless; tv; neither; pretentious; badly; touching; enjoyable; powerful; warm; moving; culture; flaws; worst; lame; mediocre; lack; routine; loud; bore; barely; stupid; provides; engrossing; wonderful; beautiful; quiet; socio-political; tired; poorly; suffers; heavy;nor; choppy; superficial  thoughtful; portrait; refreshingly; chilling; rich; beautifully; solid; 2  how bad; by bad; dull .; for bad; to bad; boring .; , dull; are bad; the beautiful; moving,; thoughtful and; , inventive; solid and; a that bad; boring ,; , flat; pointless .; badly by; on tv; so routine; lack beautiful; a beautifully; and hilarious; with dazzling; provides the; the; mediocre .; a generic; stupid ,; abysmally pathetic  provides.; and inventive; as powerful; moving and; a moving; a powerful  . too bad; exactly how bad; and never dull; shot but dull; is more funny and touching; a small gem; with a moving; cuts, fast; , fine boring; to the dull; dull, UNK; it is bad; or just plain; by turns music; smart and taut; culture into a; romantic , riveting; ... a solid; pretentious; manipulative and contrived; bag of stale; is a bad; the beautifully acted .; , gradually reveals; with the chilling; cast of whole mildly; contrived pastiche of; from this choppy; stale mate-solid; has a solid; spare yet audacious; ... a polished; both the rial.  beauty;  boring than anything else.; a major waste ... generic; nothing i reminded us that a feel-good; engrossing, seldom UNK,; between hadn't already; ,UNK plotting;superficial; problem ? no laughs.; realistic characters showing honest; a solid piece of journalistic;  ,just horribly mediocre .; dull, UNK feel.; there's nothing exactly easily the most thoughtful fictional; cute, funny, heartwarming; wrong; movie is about a boring; essentially a collection of bits with wry humor and genuine; engrossing and ultimately tragic.; 8  loud, silly, stupid and pointless.; dull, dumb and derivative horror shot in rich , shadowy black-and-white , devils an escapist con-film.; UNK's film, a boring, pretentious; this film biggest problem fection that 's pure entertainment .; , deeply absorbing piece that  ? no laughs.; film in the series looks and feels tired; do draw easy works as a; ... one of the most ingenious and entertaining; film is a chuckles but lead nowhere.; stupid, infantile, redundant, sloppy riveting , brisk delight .; bringing richer meaning to the story 's; Table 5: Examples of n-grams (n = 1, 2, 3, 5, 8) from the test data of the movie polarity dataset for which our model predicts the most positive and most negative responses.  Related Work  or object detection. The current work is related in  that it uses a recursive deep learning model.  However, RNNs require labeled tree structures and use a  Autoencoders are neural networks that learn a  resupervised score at each node. Instead, RAEs learn  duced dimensional representation of fixed-size  inhierarchical structures that are trying to capture as  puts such as image patches or bag-of-word  repremuch of the the original word vectors as possible.  sentations of text documents. They can be used to  The learned structures are not necessarily  syntactiefficiently learn feature encodings which are useful  cally plausible but can capture more of the semantic  for classification. Recently, Mirowski et al. (2010)  content of the word vectors. Other recent deep  learnlearn dynamic autoencoders for documents in a  baging methods for sentiment analysis include (Maas et  of-words format which, like ours, combine  supervised and reconstruction objectives.  The idea of applying an autoencoder in a recursive  Sentiment Analysis  setting was introduced by Pollack (1990). Pollack's  recursive auto-associative memories (RAAMs) are  Pang et al. (2002) were one of the first to experiment  similar to ours in that they are a connectionst,  feedwith sentiment classification. They show that  simHowever, RAAMs learn vector  ple bag-of-words approaches based on Naive Bayes,  representations only for fixed recursive data  strucMaxEnt models or SVMs are often insufficient for  tures, whereas our RAE builds this recursive data  predicting sentiment of documents even though they  structure. More recently, (Voegtlin and Dominey,  work well for general topic-based document  classi2005) introduced a linear modification to RAAMs  fication. Even adding specific negation words,  bithat is able to better generalize to novel  combinagrams or part-of-speech information to these  modtions of previously seen constituents. One of the  Other  major shortcomings of previous applications of  redocument-level sentiment work includes (Turney,  was their binary word representation as discussed in  and Lee, 2004). For further references, see (Pang  and Lee, 2008).  Instead of document level sentiment  classificaintroduced a max-margin framework based on  recurtion, (Wilson et al., 2005) analyze the contextual  sive neural networks (RNNs) for labeled structure  polarity of phrases and incorporate many well  deprediction. Their models are applicable to natural  signed features including dependency trees. They  language and computer vision tasks such as parsing  also show improvements by first distinguishing  between neutral and polar sentences. Our model natu-independently. In contrast, in our method a single  rally incorporates the recursive interaction between  aspect (a complex reaction to a human experience)  context and polarity words in sentences in a unified  is predicted not in terms of a fixed scale but in terms  framework while simultaneously learning the  necesof a multinomial distribution over several  interconsary features to make accurate predictions. Other  apnected, sometimes mutually exclusive emotions. A  proaches for sentence-level sentiment detection  insingle story cannot simultaneously obtain a strong  reaction in different emotional responses (by virtue  of having to sum to one).  Most previous work is centered around a given  Conclusion  sentiment lexicon or building one via heuristics  (Kim and Hovy, 2007; Esuli and Sebastiani, 2007),  We presented a novel algorithm that can accurately  manual annotation (Das and Chen, 2001) or machine  predict sentence-level sentiment distributions.  Withlearning techniques (Turney, 2002). In contrast, we  out using any hand-engineered resources such as  do not require an initial or constructed sentiment  lexsentiment lexica, parsers or sentiment shifting rules,  icon of positive and negative words. In fact, when  our model achieves state-of-the-art performance on  training our approach on documents or sentences, it  commonly used sentiment datasets. Furthermore,  jointly learns such lexica for both single words and  we introduce a new dataset that contains  distribun-grams (see Table 5). (Mao and Lebanon, 2007)  tions over a broad range of human emotions. Our  propose isotonic conditional random fields and  difevaluation shows that our model can more  accuferentiate between local, sentence-level and global,  rately predict these distributions than other models.  The work of (Polanyi and Zaenen, 2006; Choi and  Acknowledgments  Cardie, 2008) focuses on manually constructing  sevWe gratefully acknowledge the support of the Defense  eral lexica and rules for both polar words and  reAdvanced Research Projects Agency (DARPA) Machine  lated content-word negators, such as prevent  canReading Program under Air Force Research Laboratory  cer , where prevent reverses the negative polarity of  (AFRL) prime contract no. FA8750-09-C-0181. Any  opinions, findings, and conclusion or recommendations  cancer. Like our approach they capture  composiexpressed in this material are those of the author(s) and  tional semantics. However, our model does so  withdo not necessarily reflect the view of DARPA, AFRL, or  out manually constructing any rules or lexica.  the US government. This work was also supported in part  Recently, (Velikovich et al., 2010) showed how to  by the DARPA Deep Learning program under contract  use a seed lexicon and a graph propagation  framenumber FA8650-10-C-7020.  work to learn a larger sentiment lexicon that also  inWe thank Chris Potts for help with the EP data set,  Raycludes polar multi-word phrases such as once in a  mond Hsu, Bozhi See, and Alan Wu for letting us use  their system as a baseline and Jiquan Ngiam, Quoc Le,  life time . While our method can also learn  multiGabor Angeli and Andrew Maas for their feedback.  word phrases it does not require a seed set or a large  web graph. (Nakagawa et al., 2010) introduced an  approach based on CRFs with hidden variables with  very good performance. We compare to their  stateof-the-art system. We outperform them on the  stanand  dard corpora that we tested on without requiring  Exploring sentiment  summarization. In Proceedings of the AAAI Spring  external systems such as POS taggers, dependency  Symposium on Exploring Attitude and Affect in Text:  parsers and sentiment lexica. Our approach jointly  Theories and Applications.  learns the necessary features and tree structure.  Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003.  In multi-aspect rating (Snyder and Barzilay, 2007)  A neural probabilistic language model. Journal of  Maone finds several distinct aspects such as food or  serchine Learning Research, 3:1137 1155.  vice in a restaurant and then rates them on a fixed  D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent  linear scale such as 1-5 stars, where all aspects could  dirichlet allocation. Journal of Machine Learning  Reobtain just 1 star or all aspects could obtain 5 stars  search., 3:993 1022.  Y. Choi and C. Cardie. 2008. Learning with composi-B. Pang and L. Lee. 2005. Seeing stars: Exploiting class  tional semantics as structural inference for  subsentenrelationships for sentiment categorization with respect  tial sentiment analysis. In EMNLP.  to rating scales. In ACL, pages 115 124.  R. Collobert and J. Weston.  tecture for natural language processing: deep neural  ment analysis. Foundations and Trends in Information  networks with multitask learning. In Proceedings of  B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs  S. Das and M. Chen. 2001. Yahoo! for Amazon:  Exup? Sentiment classification using machine learning  tracting market sentiment from stock message boards.  In Proceedings of the Asia Pacific Finance Association  J. W. Pennebaker, R.J. Booth, and M. E. Francis. 2007.  Annual Conference (APFA).  Linguistic inquiry and word count: Liwc2007  operaK. Dave, S. Lawrence, and D. M. Pennock. 2003.  Mintors manual. University of Texas.  ing the peanut gallery: Opinion extraction and  semantic classification of product reviews. In Proceedings of  shifters.  J. B. Pollack. 1990. Recursive distributed  representaX. Ding, B. Liu, and P. S. Yu. 2008. A holistic  lexicontions. Artificial Intelligence, 46:77 105, November.  based approach to opinion mining. In Proceedings of  C. Potts. 2010. On the negativity of negation. In David  the Conference on Web Search and Web Data Mining  Lutz and Nan Li, editors, Proceedings of Semantics  and Linguistic Theory 20. CLC Publications, Ithaca,  J. L. Elman. 1991. Distributed representations, simple  recurrent networks, and grammatical structure.  MaB. Snyder and R. Barzilay. 2007. Multiple aspect  rankchine Learning, 7(2-3):195 225.  ing using the Good Grief algorithm. In HLT-NAACL.  R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning  net synsets: An application to opinion mining.  continuous phrase representations and syntactic  parsProceedings of the Association for Computational  Lining with recursive neural networks. In Proceedings of  the NIPS-2010 Deep Learning and Unsupervised  Feature Learning Workshop.  dependent distributed representations by  backpropagaR. Socher, C. C. Lin, A. Y. Ng, and C. D. Manning.  tion through structure. In Proceedings of the  Interna2011. Parsing Natural Scenes and Natural Language  tional Conference on Neural Networks (ICNN-96).  with Recursive Neural Networks. In ICML.  P. J. Stone. 1966. The General Inquirer: A Computer  Coupling niche browsers and affect analysis  Approach to Content Analysis. The MIT Press.  for an opinion mining application.  In Proceedings  J. Turian, L. Ratinov, and Y. Bengio. 2010. Word  repof Recherche d'Information Assist e par Ordinateur  resentations: a simple and general method for  semisupervised learning. In Proceedings of ACL, pages  2008. Learning to shift the polarity of words for  sentiP. Turney. 2002. Thumbs up or thumbs down?  Semanment classification. In IJCNLP.  tic orientation applied to unsupervised classification of  S. Kim and E. Hovy. 2007. Crystal: Analyzing  predicreviews. In ACL.  tive opinions on the web. In EMNLP-CoNLL.  R. McDonald. 2010. The viability of web-derived  poand C. Potts. 2011. Learning accurate, compact, and  larity lexicons. In NAACL, HLT.  interpretable tree annotation. In Proceedings of ACL.  T. Voegtlin and P. Dominey. 2005. Linear Recursive  DisY. Mao and G. Lebanon. 2007. Isotonic Conditional  tributed Representations. Neural Networks, 18(7).  Random Fields and Local Sentiment Flow. In NIPS.  J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating  expressions of opinions and emotions in language.  Lanauto-encoders for semantic indexing. In  guage Resources and Evaluation, 39.  Proceedings  T. Wilson, J. Wiebe, and P. Hoffmann. 2005.  Recognizof the NIPS 2010 Workshop on Deep Learning.  ing contextual polarity in phrase-level sentiment  analysis. In HLT/EMNLP.  dency tree-based sentiment classification using CRFs  H. Yu and V. Hatzivassiloglou. 2003. Towards  answerwith hidden variables. In NAACL, HLT.  ing opinion questions: Separating facts from opinions  B. Pang and L. Lee. 2004. A sentimental education:  and identifying the polarity of opinion sentences. In  Sentiment analysis using subjectivity summarization  based on minimum cuts. In ACL. 